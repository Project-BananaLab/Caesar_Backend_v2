# app/features/admin/services/file_ingest_service.py
# -*- coding: utf-8 -*-
import os
import tempfile
from typing import Optional
from sqlalchemy.orm import Session
from sqlalchemy import select
from datetime import datetime
import traceback

from app.features.admin.models.docs import Doc
from app.features.login.company.models import Company
from app.features.admin.services.s3_service import (
    put_file_and_checksum,
    delete_object_by_url,
)
from app.rag.internal_data_rag.internal_ingest import IngestService


def _get_company_code(db: Session, company_id: int) -> str:
    """
    company.id -> company.code(=íšŒì‚¬ì½”ë“œ) ë¡œ ë³€í™˜.
    - íšŒì‚¬ì½”ë“œëŠ” Chroma ì»¬ë ‰ì…˜ëª…ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.
    - ì—†ìœ¼ë©´ company_{id} fallback.
    """
    stmt = select(Company.code).where(Company.id == company_id).limit(1)
    code = db.execute(stmt).scalars().first()
    return (code or "").strip() or f"company_{company_id}"


def handle_upload_and_ingest(
    db: Session,
    *,
    company_id: int,
    employee_id: Optional[int],
    is_private: bool,
    file_bytes: bytes,
    file_name: str,
) -> dict:
    """
    ì—…ë¡œë“œ ì „ì²´ íŒŒì´í”„ë¼ì¸:
      1) S3 ì—…ë¡œë“œ
      2) docs INSERT ('processing')
      3) IngestService ë¡œ Chroma ì¸ë±ì‹± (íšŒì‚¬ì½”ë“œ ì»¬ë ‰ì…˜)
      4) docs UPDATE (succeeded/failed)
      5) (ì„ íƒ) ì‹¤íŒ¨ ì‹œ S3 ë¡¤ë°± ì‹œë„
    ë°˜í™˜: {"ok": bool, "docId": int, "chunks"?: int, "url"?: str, "error"?: str}
    """
    # â”€â”€ 1) S3 ì—…ë¡œë“œ (ì¸ìëª… file_bytes ì‚¬ìš©!)
    s3_url, size, checksum = put_file_and_checksum(
        file_bytes=file_bytes,
        orig_name=file_name,
    )

    # â”€â”€ 2) docs INSERT (processing)
    doc = Doc(
        company_id=company_id,
        employee_id=employee_id,          # ê´€ë¦¬ì ì—…ë¡œë“œë©´ None
        is_private=is_private,            # False=íšŒì‚¬ê³µê°œ, True=ê°œì¸ë¬¸ì„œ
        file_name=file_name,
        file_url=s3_url,
        file_size=size,
        checksum_sha256=checksum,
        ingest_status="processing",       # ì´ˆê¸° ìƒíƒœ
        chunks_count=0,
        error_text=None,
        created_at=datetime.utcnow(),
        updated_at=datetime.utcnow(),
        ingested_at=None,
    )
    db.add(doc)
    db.commit()
    db.refresh(doc)  # doc.id í™•ë³´(â†’ VectorDB ë©”íƒ€ doc_id ë¡œ ì‚¬ìš©)

    # â”€â”€ 3) ingest (íšŒì‚¬ì½”ë“œ ì»¬ë ‰ì…˜ + extra ë©”íƒ€ ë³‘í•©)
    try:
        company_code = _get_company_code(db, company_id)

        # ğŸ”´ Noneì´ ë“¤ì–´ê°€ë©´ Chromaê°€ ë©”íƒ€ë°ì´í„° ì—ëŸ¬ë¥¼ ëƒ…ë‹ˆë‹¤.
        #     â†’ Noneì€ ì•„ì˜ˆ ë„£ì§€ ë§ê³ , ê¸°ë³¸ íƒ€ì…ë§Œ ì‚¬ìš©
        extra_meta = {
            "doc_id": int(doc.id),
            "company_id": int(company_id),
            "is_private": bool(is_private),
        }
        if employee_id is not None:
            extra_meta["user_id"] = int(employee_id)

        with tempfile.TemporaryDirectory() as td:
            local_path = os.path.join(td, file_name)
            with open(local_path, "wb") as f:
                f.write(file_bytes)

            svc = IngestService()
            chunks_count, ok = svc.ingest_single_file_with_metadata(
                local_path,
                collection_name=company_code,  # â† íšŒì‚¬ì½”ë“œ ì»¬ë ‰ì…˜
                extra_meta=extra_meta,
                show_preview=False
            )

        if not ok:
            # ì‹¤íŒ¨ â†’ ìƒíƒœ ì €ì¥, ì—ëŸ¬ ë©”ì‹œì§€
            doc.ingest_status = "failed"
            doc.error_text = "embedding_or_chroma_error"
            doc.updated_at = datetime.utcnow()
            db.add(doc)
            db.commit()

            # (ì„ íƒ) ë¡¤ë°±: S3 ì§€ìš°ê¸°
            try:
                delete_object_by_url(s3_url)
            except Exception:
                pass

            return {"ok": False, "docId": doc.id, "error": "ingest_failed"}

        # â”€â”€ 4) docs UPDATE (ì„±ê³µ)
        doc.ingest_status = "succeeded"
        doc.chunks_count = chunks_count
        doc.ingested_at = datetime.utcnow()
        doc.updated_at = datetime.utcnow()
        db.add(doc)
        db.commit()

        return {"ok": True, "docId": doc.id, "chunks": chunks_count, "url": s3_url}

    except Exception as e:
        # ì‹¤íŒ¨ ì‹œ ìƒíƒœ/ì—ëŸ¬ ë©”ì‹œì§€ ë‚¨ê¹€
        doc.ingest_status = "failed"
        doc.error_text = f"{type(e).__name__}: {e}"
        doc.updated_at = datetime.utcnow()
        db.add(doc)
        db.commit()

        # (ì„ íƒ) ë¡¤ë°±: S3 ì§€ìš°ê¸°
        try:
            delete_object_by_url(s3_url)
        except Exception:
            pass

        traceback.print_exc()
        return {"ok": False, "docId": doc.id, "error": str(e)}


def delete_doc_everywhere(db: Session, *, doc_id: int) -> dict:
    """
    ë¬¸ì„œë¥¼ DB/S3/VectorDBì—ì„œ ë™ì‹œ ì •ë¦¬.
    - VectorDBëŠ” í•´ë‹¹ íšŒì‚¬ì½”ë“œ ì»¬ë ‰ì…˜ì—ì„œ where={"doc_id": doc_id} ë¡œ ì‚­ì œ.
    """
    doc = db.get(Doc, doc_id)
    if not doc:
        return {"ok": False, "error": "not_found"}

    # S3 ê°ì²´ ì‚­ì œ
    try:
        delete_object_by_url(doc.file_url)
    except Exception:
        pass

    # VectorDB ì‚­ì œ
    try:
        company_code = _get_company_code(db, doc.company_id)
        svc = IngestService()
        col = svc.get_chroma_collection(company_code)
        col.delete(where={"doc_id": int(doc_id)})
    except Exception:
        pass

    # DB ì‚­ì œ
    db.delete(doc)
    db.commit()
    return {"ok": True, "deleted": doc_id}
