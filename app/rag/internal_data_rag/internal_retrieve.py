# app/rag/internal_data_rag/internal_retrieve.py
# -*- coding: utf-8 -*-
# RAG ê²€ìƒ‰ & ë‹µë³€ (ì‹œíŠ¸/í˜ì´ì§€ ë©”íƒ€ í‘œì‹œ ë³´ê°•)
import os
from typing import List, Tuple
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import tool
from langchain_core.documents import Document

load_dotenv()

CHROMA_PATH = os.getenv("CHROMA_PATH", "./chroma_data")
CHROMA_PATH = os.path.abspath(CHROMA_PATH)
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "inside_data1")
EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
CHAT_MODEL = os.getenv("CHAT_MODEL", "gpt-4o-mini")
MAX_CONTEXT_CHARS = int(os.getenv("MAX_CONTEXT_CHARS", "12000"))

# Chroma Cloud ì„¤ì •
CHROMA_TENANT = os.getenv("CHROMA_TENANT")
CHROMA_DATABASE = os.getenv("CHROMA_DATABASE")
CHROMA_API_KEY = os.getenv("CHROMA_API_KEY")

_embeddings = OpenAIEmbeddings(model=EMBED_MODEL)

# Chroma Cloud ì—°ê²°
import chromadb

# Chroma Cloud í´ë¼ì´ì–¸íŠ¸ (CloudClient ë°©ì‹)
chroma_client = chromadb.CloudClient(
    tenant=CHROMA_TENANT,
    database=CHROMA_DATABASE,
    api_key=CHROMA_API_KEY,
)

_vectorstore = Chroma(
    client=chroma_client,
    embedding_function=_embeddings,
    collection_name=COLLECTION_NAME,
)
print(f"âœ… Chroma Cloud ì—°ê²°: tenant={CHROMA_TENANT}, database={CHROMA_DATABASE}")

# ë¡œì»¬ Chroma ì—°ê²° (ì£¼ì„ ì²˜ë¦¬)
# _vectorstore = Chroma(
#     collection_name=COLLECTION_NAME,
#     embedding_function=_embeddings,
#     persist_directory=CHROMA_PATH,
# )
# print(f"âœ… ë¡œì»¬ Chroma ì—°ê²°: {CHROMA_PATH}")
_llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)

_prompt = ChatPromptTemplate.from_messages(
    [
        ("system",
         "ë‹¹ì‹ ì€ ì‚¬ë‚´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ë‹µí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
         "ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹µë³€í•˜ê³ , "
         "ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. ê°€ëŠ¥í•˜ë‹¤ë©´ ì¶œì²˜(íŒŒì¼/ì‹œíŠ¸/í˜ì´ì§€)ë¥¼ í•¨ê»˜ í‘œì‹œí•˜ì„¸ìš”."),
        ("user", "ì§ˆë¬¸: {question}\n\nì°¸ê³  ì»¨í…ìŠ¤íŠ¸(ì—¬ëŸ¬ ì²­í¬):\n{context}"),
    ]
)
_parser = StrOutputParser()

def _stable_similarity(distance: float) -> float:
    try:
        d = float(distance)
    except Exception:
        d = 0.0
    if d < 0:
        d = 0.0
    return 1.0 / (1.0 + d)

def _truncate_context_blocks(blocks: List[Tuple[str, dict]], max_chars: int) -> str:
    sorted_blocks = sorted(blocks, key=lambda x: float(x[1].get("similarity_score", 0.0)), reverse=True)
    acc: List[str] = []
    total = 0
    sep = "\n\n---\n\n"
    for doc, meta in sorted_blocks:
        src = meta.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
        sheet = meta.get('sheet')
        page = meta.get('page')
        chunk = meta.get('chunk_idx', 'N/A')
        src_tag = f"[ì¶œì²˜: {src}"
        if sheet: src_tag += f" / ì‹œíŠ¸: {sheet}"
        if page: src_tag += f" / í˜ì´ì§€: {page}"
        src_tag += f" / ì²­í¬: {chunk}]"
        block = f"{src_tag}\n{doc}"
        add_len = len(block) + (len(sep) if acc else 0)
        if total + add_len > max_chars:
            break
        if acc:
            acc.append(sep); total += len(sep)
        acc.append(block); total += len(block)
    return "".join(acc)

class RetrieveService:
    def __init__(self):
        self.vectorstore = _vectorstore
        self.llm = _llm
        self.prompt = _prompt
        self.parser = _parser

    def retrieve_documents(self, query: str, top_k: int = 3) -> List[Tuple[str, dict]]:
        try:
            print(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰: '{query}' (ìƒìœ„ {top_k}ê°œ)")
            results: List[Tuple[Document, float]] = self.vectorstore.similarity_search_with_score(query, k=top_k)
            if not results:
                print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []
            contexts: List[Tuple[str, dict]] = []
            print(f"âœ… {len(results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            for i, (doc, distance) in enumerate(results, start=1):
                sim = _stable_similarity(distance)
                meta = dict(doc.metadata or {}); meta["similarity_score"] = sim
                preview = (doc.page_content[:80] + "...") if len(doc.page_content) > 80 else doc.page_content
                print(f"  [Rank {i}] ìœ ì‚¬ë„={sim:.4f}, source={meta.get('source')}, "
                      f"sheet={meta.get('sheet')}, page={meta.get('page')}, chunk={meta.get('chunk_idx')}")
                print(f"          ë‚´ìš©: {preview}")
                contexts.append((doc.page_content, meta))
            return contexts
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def generate_answer(self, query: str, contexts: List[Tuple[str, dict]], model: str | None = None) -> str:
        if not contexts:
            return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
        try:
            model_label = model or getattr(self.llm, "model", getattr(self.llm, "model_name", "unknown"))
            print(f"âš™ï¸ ë‹µë³€ ìƒì„± ì¤‘... ({len(contexts)}ê°œ ì»¨í…ìŠ¤íŠ¸, ëª¨ë¸: {model_label})")
            context_text = _truncate_context_blocks(contexts, max_chars=MAX_CONTEXT_CHARS)
            used_llm = self.llm if model is None else ChatOpenAI(model=model, temperature=0)
            chain = self.prompt | used_llm | self.parser
            answer: str = chain.invoke({"question": query, "context": context_text})
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            return answer
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def query_rag(self, query: str, top_k: int = 4, model: str | None = None, show_sources: bool = True) -> str:
        print(f"\nğŸ” ì§ˆì˜: {query}")
        contexts = self.retrieve_documents(query, top_k)
        if not contexts:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ë¶€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        answer = self.generate_answer(query, contexts, model)
        if show_sources:
            sources = []
            for _, meta in contexts:
                src = meta.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                sheet = meta.get('sheet')
                page = meta.get('page')
                chunk = meta.get('chunk_idx', 'N/A')
                tag = f"- {src}"
                if sheet: tag += f" / ì‹œíŠ¸ {sheet}"
                if page: tag += f" / í˜ì´ì§€ {page}"
                tag += f" (ì²­í¬ {chunk})"
                if tag not in sources:
                    sources.append(tag)
            return f"{answer}\n\nğŸ“‹ ì°¸ê³ í•œ ë¬¸ì„œ:\n" + "\n".join(sources)
        return answer

    def interactive_mode(self):
        print("\nğŸ¯ ëŒ€í™”í˜• RAG ê²€ìƒ‰ ì‹œì‘! (ì¢…ë£Œ: ë¹ˆ ì¤„)")
        print("-" * 60)
        while True:
            try:
                q = input("\n> ").strip()
                if not q:
                    print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                print("\n=== ë‹µë³€ ===")
                print(self.query_rag(q))
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")

_retrieve_service = RetrieveService()

@tool
def rag_search_tool(query: str) -> str:
    """
        Search and answer questions from uploaded files (PDF, DOCX, XLSX, etc.).
        
        ğŸ¯ This tool ONLY searches content from uploaded files:
        - PDF documents (regulations, manuals, reports, etc.)
        - Word documents (policies, guidelines, contracts, etc.)  
        - Excel files (data, forms, status reports, etc.)
        - Other uploaded document files
        
        Search scope:
        - Company public documents: Company policies, manuals, announcements, forms, etc.
        - Personal uploaded documents: Files personally uploaded by the user
        
        Use when:
        - File-based questions like "production status writing methods", "employee regulations", "manuals"
        - Specific content or procedure inquiries from uploaded documents
        - Questions about forms, templates, formats
        
        Args:
            query (str): Question or keyword to search in uploaded documents
            
        Returns:
            str: Answer based on uploaded documents
    """
    print(f"\nğŸ“š RAG ë„êµ¬ ì‹¤í–‰: '{query}'")
    return _retrieve_service.query_rag(query, top_k=3)

rag_tools = [rag_search_tool]

def retrieve_documents(query: str, top_k: int = 3) -> List[Tuple[str, dict]]:
    return RetrieveService().retrieve_documents(query, top_k)

def generate_answer(query: str, contexts: List[Tuple[str, dict]], model: str | None = None) -> str:
    return RetrieveService().generate_answer(query, contexts, model)

def query_rag(query: str, top_k: int = 4, model: str | None = None) -> str:
    return RetrieveService().query_rag(query, top_k, model)

def _healthcheck_vectorstore() -> bool:
    try:
        _ = _vectorstore.similarity_search("__healthcheck__", k=1)
        return True
    except Exception as e:
        print(f"âŒ Chroma í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
        return False

def main():
    print("=" * 80)
    print("ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì„œë¹„ìŠ¤")
    print("=" * 80)
    print(f"ğŸ“ CHROMA_PATH: {CHROMA_PATH}")
    print(f"ğŸ—„ï¸ COLLECTION_NAME: {COLLECTION_NAME}")
    print(f"ğŸ”¤ EMBED_MODEL: {EMBED_MODEL}")
    print(f"ğŸ§  CHAT_MODEL: {CHAT_MODEL}")
    print(f"ğŸ§» MAX_CONTEXT_CHARS: {MAX_CONTEXT_CHARS}")

    if not _healthcheck_vectorstore():
        print("ë¨¼ì € ingest íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì ì¬í•˜ì„¸ìš”.")
        return
    RetrieveService().interactive_mode()

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ê°„ë‹¨ ì§ˆì˜ 3ê°œ ì‹¤í–‰")
        for q in ["ê¸°ë¡ë¬¼ ê´€ë¦¬", "ê´€ë¦¬ê¸°ì¤€í‘œê°€ ë­ì•¼?", "ì•¼ê°„ ë° íœ´ì¼ê·¼ë¡œ ê´€ë ¨ ê·œì • ì•Œë ¤ì¤˜"]:
            print("\nQ:", q)
            print(query_rag(q))
            print("-" * 60)
    else:
        main()
