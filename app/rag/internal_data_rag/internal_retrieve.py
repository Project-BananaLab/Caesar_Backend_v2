# app/rag/internal_data_rag/internal_retrieve.py
# -*- coding: utf-8 -*-
# RAG ê²€ìƒ‰ & ë‹µë³€ (í•˜ì´ë¸Œë¦¬ë“œ RRF: ë²¡í„° + TF-IDF) + ì‹œíŠ¸/í˜ì´ì§€/ìœ í˜• í‘œê¸°
#
# - TF-IDF í¬ì†Œ ê²€ìƒ‰ ë¡œë”(_SparseSearcher) + RRF ê²°í•©(_rrf_fusion)
# - í¬ì†Œ ì¸ë±ìŠ¤ê°€ ë¡œì»¬ì— ì—†ìœ¼ë©´ S3ì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ í›„ ìºì‹œ
# - generate_answer(): content_kind ìš°ì„ ìˆœìœ„(í…ìŠ¤íŠ¸/ocr > vlm)
# - ì§ˆì˜ í™•ì¥(ë™ì˜ì–´/í”Œë«í¼ ì •ê·œí™” + PRF) â†’ dense/sparseì— ë™ì¼ ì ìš©
#
# (ì¶”ê°€: ìµœì†Œìˆ˜ì •)
# â‘  ìœ ì‚¬ë„ ì„ê³„ê°’ ì»· + ë¬¸ì„œ/ìœ í˜•ë³„ ì¿¼í„°ë§(_apply_threshold_and_quota)
# â‘¡ Cross-Encoder ì¬ì •ë ¬(_rerank) â€” ENABLE_RERANKë¡œ on/off
#
# â›ï¸ ë²„ê·¸í”½ìŠ¤(í•µì‹¬): _rrf_fusionì´ distanceë¥¼ ì˜ëª» ê³„ì‚°í•˜ë˜ ë¬¸ì œ ìˆ˜ì •
#     - meta['rrf_sim'] = RRF ì ìˆ˜ì˜ 0~1 ì •ê·œí™” ê°’
#     - ë°˜í™˜ distance = 1 - rrf_sim  (ì‘ì„ìˆ˜ë¡ ê°€ê¹ë‹¤)
#     - ì„ê³„/ì¶œë ¥ ìœ ì‚¬ë„ì—ì„œ rrf_sim ìš°ì„  ì‚¬ìš©
#
# â–³ ê°œì„ (ì •ë°€ë„ ë³´ìˆ˜ì  íšŒë³µ í¬í•¨):
# - (A) í˜ì´ì§€/ì¶œì²˜ ë””ë“‘ ì™„í™”: page ì—†ìœ¼ë©´ ë””ë“‘ X, (source,page,sheet) ê¸°ì¤€ í˜ì´ì§€ë‹¹ ìµœëŒ€ Nê°œ í—ˆìš©
# - (B) LLM Multi-Query: ì§§ì€ ì§ˆì˜ì—ë§Œ 1íšŒ ì¬ì‘ì„±(ê¸°ë³¸) â†’ query drift ì–µì œ
# - (C) ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µ ì œê±°: ë¸”ë¡ ë‹¨ìœ„, ë™ì¼ ë¼ì¸ 2íšŒê¹Œì§„ í—ˆìš©(ì¤‘ìš” ë¬¸êµ¬ ë³´ì¡´)
# - (D) ìŠ¤ì½”ì–´ ë¡œê¹…: ì„ê³„/ë¬¸ì„œ/ìœ í˜• ì¿¼í„° drop ì¹´ìš´íŠ¸ ì¶œë ¥
# - (E) ì„ê³„ê°’ ì ì‘í™”: rrf_sim ë¶„í¬ ê¸°ë°˜ìœ¼ë¡œ ì»· ë³´ì •(ê¸°ë³¸ê°’ì€ í•˜í•œì„ )
# - (F) CE ë¦¬ë­í¬ ê°œì„ : MultiBERT ì‚¬ìš© â†’ í•œêµ­ì–´ í¬í•¨ ì¿¼ë¦¬ë„ CE ì ìš©, ì…ë ¥ ê¸¸ì´/ë°°ì¹˜ íŠœë‹

import os
import re
import json
import unicodedata
from pathlib import Path
from typing import List, Tuple, Dict, Any
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import tool
from langchain_core.documents import Document

# í¬ì†Œ ê²€ìƒ‰(ìˆìœ¼ë©´ ì‚¬ìš©)
try:
    from sklearn.metrics.pairwise import cosine_similarity
    import joblib
except Exception:
    cosine_similarity = None
    joblib = None

# Cross-encoder(ì„ íƒ)
try:
    from sentence_transformers import CrossEncoder
except Exception:
    CrossEncoder = None

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHROMA_PATH = os.getenv("CHROMA_PATH", "./chroma_data")
CHROMA_PATH = os.path.abspath(CHROMA_PATH)
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "inside_data4")
EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
CHAT_MODEL = os.getenv("CHAT_MODEL", "gpt-4o-mini")
MAX_CONTEXT_CHARS = int(os.getenv("MAX_CONTEXT_CHARS", "12000"))
SPARSE_INDEX_PATH = os.getenv("SPARSE_INDEX_PATH", os.path.join(CHROMA_PATH, "sparse"))

# ì„ê³„/ì¿¼í„°/ë¦¬ë­í¬
SIM_THRESHOLD = float(os.getenv("SIM_THRESHOLD", "0.25"))
DOC_QUOTA = int(os.getenv("DOC_QUOTA", "15"))
KIND_QUOTA = os.getenv("KIND_QUOTA", "text:12,image_ocr:8,image_vlm:4")
ENABLE_RERANK = os.getenv("ENABLE_RERANK", "true").lower() == "true"
CROSS_ENCODER_MODEL = os.getenv("CROSS_ENCODER_MODEL", "cross-encoder/ms-marco-MultiBERT-L-12")
# CE ì…ë ¥ ìµœì í™”
CE_MAX_CHARS = int(os.getenv("CE_MAX_CHARS", "1800"))
CE_BATCH_SIZE = int(os.getenv("CE_BATCH_SIZE", "16"))

# LLM Multi-Query (ë³´ìˆ˜í™”)
ENABLE_LLM_MULTIQUERY = os.getenv("ENABLE_LLM_MULTIQUERY", "true").lower() == "true"
N_QUERY_REWRITES = int(os.getenv("N_QUERY_REWRITES", "1"))  # ê¸°ë³¸ 1ë¡œ ì™„í™”
SHORT_QUERY_CHAR = int(os.getenv("SHORT_QUERY_CHAR", "40"))

# ë””ë“‘ íŒŒë¼ë¯¸í„°
MAX_PER_PAGE = int(os.getenv("MAX_PER_PAGE", "2"))  # í˜ì´ì§€ë‹¹ ìµœëŒ€ ìœ ì§€ ê°œìˆ˜

# S3(ì˜µì…˜)
S3_BUCKET = os.getenv("S3_BUCKET")
S3_REGION = os.getenv("S3_REGION")
AWS_S3_PREFIX = os.getenv("AWS_S3_PREFIX", "rag/sparse")

# ì§ˆì˜ ë™ì˜ì–´ ì‚¬ì „(ì™¸ë¶€ íŒŒì¼ ê²½ë¡œ - ì„ íƒ)
QUERY_SYNONYM_PATH = os.getenv("QUERY_SYNONYM_PATH")  # e.g. ./config/synonyms.json

# Chroma Cloud
CHROMA_TENANT = os.getenv("CHROMA_TENANT")
CHROMA_DATABASE = os.getenv("CHROMA_DATABASE")
CHROMA_API_KEY = os.getenv("CHROMA_API_KEY")

_embeddings = OpenAIEmbeddings(model=EMBED_MODEL)

# Chroma Cloud í´ë¼ì´ì–¸íŠ¸ + ë²¡í„°ìŠ¤í† ì–´
import chromadb
chroma_client = chromadb.CloudClient(
    tenant=CHROMA_TENANT,
    database=CHROMA_DATABASE,
    api_key=CHROMA_API_KEY,
)
_vectorstore = Chroma(
    client=chroma_client,
    embedding_function=_embeddings,
    collection_name=COLLECTION_NAME,
)
_llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)

_prompt = ChatPromptTemplate.from_messages(
    [
        ("system",
         "ë‹¹ì‹ ì€ ì‚¬ë‚´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ë‹µí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
         "ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹µë³€í•˜ê³ , "
         "ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. ê°€ëŠ¥í•˜ë‹¤ë©´ ì¶œì²˜(íŒŒì¼/ì‹œíŠ¸/í˜ì´ì§€)ë¥¼ í•¨ê»˜ í‘œì‹œí•˜ì„¸ìš”."),
        ("user", "ì§ˆë¬¸: {question}\n\nì°¸ê³  ì»¨í…ìŠ¤íŠ¸(ì—¬ëŸ¬ ì²­í¬):\n{context}"),
    ]
)
_parser = StrOutputParser()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _stable_similarity(distance: float) -> float:
    """ë²¡í„°/TF-IDF distance(ì‘ì„ìˆ˜ë¡ ê°€ê¹Œì›€) â†’ 0~1 ìœ ì‚¬ë„."""
    try:
        d = float(distance)
    except Exception:
        d = 0.0
    if d < 0:
        d = 0.0
    return 1.0 / (1.0 + d)

def _truncate_context_blocks(blocks: List[Tuple[str, dict]], max_chars: int) -> str:
    sorted_blocks = sorted(blocks, key=lambda x: float(x[1].get("similarity_score", 0.0)), reverse=True)
    acc: List[str] = []; total = 0; sep = "\n\n---\n\n"
    for doc, meta in sorted_blocks:
        src = meta.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
        sheet = meta.get('sheet'); page = meta.get('page')
        chunk = meta.get('chunk_idx', 'N/A')
        kind = meta.get('content_kind', meta.get('content_type', 'text'))
        src_tag = f"[ì¶œì²˜: {src}"
        if sheet: src_tag += f" / ì‹œíŠ¸: {sheet}"
        if page: src_tag += f" / í˜ì´ì§€: {page}"
        src_tag += f" / ì²­í¬: {chunk} / ìœ í˜•: {kind}]"
        block = f"{src_tag}\n{doc}"
        add_len = len(block) + (len(sep) if acc else 0)
        if total + add_len > max_chars:
            break
        if acc: acc.append(sep); total += len(sep)
        acc.append(block); total += len(block)
    return "".join(acc)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë””ë“‘ (ë³´ìˆ˜ì ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _dedup_by_source_page(pairs: List[Tuple[Document, float]]) -> List[Tuple[Document, float]]:
    """
    (source, page, sheet) ê¸°ì¤€ ë³´ìˆ˜ì  ë””ë“‘.
    - pageê°€ Noneì´ë©´ ë””ë“‘í•˜ì§€ ì•ŠìŒ(ë©”íƒ€ ë¶€ì‹¤ ë³´í˜¸)
    - í˜ì´ì§€ë‹¹ ìµœëŒ€ MAX_PER_PAGEê°œê¹Œì§€ í—ˆìš©
    """
    counts: Dict[Tuple[Any, Any, Any], int] = {}
    out = []
    for doc, dist in pairs:
        m = doc.metadata or {}
        src = m.get("source")
        page = m.get("page")
        sheet = m.get("sheet")
        if page is None:
            out.append((doc, dist))
            continue
        key = (src, page, sheet)
        if counts.get(key, 0) >= MAX_PER_PAGE:
            continue
        counts[key] = counts.get(key, 0) + 1
        out.append((doc, dist))
    return out

# ë¸”ë¡ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°(ë™ì¼ ë¼ì¸ 3íšŒì§¸ë¶€í„° ì œê±°)
def _dedup_sentences_blockwise(text: str) -> str:
    blocks = [b for b in re.split(r"\n\s*---\s*\n", text) if b.strip()]
    new_blocks = []
    for b in blocks:
        lines = [ln for ln in b.splitlines()]
        seen: Dict[str, int] = {}
        acc = []
        for ln in lines:
            key = ln.strip().lower()
            c = seen.get(key, 0) + 1
            seen[key] = c
            if c > 2:
                continue
            acc.append(ln)
        new_blocks.append("\n".join(acc))
    return "\n\n---\n\n".join(new_blocks)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NEW: ì§ˆì˜ í™•ì¥ (ë™ì˜ì–´/ì •ê·œí™” + PRF)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_synonym_dict() -> dict:
    try:
        if QUERY_SYNONYM_PATH and Path(QUERY_SYNONYM_PATH).exists():
            with open(QUERY_SYNONYM_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return {
        "ìœ íŠœë¸Œ": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ê°œì¸ë°©ì†¡", "ì˜ìƒ ì—…ë¡œë“œ"],
        "ë¸Œì´ë¡œê·¸": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ê°œì¸ë°©ì†¡", "ì˜ìƒ ì œì‘", "ë™ì˜ìƒ ì—…ë¡œë“œ"],
        "í‹±í†¡": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ì†Œì…œë¯¸ë””ì–´"],
        "ì¸ìŠ¤íƒ€": ["ì†Œì…œë¯¸ë””ì–´", "SNS", "ì½˜í…ì¸  ì—…ë¡œë“œ"],
        "ë¼ì´ë¸Œ": ["ì‹¤ì‹œê°„ ë°©ì†¡", "ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë°", "ì¸í„°ë„· ê°œì¸ë°©ì†¡"],
        "ìˆ˜ìµ": ["ê´‘ê³ ìˆ˜ìµ", "ìˆ˜ìµ ì°½ì¶œ", "í˜‘ì°¬", "ê°„ì ‘ê´‘ê³ ", "PPL"],
        "í˜‘ì°¬": ["ê°„ì ‘ê´‘ê³ ", "PPL", "ëŒ€ê°€ì„± ì œê³µ"],
        "ê²¸ì—…": ["ê²¸ì—…í—ˆê°€", "ê²¸ì—…ì˜ í—ˆê°€", "ë¶€ì—…"],
        "ë³´ì•ˆ": ["ì§ë¬´ìƒ ë¹„ë°€", "ì˜ì—…ë¹„ë°€", "ì •ë³´ë³´ì•ˆ"],
        "ëª…ì˜ˆí›¼ì†": ["ëª…ì˜ˆ ì¹¨í•´", "ê¶Œë¦¬ ì¹¨í•´"],
        # ì¸ì‚¬/ê²½ë ¥ ê´€ë ¨ ë™ì˜ì–´ ì¶”ê°€
        "ì´ë ¥ì‚¬": ["ì´ë ¥ì„œ", "ê²½ë ¥", "ê²½í—˜", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„", "ê°œì¸ì •ë³´"],
        "ì´ë ¥ì„œ": ["ì´ë ¥ì‚¬", "ê²½ë ¥ì„œ", "ê²½ë ¥", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„", "ê°œì¸ì •ë³´"],
        "ê²½ë ¥": ["ì´ë ¥ì„œ", "ì´ë ¥ì‚¬", "ê²½í—˜", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„"],
        "ê²½í—˜": ["ì´ë ¥ì„œ", "ì´ë ¥ì‚¬", "ê²½ë ¥", "ì»¤ë¦¬ì–´"],
    }

_SYNONYM_DICT = _load_synonym_dict()
_NORMALIZE_MAP = {
    "youtube": "ìœ íŠœë¸Œ",
    "yt": "ìœ íŠœë¸Œ",
    "vlog": "ë¸Œì´ë¡œê·¸",
    "shorts": "ì‡¼ì¸ ",
    "tiktok": "í‹±í†¡",
    "instagram": "ì¸ìŠ¤íƒ€",
}

def _normalize_terms(q: str) -> str:
    low = q.lower()
    for a, b in _NORMALIZE_MAP.items():
        low = re.sub(rf"\b{re.escape(a)}\b", b, low)
    return low

_STOP = set(["ë°","ë˜ëŠ”","ê·¸ë¦¬ê³ ","ê´€ë ¨","ê´€ë ¨ëœ","ì—¬ë¶€","ê²ƒ","ìˆ˜","ë“±","í•´ë‹¹","í•˜ëŠ”","ê²½ìš°","ëŒ€í•œ","ìœ¼ë¡œ","ì—ì„œ","í•˜ë‹¤"])
def _extract_keywords(text: str, top_k: int = 6) -> list:
    toks = re.split(r"[^ê°€-í£A-Za-z0-9]+", text)
    cnt: Dict[str, int] = {}
    for t in toks:
        if len(t) < 2 or t in _STOP:
            continue
        cnt[t] = cnt.get(t, 0) + 1
    return [w for w,_ in sorted(cnt.items(), key=lambda x: x[1], reverse=True)[:top_k]]

def _expand_query_base(q: str) -> str:
    qn = _normalize_terms(q)
    aug: List[str] = []
    for k, vs in _SYNONYM_DICT.items():
        if k in qn:
            aug.extend(vs)
    if any(x in qn for x in ["ìœ íŠœë¸Œ","ë¸Œì´ë¡œê·¸","í‹±í†¡","ì¸ìŠ¤íƒ€","ë¼ì´ë¸Œ","ì‡¼ì¸ "]):
        aug.extend(["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ê²¸ì—…í—ˆê°€", "ì œ82ì¡°", "ì¸í„°ë„· ê°œì¸ë°©ì†¡ í™œë™ ì œí•œ"])
    aug = list(dict.fromkeys([v for v in aug if v not in q]))  # ì¤‘ë³µ ì œê±°
    return q if not aug else f"{q} " + " ".join(aug)

def _augment_with_prf(vectorstore, q: str, k_init: int = 4, kw_top: int = 6) -> str:
    try:
        hits = vectorstore.similarity_search_with_score(q, k=k_init)
        if not hits:
            return q
        blob = "\n".join([d.page_content for d,_ in hits])
        kws = _extract_keywords(blob, top_k=kw_top)
        kws = [w for w in kws if w not in q]
        return f"{q} " + " ".join(kws) if kws else q
    except Exception:
        return q

def expand_query(vectorstore, query: str) -> str:
    q1 = _expand_query_base(query)
    q2 = _augment_with_prf(vectorstore, q1)
    if q2 != query:
        print(f"ğŸ§© ì¿¼ë¦¬ í™•ì¥: '{query}' â†’ '{q2}'")
    return q2

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM Multi-Query ì¬ì‘ì„± (ë³´ìˆ˜í™”)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _should_multiquery(q: str) -> bool:
    return ENABLE_LLM_MULTIQUERY and len(q) <= SHORT_QUERY_CHAR

def _llm_rewrites(llm: ChatOpenAI, query: str, n: int) -> List[str]:
    if not _should_multiquery(query) or n <= 0:
        return []
    try:
        prompt = (
            "ë‹¤ìŒ ì§ˆë¬¸ì„ ë¬¸ì„œ ê²€ìƒ‰ì— ê°•ê±´í•œ í˜•íƒœë¡œ {n}ê°œ ì¬ì‘ì„±í•˜ì„¸ìš”.\n"
            "- í•µì‹¬ í‚¤ì›Œë“œëŠ” ìœ ì§€í•˜ë˜ í‘œí˜„ì„ ë‹¤ì–‘í™”(ë™ì˜ì–´/ì „ë¬¸ìš©ì–´)\n"
            "- í•œêµ­ì–´ ê³µì‹ ë¬¸ì„œ ìš©ì–´ ì„ í˜¸\n"
            "- í•œ ì¤„ì— í•˜ë‚˜ì”© ì¶œë ¥"
        ).format(n=n)
        txt = llm.invoke(prompt + "\n\nì§ˆë¬¸: " + query).content
        rewrites = [line.strip("-â€¢ ").strip() for line in txt.splitlines() if line.strip()]
        uniq, seen = [], set()
        for r in rewrites:
            k = r.lower()
            if k not in seen:
                seen.add(k); uniq.append(r)
        return uniq[:n]
    except Exception:
        return []

def _multi_query_dense(vectorstore, base_q: str, llm: ChatOpenAI, top_each: int) -> List[Tuple[Document, float]]:
    qs = [base_q]
    qs.extend(_llm_rewrites(llm, base_q, N_QUERY_REWRITES))
    qs = list(dict.fromkeys([q for q in qs if q]))
    pool: List[Tuple[Document, float]] = []
    for i, q in enumerate(qs):
        results = vectorstore.similarity_search_with_score(q, k=top_each)
        weight = 2.0 if i == 0 else 1.0  # ì›ë³¸ ì¿¼ë¦¬ ê°€ì¤‘
        pool.extend([(doc, dist / weight) for doc, dist in results])
    return _rrf_fusion(pool, [], k_rrf=60, top_k=min(6, len(qs) * top_each))  # ì†Œê·œëª¨ RRF

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# S3 ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _s3_configured() -> bool:
    return all(os.getenv(k) for k in ["S3_BUCKET", "S3_REGION", "AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"])

def _download_sparse_from_s3(local_docs_fp: str, local_tfidf_fp: str, collection_name: str):
    if not _s3_configured():
        return False
    try:
        import boto3
        s3 = boto3.client(
            "s3",
            region_name=os.getenv("S3_REGION"),
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        )
        prefix = os.getenv("AWS_S3_PREFIX", "rag/sparse")
        key_docs = f"{prefix}/{collection_name}_docs.joblib"
        key_tfidf = f"{prefix}/{collection_name}_tfidf.joblib"
        bucket = os.getenv("S3_BUCKET")
        s3.download_file(bucket, key_docs, local_docs_fp)
        s3.download_file(bucket, key_tfidf, local_tfidf_fp)
        print(f"â˜ï¸  S3ì—ì„œ í¬ì†Œ ì¸ë±ìŠ¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ â†’ {local_docs_fp}, {local_tfidf_fp}")
        return True
    except Exception as e:
        print(f"âš ï¸ S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨(í¬ì†Œ ê²€ìƒ‰ ë¹„í™œì„±): {e}")
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¬ì†Œ ê²€ìƒ‰ê¸°(TF-IDF)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class _SparseSearcher:
    def __init__(self, base_dir: str, collection_name: str):
        os.makedirs(base_dir, exist_ok=True)
        self.collection_name = collection_name
        self.data_fp = os.path.join(base_dir, f"{collection_name}_docs.joblib")
        self.index_fp = os.path.join(base_dir, f"{collection_name}_tfidf.joblib")
        self.enabled = joblib is not None and cosine_similarity is not None
        self.loaded = False
        self.ids = []; self.docs = []; self.metas = []
        self.vectorizer = None; self.mat = None

    def _ensure_local(self):
        if os.path.exists(self.data_fp) and os.path.exists(self.index_fp):
            return
        _download_sparse_from_s3(self.data_fp, self.index_fp, self.collection_name)

    def _lazy_load(self):
        if self.loaded or not self.enabled:
            return
        self._ensure_local()
        if not (os.path.exists(self.data_fp) and os.path.exists(self.index_fp)):
            self.enabled = False
            print("â„¹ï¸ TF-IDF ë¯¸í™œì„±í™”(ë¡œì»¬ .joblib ì—†ìŒ) â†’ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©.")
            return
        self.ids, self.docs, self.metas = joblib.load(self.data_fp)
        self.vectorizer, self.mat = joblib.load(self.index_fp)
        self.loaded = True
        print(f"âœ… TF-IDF í™œì„±í™”: {self.collection_name} (docs={len(self.ids)})")

    def search(self, query: str, k: int = 12) -> List[Tuple[Document, float]]:
        if not self.enabled:
            return []
        self._lazy_load()
        if not self.loaded:
            return []
        qv = self.vectorizer.transform([query])
        sims = cosine_similarity(qv, self.mat).ravel()
        idxs = sims.argsort()[::-1][:k]
        out = []
        for i in idxs:
            out.append((Document(page_content=self.docs[i], metadata=self.metas[i]), float(1 - sims[i])))
        return out

_sparse = _SparseSearcher(SPARSE_INDEX_PATH, COLLECTION_NAME)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RRF ê²°í•©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _rrf_fusion(
    dense: List[Tuple[Document, float]],
    sparse: List[Tuple[Document, float]],
    k_rrf: int = 60,
    top_k: int = 6,
):
    """
    ì—¬ëŸ¬ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸(dense/sparse)ë¥¼ RRFë¡œ ê²°í•©.
    - meta['rrf_score'] : ì›ì‹œ RRF ëˆ„ì ê°’
    - meta['rrf_sim']   : 0~1 ì •ê·œí™” ìœ ì‚¬ë„
    - ë°˜í™˜ distance     : 1 - rrf_sim (ì‘ì„ìˆ˜ë¡ ê°€ê¹ë‹¤)
    """
    pools: Dict[str, Dict[str, Any]] = {}
    for rank, (doc, _) in enumerate(dense, start=1):
        key = (doc.metadata or {}).get("chunk_hash") or f"d:{rank}:{hash(doc.page_content)%10_000_000}"
        pools.setdefault(key, {"doc": doc, "rrf": 0.0})
        pools[key]["rrf"] += 1.0 / (k_rrf + rank)
    for rank, (doc, _) in enumerate(sparse, start=1):
        key = (doc.metadata or {}).get("chunk_hash") or f"s:{rank}:{hash(doc.page_content)%10_000_000}"
        pools.setdefault(key, {"doc": doc, "rrf": 0.0})
        pools[key]["rrf"] += 1.0 / (k_rrf + rank)

    fused = sorted(pools.values(), key=lambda x: x["rrf"], reverse=True)[:top_k]
    max_rrf = max((f["rrf"] for f in fused), default=1.0)

    out: List[Tuple[Document, float]] = []
    for f in fused:
        doc = f["doc"]
        rrf = f["rrf"]
        rrf_sim = rrf / max_rrf if max_rrf > 0 else 0.0
        dist = 1.0 - rrf_sim
        meta = dict(doc.metadata or {})
        meta["rrf_score"] = rrf
        meta["rrf_sim"] = rrf_sim
        doc.metadata = meta
        out.append((doc, dist))
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„ê³„ê°’/ì¿¼í„°/ë¦¬ë­ì»¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _parse_kind_quota(s: str) -> Dict[str, int]:
    out: Dict[str, int] = {}
    for tok in s.split(","):
        if ":" in tok:
            k, v = tok.split(":", 1)
            try:
                out[k.strip()] = int(v)
            except:
                pass
    return out

_KIND_QUOTA = _parse_kind_quota(KIND_QUOTA)
_cross_encoder = None

def _get_cross_encoder():
    global _cross_encoder
    if _cross_encoder is None and CrossEncoder and ENABLE_RERANK:
        try:
            _cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL)
            print(f"âœ… Cross-encoder ë¡œë“œ: {CROSS_ENCODER_MODEL}")
        except Exception as e:
            print(f"âš ï¸ Cross-encoder ë¡œë“œ ì‹¤íŒ¨: {e}")
    return _cross_encoder

def _adaptive_threshold(sims: List[float], base: float) -> float:
    if not sims:
        return base
    xs = sorted(sims)
    q30 = xs[int(len(xs)*0.30)]
    thr = max(base, min(q30, 0.35))  # cap 0.35
    return thr

def _apply_threshold_and_quota(pairs: List[Tuple[Document, float]], top_k: int) -> List[Tuple[Document, float]]:
    """
    - rrf_sim(ê°€ëŠ¥ ì‹œ) ê¸°ë°˜ ì„ê³„ê°’ ì»·, ì—†ìœ¼ë©´ distanceâ†’_stable_similarity ë°±ì—….
    - ë¶„í¬ ê¸°ë°˜ ì ì‘í˜• ì„ê³„ê°’ ì‚¬ìš©(í•˜í•œ: SIM_THRESHOLD).
    - source/kind ì¿¼í„° ì ìš©.
    - ì»· ì‚¬ìœ  ì¹´ìš´íŠ¸ ë¡œê¹….
    """
    scored_meta = []
    for doc, dist in pairs:
        meta = doc.metadata or {}
        sim = meta.get("rrf_sim", _stable_similarity(dist))
        scored_meta.append((doc, dist, sim))

    sims = [s for _, _, s in scored_meta]
    thr = _adaptive_threshold(sims, SIM_THRESHOLD)

    scored = []
    dropped_reason_counts = {"threshold": 0, "doc_quota": 0, "kind_quota": 0}

    for doc, dist, sim in scored_meta:
        if sim >= thr:
            scored.append((doc, dist, sim))
        else:
            dropped_reason_counts["threshold"] += 1

    by_src: Dict[str, int] = {}
    by_kind: Dict[str, int] = {}
    kept: List[Tuple[Document, float]] = []

    for doc, dist, sim in sorted(scored, key=lambda x: x[2], reverse=True):
        meta = doc.metadata or {}
        src = meta.get("source", "unknown")
        kind = meta.get("content_kind") or meta.get("content_type") or "text"

        if by_src.get(src, 0) >= DOC_QUOTA:
            dropped_reason_counts["doc_quota"] += 1
            continue
        if _KIND_QUOTA.get(kind, 10**9) <= by_kind.get(kind, 0):
            dropped_reason_counts["kind_quota"] += 1
            continue

        kept.append((doc, dist))
        by_src[src] = by_src.get(src, 0) + 1
        by_kind[kind] = by_kind.get(kind, 0) + 1
        if len(kept) >= top_k:
            break

    print(f"ì„ê³„/ì¿¼í„° ì ìš© ê²°ê³¼: thr={thr:.3f}, kept={len(kept)}, dropped={dropped_reason_counts}")
    return kept

def _clip(txt: str, max_chars: int) -> str:
    return txt if len(txt) <= max_chars else txt[:max_chars]

def _rerank(query: str, pairs: List[Tuple[Document, float]], top_k: int) -> List[Tuple[Document, float]]:
    # MultiBERTëŠ” ë‹¤êµ­ì–´ ì§€ì› â†’ í•œêµ­ì–´ í¬í•¨ ì§ˆì˜ë„ CE ì ìš©
    ce = _get_cross_encoder()
    if not ce or not pairs:
        return pairs[:top_k]
    try:
        texts = [(query, _clip(d.page_content, CE_MAX_CHARS)) for d, _ in pairs]
        # sentence-transformers CrossEncoderëŠ” batch_size ì¸ìë¥¼ ì§€ì›
        scores = ce.predict(texts, batch_size=CE_BATCH_SIZE)
        ranked = sorted(zip(pairs, scores), key=lambda x: x[1], reverse=True)[:top_k]
        return [p for (p, _) in ranked]
    except Exception as e:
        print(f"âš ï¸ Cross-encoder ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        return pairs[:top_k]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ” ì •í™• ë¬¸ìì—´ ë¶€ìŠ¤íŒ… (ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _boost_exact_match(query: str, pairs: List[Tuple[Document, float]], factor: float = 0.85) -> List[Tuple[Document, float]]:
    """
    ì¿¼ë¦¬(ì •í™• ë¬¸ìì—´)ê°€ ì²­í¬ ë³¸ë¬¸ì— í¬í•¨ë˜ë©´ ê±°ë¦¬ë¥¼ factor ë°°ë¡œ ì¤„ì—¬(=ìœ ì‚¬ë„â†‘) ìƒìœ„ë¡œ ëŒì–´ì˜¬ë¦¼.
    - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ ë§¤ì¹­ë„ í¬í•¨
    """
    q = (query or "").strip()
    if not q or not pairs:
        return pairs
    q_low = q.lower()
    boosted: List[Tuple[Document, float]] = []
    for doc, dist in pairs:
        txt = doc.page_content or ""
        if (q in txt) or (q_low in txt.lower()):
            dist *= factor
        boosted.append((doc, dist))
    return boosted

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„œë¹„ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class RetrieveService:
    def __init__(self):
        self.vectorstore = _vectorstore
        self.sparse = _SparseSearcher(SPARSE_INDEX_PATH, COLLECTION_NAME)
        self.llm = _llm
        self.prompt = _prompt
        self.parser = _parser

    def retrieve_documents(self, query: str, top_k: int = 6) -> List[Tuple[str, dict]]:
        try:
            # ì¿¼ë¦¬ í™•ì¥
            q_exp = expand_query(self.vectorstore, query)
            print(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰(í•˜ì´ë¸Œë¦¬ë“œ RRF): '{q_exp}'")

            # dense: LLM multi-query ì¬ì‘ì„±(ë³´ìˆ˜í™”) â†’ ì†Œê·œëª¨ RRF ìœµí•©
            dense: List[Tuple[Document, float]] = _multi_query_dense(
                self.vectorstore, q_exp, self.llm, top_each=max(3, top_k // 2)
            )

            # sparse
            sparse: List[Tuple[Document, float]] = self.sparse.search(q_exp, k=max(12, top_k * 2))

            # 1) RRF ê²°í•© (í›„ë³´êµ° ì¶©ë¶„ í™•ë³´)
            fused = _rrf_fusion(dense, sparse, k_rrf=60, top_k=max(top_k * 3, 20))

            # 1-1) í˜ì´ì§€/ì¶œì²˜ ì¤‘ë³µ ì œê±°(ë³´ìˆ˜ì )
            fused = _dedup_by_source_page(fused)

            # 2) ì„ê³„ + ì¿¼í„°(ì ì‘í˜• ì»·)
            fused = _apply_threshold_and_quota(fused, top_k=max(top_k * 2, 20))

            # 2-1) ğŸ” ì •í™• ë¬¸ìì—´ ë¶€ìŠ¤íŒ…
            fused = _boost_exact_match(query, fused)

            # 3) CE ë¦¬ë­í¬ â†’ ìµœì¢… top_k
            fused = _rerank(query, fused, top_k=top_k)

            if not fused:
                print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []

            contexts: List[Tuple[str, dict]] = []
            print(f"âœ… ìµœì¢… ìƒìœ„ {len(fused)}ê°œ ê²°ê³¼")
            for i, (doc, distance) in enumerate(fused, start=1):
                meta = dict(doc.metadata or {})
                sim = meta.get("rrf_sim", _stable_similarity(distance))
                meta["similarity_score"] = sim
                preview = (doc.page_content[:100] + "...") if len(doc.page_content) > 100 else doc.page_content
                print(
                    f"  [Rank {i}] sim={sim:.4f} src={meta.get('source')} "
                    f"sheet={meta.get('sheet')} page={meta.get('page')} "
                    f"chunk={meta.get('chunk_idx')} kind={meta.get('content_kind') or meta.get('content_type')}"
                )
                print(f"          ë‚´ìš©: {preview}")
                contexts.append((doc.page_content, meta))
            return contexts
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def generate_answer(self, query: str, contexts: List[Tuple[str, dict]], model: str | None = None) -> str:
        if not contexts:
            return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
        try:
            def _rank_key(item):
                _, meta = item
                kind = (meta.get("content_kind") or meta.get("content_type") or "text")
                pri = 2
                if kind in ("image_vlm",): pri = 0
                if kind in ("image_ocr", "text"): pri = 3
                return (pri, meta.get("similarity_score", 0.0))

            contexts = sorted(contexts, key=_rank_key, reverse=True)

            model_label = model or getattr(self.llm, "model", getattr(self.llm, "model_name", "unknown"))
            print(f"âš™ï¸ ë‹µë³€ ìƒì„± ì¤‘... (ì»¨í…ìŠ¤íŠ¸ {len(contexts)}ê°œ, ëª¨ë¸: {model_label})")
            context_text = _truncate_context_blocks(contexts, max_chars=MAX_CONTEXT_CHARS)
            # ë¸”ë¡ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°(3íšŒì§¸ë¶€í„° ì œê±°)
            context_text = _dedup_sentences_blockwise(context_text)

            used_llm = self.llm if model is None else ChatOpenAI(model=model, temperature=0)
            chain = self.prompt | used_llm | self.parser
            answer: str = chain.invoke({"question": query, "context": context_text})
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            return answer
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def query_rag(self, query: str, top_k: int = 6, model: str | None = None, show_sources: bool = True) -> str:
        print(f"\nğŸ” ì§ˆì˜: {query}")
        contexts = self.retrieve_documents(query, top_k)
        if not contexts:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ë¶€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        answer = self.generate_answer(query, contexts, model)
        if show_sources:
            sources = []
            for _, meta in contexts:
                src = meta.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
                sheet = meta.get('sheet'); page = meta.get('page')
                chunk = meta.get('chunk_idx', 'N/A')
                kind = meta.get('content_kind') or meta.get('content_type')
                tag = f"- {src}"
                if sheet: tag += f" / ì‹œíŠ¸ {sheet}"
                if page: tag += f" / í˜ì´ì§€ {page}"
                tag += f" (ì²­í¬ {chunk}, ìœ í˜• {kind})"
                if tag not in sources:
                    sources.append(tag)
            return f"{answer}\n\nğŸ“‹ ì°¸ê³ í•œ ë¬¸ì„œ:\n" + "\n".join(sources)
        return answer

    def interactive_mode(self):
        print("\nğŸ¯ ëŒ€í™”í˜• RAG ê²€ìƒ‰ ì‹œì‘! (ì¢…ë£Œ: ë¹ˆ ì¤„)")
        print("-" * 60)
        while True:
            try:
                q = input("\n> ").strip()
                if not q:
                    print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break
                print("\n=== ë‹µë³€ ===")
                print(self.query_rag(q))
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")

_retrieve_service = RetrieveService()

@tool
def rag_search_tool(query: str) -> str:
    """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ê²€ìƒ‰/ë‹µë³€ (PDF/DOCX/XLSX ë“±)."""
    print(f"\nğŸ“š RAG ë„êµ¬ ì‹¤í–‰: '{query}'")
    return _retrieve_service.query_rag(query, top_k=6)

rag_tools = [rag_search_tool]

def retrieve_documents(query: str, top_k: int = 6) -> List[Tuple[str, dict]]:
    return RetrieveService().retrieve_documents(query, top_k)

def generate_answer(query: str, contexts: List[Tuple[str, dict]], model: str | None = None) -> str:
    return RetrieveService().generate_answer(query, contexts, model)

def query_rag(query: str, top_k: int = 6, model: str | None = None) -> str:
    return RetrieveService().query_rag(query, top_k, model)

def _healthcheck_vectorstore() -> bool:
    try:
        _ = _vectorstore.similarity_search("__healthcheck__", k=1)
        return True
    except Exception as e:
        print(f"âŒ Chroma í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
        return False

def main():
    print("=" * 80)
    print("ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì„œë¹„ìŠ¤ (í•˜ì´ë¸Œë¦¬ë“œ RRF)")
    print("=" * 80)
    print(f"ğŸ“ CHROMA_PATH: {CHROMA_PATH}")
    print(f"ğŸ—„ï¸ COLLECTION_NAME: {COLLECTION_NAME}")
    print(f"ğŸ”¤ EMBED_MODEL: {EMBED_MODEL}")
    print(f"ğŸ§  CHAT_MODEL: {CHAT_MODEL}")
    print(f"ğŸ§» MAX_CONTEXT_CHARS: {MAX_CONTEXT_CHARS}")
    print(f"ğŸ“¦ SPARSE_INDEX_PATH: {SPARSE_INDEX_PATH}")
    print(f"âš™ï¸ SIM_THRESHOLD={SIM_THRESHOLD}, ENABLE_RERANK={ENABLE_RERANK}, KIND_QUOTA='{KIND_QUOTA}', "
          f"N_QUERY_REWRITES={N_QUERY_REWRITES}, MAX_PER_PAGE={MAX_PER_PAGE}, "
          f"CE_MAX_CHARS={CE_MAX_CHARS}, CE_BATCH_SIZE={CE_BATCH_SIZE}")

    if not _healthcheck_vectorstore():
        print("ë¨¼ì € ingest íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì ì¬í•˜ì„¸ìš”.")
        return
    RetrieveService().interactive_mode()

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ê°„ë‹¨ ì§ˆì˜ 3ê°œ ì‹¤í–‰")
        for q in ["ê¸°ë¡ë¬¼ ê´€ë¦¬", "ê´€ë¦¬ê¸°ì¤€í‘œê°€ ë­ì•¼?", "ì•¼ê°„ ë° íœ´ì¼ê·¼ë¡œ ê´€ë ¨ ê·œì • ì•Œë ¤ì¤˜"]:
            print("\nQ:", q)
            print(query_rag(q))
            print("-" * 60)
    else:
        main()


# # app/rag/internal_data_rag/internal_retrieve.py
# # -*- coding: utf-8 -*-
# # RAG ê²€ìƒ‰ & ë‹µë³€ (í•˜ì´ë¸Œë¦¬ë“œ RRF: ë²¡í„° + TF-IDF) + ì‹œíŠ¸/í˜ì´ì§€/ìœ í˜• í‘œê¸°
# #
# # - TF-IDF í¬ì†Œ ê²€ìƒ‰ ë¡œë”(_SparseSearcher) + RRF ê²°í•©(_rrf_fusion)
# # - í¬ì†Œ ì¸ë±ìŠ¤ê°€ ë¡œì»¬ì— ì—†ìœ¼ë©´ S3ì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ í›„ ìºì‹œ
# # - generate_answer(): content_kind ìš°ì„ ìˆœìœ„(í…ìŠ¤íŠ¸/ocr > vlm)
# # - ì§ˆì˜ í™•ì¥(ë™ì˜ì–´/í”Œë«í¼ ì •ê·œí™” + PRF) â†’ dense/sparseì— ë™ì¼ ì ìš©
# #
# # (ì¶”ê°€: ìµœì†Œìˆ˜ì •)
# # â‘  ìœ ì‚¬ë„ ì„ê³„ê°’ ì»· + ë¬¸ì„œ/ìœ í˜•ë³„ ì¿¼í„°ë§(_apply_threshold_and_quota)
# # â‘¡ Cross-Encoder ì¬ì •ë ¬(_rerank) â€” ENABLE_RERANKë¡œ on/off
# #
# # â›ï¸ ë²„ê·¸í”½ìŠ¤(í•µì‹¬): _rrf_fusionì´ distanceë¥¼ ì˜ëª» ê³„ì‚°í•˜ë˜ ë¬¸ì œ ìˆ˜ì •
# #     - meta['rrf_sim'] = RRF ì ìˆ˜ì˜ 0~1 ì •ê·œí™” ê°’
# #     - ë°˜í™˜ distance = 1 - rrf_sim  (ì‘ì„ìˆ˜ë¡ ê°€ê¹ë‹¤)
# #     - ì„ê³„/ì¶œë ¥ ìœ ì‚¬ë„ì—ì„œ rrf_sim ìš°ì„  ì‚¬ìš©
# #
# # â–³ ê°œì„ (ì •ë°€ë„ ë³´ìˆ˜ì  íšŒë³µ í¬í•¨):
# # - (A) í˜ì´ì§€/ì¶œì²˜ ë””ë“‘ ì™„í™”: page ì—†ìœ¼ë©´ ë””ë“‘ X, (source,page,sheet) ê¸°ì¤€ í˜ì´ì§€ë‹¹ ìµœëŒ€ Nê°œ í—ˆìš©
# # - (B) LLM Multi-Query: ì§§ì€ ì§ˆì˜ì—ë§Œ 1íšŒ ì¬ì‘ì„±(ê¸°ë³¸) â†’ query drift ì–µì œ
# # - (C) ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µ ì œê±°: ë¸”ë¡ ë‹¨ìœ„, ë™ì¼ ë¼ì¸ 2íšŒê¹Œì§„ í—ˆìš©(ì¤‘ìš” ë¬¸êµ¬ ë³´ì¡´)
# # - (D) ìŠ¤ì½”ì–´ ë¡œê¹…: ì„ê³„/ë¬¸ì„œ/ìœ í˜• ì¿¼í„° drop ì¹´ìš´íŠ¸ ì¶œë ¥
# # - (E) ì„ê³„ê°’ ì ì‘í™”: rrf_sim ë¶„í¬ ê¸°ë°˜ìœ¼ë¡œ ì»· ë³´ì •(ê¸°ë³¸ê°’ì€ í•˜í•œì„ )
# # - (F) CE ë¦¬ë­í¬ ê°œì„ : MultiBERT ì‚¬ìš© â†’ í•œêµ­ì–´ í¬í•¨ ì¿¼ë¦¬ë„ CE ì ìš©, ì…ë ¥ ê¸¸ì´/ë°°ì¹˜ íŠœë‹

# import os
# import re
# import json
# import unicodedata
# from pathlib import Path
# from typing import List, Tuple, Dict, Any
# from dotenv import load_dotenv

# from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain_chroma import Chroma
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.tools import tool
# from langchain_core.documents import Document

# # í¬ì†Œ ê²€ìƒ‰(ìˆìœ¼ë©´ ì‚¬ìš©)
# try:
#     from sklearn.metrics.pairwise import cosine_similarity
#     import joblib
# except Exception:
#     cosine_similarity = None
#     joblib = None

# # Cross-encoder(ì„ íƒ)
# try:
#     from sentence_transformers import CrossEncoder
# except Exception:
#     CrossEncoder = None

# load_dotenv()

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # í™˜ê²½ ë³€ìˆ˜
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CHROMA_PATH = os.getenv("CHROMA_PATH", "./chroma_data")
# CHROMA_PATH = os.path.abspath(CHROMA_PATH)
# COLLECTION_NAME = os.getenv("COLLECTION_NAME", "inside_data3")
# EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
# CHAT_MODEL = os.getenv("CHAT_MODEL", "gpt-4o-mini")
# MAX_CONTEXT_CHARS = int(os.getenv("MAX_CONTEXT_CHARS", "12000"))
# SPARSE_INDEX_PATH = os.getenv("SPARSE_INDEX_PATH", os.path.join(CHROMA_PATH, "sparse"))

# # ì„ê³„/ì¿¼í„°/ë¦¬ë­í¬
# SIM_THRESHOLD = float(os.getenv("SIM_THRESHOLD", "0.25"))
# DOC_QUOTA = int(os.getenv("DOC_QUOTA", "15"))
# KIND_QUOTA = os.getenv("KIND_QUOTA", "text:12,image_ocr:8,image_vlm:4")
# ENABLE_RERANK = os.getenv("ENABLE_RERANK", "true").lower() == "true"
# CROSS_ENCODER_MODEL = os.getenv("CROSS_ENCODER_MODEL", "cross-encoder/ms-marco-MultiBERT-L-12")
# # CE ì…ë ¥ ìµœì í™”
# CE_MAX_CHARS = int(os.getenv("CE_MAX_CHARS", "1800"))
# CE_BATCH_SIZE = int(os.getenv("CE_BATCH_SIZE", "16"))

# # LLM Multi-Query (ë³´ìˆ˜í™”)
# ENABLE_LLM_MULTIQUERY = os.getenv("ENABLE_LLM_MULTIQUERY", "true").lower() == "true"
# N_QUERY_REWRITES = int(os.getenv("N_QUERY_REWRITES", "1"))  # ê¸°ë³¸ 1ë¡œ ì™„í™”
# SHORT_QUERY_CHAR = int(os.getenv("SHORT_QUERY_CHAR", "40"))

# # ë””ë“‘ íŒŒë¼ë¯¸í„°
# MAX_PER_PAGE = int(os.getenv("MAX_PER_PAGE", "2"))  # í˜ì´ì§€ë‹¹ ìµœëŒ€ ìœ ì§€ ê°œìˆ˜

# # S3(ì˜µì…˜)
# S3_BUCKET = os.getenv("S3_BUCKET")
# S3_REGION = os.getenv("S3_REGION")
# AWS_S3_PREFIX = os.getenv("AWS_S3_PREFIX", "rag/sparse")

# # ì§ˆì˜ ë™ì˜ì–´ ì‚¬ì „(ì™¸ë¶€ íŒŒì¼ ê²½ë¡œ - ì„ íƒ)
# QUERY_SYNONYM_PATH = os.getenv("QUERY_SYNONYM_PATH")  # e.g. ./config/synonyms.json

# # Chroma Cloud
# CHROMA_TENANT = os.getenv("CHROMA_TENANT")
# CHROMA_DATABASE = os.getenv("CHROMA_DATABASE")
# CHROMA_API_KEY = os.getenv("CHROMA_API_KEY")

# _embeddings = OpenAIEmbeddings(model=EMBED_MODEL)

# # Chroma Cloud í´ë¼ì´ì–¸íŠ¸ + ë²¡í„°ìŠ¤í† ì–´
# import chromadb
# chroma_client = chromadb.CloudClient(
#     tenant=CHROMA_TENANT,
#     database=CHROMA_DATABASE,
#     api_key=CHROMA_API_KEY,
# )
# _vectorstore = Chroma(
#     client=chroma_client,
#     embedding_function=_embeddings,
#     collection_name=COLLECTION_NAME,
# )
# _llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)

# _prompt = ChatPromptTemplate.from_messages(
#     [
#         ("system",
#          "ë‹¹ì‹ ì€ ì‚¬ë‚´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ë‹µí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
#          "ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹µë³€í•˜ê³ , "
#          "ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”. ê°€ëŠ¥í•˜ë‹¤ë©´ ì¶œì²˜(íŒŒì¼/ì‹œíŠ¸/í˜ì´ì§€)ë¥¼ í•¨ê»˜ í‘œì‹œí•˜ì„¸ìš”."),
#         ("user", "ì§ˆë¬¸: {question}\n\nì°¸ê³  ì»¨í…ìŠ¤íŠ¸(ì—¬ëŸ¬ ì²­í¬):\n{context}"),
#     ]
# )
# _parser = StrOutputParser()

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # ìœ í‹¸
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def _stable_similarity(distance: float) -> float:
#     """ë²¡í„°/TF-IDF distance(ì‘ì„ìˆ˜ë¡ ê°€ê¹Œì›€) â†’ 0~1 ìœ ì‚¬ë„."""
#     try:
#         d = float(distance)
#     except Exception:
#         d = 0.0
#     if d < 0:
#         d = 0.0
#     return 1.0 / (1.0 + d)

# def _truncate_context_blocks(blocks: List[Tuple[str, dict]], max_chars: int) -> str:
#     sorted_blocks = sorted(blocks, key=lambda x: float(x[1].get("similarity_score", 0.0)), reverse=True)
#     acc: List[str] = []; total = 0; sep = "\n\n---\n\n"
#     for doc, meta in sorted_blocks:
#         src = meta.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
#         sheet = meta.get('sheet'); page = meta.get('page')
#         chunk = meta.get('chunk_idx', 'N/A')
#         kind = meta.get('content_kind', meta.get('content_type', 'text'))
#         src_tag = f"[ì¶œì²˜: {src}"
#         if sheet: src_tag += f" / ì‹œíŠ¸: {sheet}"
#         if page: src_tag += f" / í˜ì´ì§€: {page}"
#         src_tag += f" / ì²­í¬: {chunk} / ìœ í˜•: {kind}]"
#         block = f"{src_tag}\n{doc}"
#         add_len = len(block) + (len(sep) if acc else 0)
#         if total + add_len > max_chars:
#             break
#         if acc: acc.append(sep); total += len(sep)
#         acc.append(block); total += len(block)
#     return "".join(acc)

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë””ë“‘ (ë³´ìˆ˜ì ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def _dedup_by_source_page(pairs: List[Tuple[Document, float]]) -> List[Tuple[Document, float]]:
#     """
#     (source, page, sheet) ê¸°ì¤€ ë³´ìˆ˜ì  ë””ë“‘.
#     - pageê°€ Noneì´ë©´ ë””ë“‘í•˜ì§€ ì•ŠìŒ(ë©”íƒ€ ë¶€ì‹¤ ë³´í˜¸)
#     - í˜ì´ì§€ë‹¹ ìµœëŒ€ MAX_PER_PAGEê°œê¹Œì§€ í—ˆìš©
#     """
#     counts: Dict[Tuple[Any, Any, Any], int] = {}
#     out = []
#     for doc, dist in pairs:
#         m = doc.metadata or {}
#         src = m.get("source")
#         page = m.get("page")
#         sheet = m.get("sheet")
#         if page is None:
#             out.append((doc, dist))
#             continue
#         key = (src, page, sheet)
#         if counts.get(key, 0) >= MAX_PER_PAGE:
#             continue
#         counts[key] = counts.get(key, 0) + 1
#         out.append((doc, dist))
#     return out

# # ë¸”ë¡ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°(ë™ì¼ ë¼ì¸ 3íšŒì§¸ë¶€í„° ì œê±°)
# def _dedup_sentences_blockwise(text: str) -> str:
#     blocks = [b for b in re.split(r"\n\s*---\s*\n", text) if b.strip()]
#     new_blocks = []
#     for b in blocks:
#         lines = [ln for ln in b.splitlines()]
#         seen: Dict[str, int] = {}
#         acc = []
#         for ln in lines:
#             key = ln.strip().lower()
#             c = seen.get(key, 0) + 1
#             seen[key] = c
#             if c > 2:
#                 continue
#             acc.append(ln)
#         new_blocks.append("\n".join(acc))
#     return "\n\n---\n\n".join(new_blocks)

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # NEW: ì§ˆì˜ í™•ì¥ (ë™ì˜ì–´/ì •ê·œí™” + PRF)
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def _load_synonym_dict() -> dict:
#     try:
#         if QUERY_SYNONYM_PATH and Path(QUERY_SYNONYM_PATH).exists():
#             with open(QUERY_SYNONYM_PATH, "r", encoding="utf-8") as f:
#                 return json.load(f)
#     except Exception:
#         pass
#     return {
#         "ìœ íŠœë¸Œ": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ê°œì¸ë°©ì†¡", "ì˜ìƒ ì—…ë¡œë“œ"],
#         "ë¸Œì´ë¡œê·¸": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ê°œì¸ë°©ì†¡", "ì˜ìƒ ì œì‘", "ë™ì˜ìƒ ì—…ë¡œë“œ"],
#         "í‹±í†¡": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ì†Œì…œë¯¸ë””ì–´"],
#         "ì¸ìŠ¤íƒ€": ["ì†Œì…œë¯¸ë””ì–´", "SNS", "ì½˜í…ì¸  ì—…ë¡œë“œ"],
#         "ë¼ì´ë¸Œ": ["ì‹¤ì‹œê°„ ë°©ì†¡", "ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë°", "ì¸í„°ë„· ê°œì¸ë°©ì†¡"],
#         "ìˆ˜ìµ": ["ê´‘ê³ ìˆ˜ìµ", "ìˆ˜ìµ ì°½ì¶œ", "í˜‘ì°¬", "ê°„ì ‘ê´‘ê³ ", "PPL"],
#         "í˜‘ì°¬": ["ê°„ì ‘ê´‘ê³ ", "PPL", "ëŒ€ê°€ì„± ì œê³µ"],
#         "ê²¸ì—…": ["ê²¸ì—…í—ˆê°€", "ê²¸ì—…ì˜ í—ˆê°€", "ë¶€ì—…"],
#         "ë³´ì•ˆ": ["ì§ë¬´ìƒ ë¹„ë°€", "ì˜ì—…ë¹„ë°€", "ì •ë³´ë³´ì•ˆ"],
#         "ëª…ì˜ˆí›¼ì†": ["ëª…ì˜ˆ ì¹¨í•´", "ê¶Œë¦¬ ì¹¨í•´"],
#         # ì¸ì‚¬/ê²½ë ¥ ê´€ë ¨ ë™ì˜ì–´ ì¶”ê°€
#         "ì´ë ¥ì‚¬": ["ì´ë ¥ì„œ", "ê²½ë ¥", "ê²½í—˜", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„", "ê°œì¸ì •ë³´"],
#         "ì´ë ¥ì„œ": ["ì´ë ¥ì‚¬", "ê²½ë ¥ì„œ", "ê²½ë ¥", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„", "ê°œì¸ì •ë³´"],
#         "ê²½ë ¥": ["ì´ë ¥ì„œ", "ì´ë ¥ì‚¬", "ê²½í—˜", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„"],
#         "ê²½í—˜": ["ì´ë ¥ì„œ", "ì´ë ¥ì‚¬", "ê²½ë ¥", "ì»¤ë¦¬ì–´"],
#     }

# _SYNONYM_DICT = _load_synonym_dict()
# _NORMALIZE_MAP = {
#     "youtube": "ìœ íŠœë¸Œ",
#     "yt": "ìœ íŠœë¸Œ",
#     "vlog": "ë¸Œì´ë¡œê·¸",
#     "shorts": "ì‡¼ì¸ ",
#     "tiktok": "í‹±í†¡",
#     "instagram": "ì¸ìŠ¤íƒ€",
# }

# def _normalize_terms(q: str) -> str:
#     low = q.lower()
#     for a, b in _NORMALIZE_MAP.items():
#         low = re.sub(rf"\b{re.escape(a)}\b", b, low)
#     return low

# _STOP = set(["ë°","ë˜ëŠ”","ê·¸ë¦¬ê³ ","ê´€ë ¨","ê´€ë ¨ëœ","ì—¬ë¶€","ê²ƒ","ìˆ˜","ë“±","í•´ë‹¹","í•˜ëŠ”","ê²½ìš°","ëŒ€í•œ","ìœ¼ë¡œ","ì—ì„œ","í•˜ë‹¤"])
# def _extract_keywords(text: str, top_k: int = 6) -> list:
#     toks = re.split(r"[^ê°€-í£A-Za-z0-9]+", text)
#     cnt: Dict[str, int] = {}
#     for t in toks:
#         if len(t) < 2 or t in _STOP:
#             continue
#         cnt[t] = cnt.get(t, 0) + 1
#     return [w for w,_ in sorted(cnt.items(), key=lambda x: x[1], reverse=True)[:top_k]]

# def _expand_query_base(q: str) -> str:
#     qn = _normalize_terms(q)
#     aug: List[str] = []
#     for k, vs in _SYNONYM_DICT.items():
#         if k in qn:
#             aug.extend(vs)
#     if any(x in qn for x in ["ìœ íŠœë¸Œ","ë¸Œì´ë¡œê·¸","í‹±í†¡","ì¸ìŠ¤íƒ€","ë¼ì´ë¸Œ","ì‡¼ì¸ "]):
#         aug.extend(["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ê²¸ì—…í—ˆê°€", "ì œ82ì¡°", "ì¸í„°ë„· ê°œì¸ë°©ì†¡ í™œë™ ì œí•œ"])
#     aug = list(dict.fromkeys([v for v in aug if v not in q]))  # ì¤‘ë³µ ì œê±°
#     return q if not aug else f"{q} " + " ".join(aug)

# def _augment_with_prf(vectorstore, q: str, k_init: int = 4, kw_top: int = 6) -> str:
#     try:
#         hits = vectorstore.similarity_search_with_score(q, k=k_init)
#         if not hits:
#             return q
#         blob = "\n".join([d.page_content for d,_ in hits])
#         kws = _extract_keywords(blob, top_k=kw_top)
#         kws = [w for w in kws if w not in q]
#         return f"{q} " + " ".join(kws) if kws else q
#     except Exception:
#         return q

# def expand_query(vectorstore, query: str) -> str:
#     q1 = _expand_query_base(query)
#     q2 = _augment_with_prf(vectorstore, q1)
#     if q2 != query:
#         print(f"ğŸ§© ì¿¼ë¦¬ í™•ì¥: '{query}' â†’ '{q2}'")
#     return q2

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # LLM Multi-Query ì¬ì‘ì„± (ë³´ìˆ˜í™”)
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def _should_multiquery(q: str) -> bool:
#     return ENABLE_LLM_MULTIQUERY and len(q) <= SHORT_QUERY_CHAR

# def _llm_rewrites(llm: ChatOpenAI, query: str, n: int) -> List[str]:
#     if not _should_multiquery(query) or n <= 0:
#         return []
#     try:
#         prompt = (
#             "ë‹¤ìŒ ì§ˆë¬¸ì„ ë¬¸ì„œ ê²€ìƒ‰ì— ê°•ê±´í•œ í˜•íƒœë¡œ {n}ê°œ ì¬ì‘ì„±í•˜ì„¸ìš”.\n"
#             "- í•µì‹¬ í‚¤ì›Œë“œëŠ” ìœ ì§€í•˜ë˜ í‘œí˜„ì„ ë‹¤ì–‘í™”(ë™ì˜ì–´/ì „ë¬¸ìš©ì–´)\n"
#             "- í•œêµ­ì–´ ê³µì‹ ë¬¸ì„œ ìš©ì–´ ì„ í˜¸\n"
#             "- í•œ ì¤„ì— í•˜ë‚˜ì”© ì¶œë ¥"
#         ).format(n=n)
#         txt = llm.invoke(prompt + "\n\nì§ˆë¬¸: " + query).content
#         rewrites = [line.strip("-â€¢ ").strip() for line in txt.splitlines() if line.strip()]
#         uniq, seen = [], set()
#         for r in rewrites:
#             k = r.lower()
#             if k not in seen:
#                 seen.add(k); uniq.append(r)
#         return uniq[:n]
#     except Exception:
#         return []

# def _multi_query_dense(vectorstore, base_q: str, llm: ChatOpenAI, top_each: int) -> List[Tuple[Document, float]]:
#     qs = [base_q]
#     qs.extend(_llm_rewrites(llm, base_q, N_QUERY_REWRITES))
#     qs = list(dict.fromkeys([q for q in qs if q]))
#     pool: List[Tuple[Document, float]] = []
#     for i, q in enumerate(qs):
#         results = vectorstore.similarity_search_with_score(q, k=top_each)
#         weight = 2.0 if i == 0 else 1.0  # ì›ë³¸ ì¿¼ë¦¬ ê°€ì¤‘
#         pool.extend([(doc, dist / weight) for doc, dist in results])
#     return _rrf_fusion(pool, [], k_rrf=60, top_k=min(6, len(qs) * top_each))  # ì†Œê·œëª¨ RRF

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # S3 ìœ í‹¸
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def _s3_configured() -> bool:
#     return all(os.getenv(k) for k in ["S3_BUCKET", "S3_REGION", "AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"])

# def _download_sparse_from_s3(local_docs_fp: str, local_tfidf_fp: str, collection_name: str):
#     if not _s3_configured():
#         return False
#     try:
#         import boto3
#         s3 = boto3.client(
#             "s3",
#             region_name=os.getenv("S3_REGION"),
#             aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
#             aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
#         )
#         prefix = os.getenv("AWS_S3_PREFIX", "rag/sparse")
#         key_docs = f"{prefix}/{collection_name}_docs.joblib"
#         key_tfidf = f"{prefix}/{collection_name}_tfidf.joblib"
#         bucket = os.getenv("S3_BUCKET")
#         s3.download_file(bucket, key_docs, local_docs_fp)
#         s3.download_file(bucket, key_tfidf, local_tfidf_fp)
#         print(f"â˜ï¸  S3ì—ì„œ í¬ì†Œ ì¸ë±ìŠ¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ â†’ {local_docs_fp}, {local_tfidf_fp}")
#         return True
#     except Exception as e:
#         print(f"âš ï¸ S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨(í¬ì†Œ ê²€ìƒ‰ ë¹„í™œì„±): {e}")
#         return False

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # í¬ì†Œ ê²€ìƒ‰ê¸°(TF-IDF)
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# class _SparseSearcher:
#     def __init__(self, base_dir: str, collection_name: str):
#         os.makedirs(base_dir, exist_ok=True)
#         self.collection_name = collection_name
#         self.data_fp = os.path.join(base_dir, f"{collection_name}_docs.joblib")
#         self.index_fp = os.path.join(base_dir, f"{collection_name}_tfidf.joblib")
#         self.enabled = joblib is not None and cosine_similarity is not None
#         self.loaded = False
#         self.ids = []; self.docs = []; self.metas = []
#         self.vectorizer = None; self.mat = None

#     def _ensure_local(self):
#         if os.path.exists(self.data_fp) and os.path.exists(self.index_fp):
#             return
#         _download_sparse_from_s3(self.data_fp, self.index_fp, self.collection_name)

#     def _lazy_load(self):
#         if self.loaded or not self.enabled:
#             return
#         self._ensure_local()
#         if not (os.path.exists(self.data_fp) and os.path.exists(self.index_fp)):
#             self.enabled = False
#             print("â„¹ï¸ TF-IDF ë¯¸í™œì„±í™”(ë¡œì»¬ .joblib ì—†ìŒ) â†’ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©.")
#             return
#         self.ids, self.docs, self.metas = joblib.load(self.data_fp)
#         self.vectorizer, self.mat = joblib.load(self.index_fp)
#         self.loaded = True
#         print(f"âœ… TF-IDF í™œì„±í™”: {self.collection_name} (docs={len(self.ids)})")

#     def search(self, query: str, k: int = 12) -> List[Tuple[Document, float]]:
#         if not self.enabled:
#             return []
#         self._lazy_load()
#         if not self.loaded:
#             return []
#         qv = self.vectorizer.transform([query])
#         sims = cosine_similarity(qv, self.mat).ravel()
#         idxs = sims.argsort()[::-1][:k]
#         out = []
#         for i in idxs:
#             out.append((Document(page_content=self.docs[i], metadata=self.metas[i]), float(1 - sims[i])))
#         return out

# _sparse = _SparseSearcher(SPARSE_INDEX_PATH, COLLECTION_NAME)

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # RRF ê²°í•©
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def _rrf_fusion(
#     dense: List[Tuple[Document, float]],
#     sparse: List[Tuple[Document, float]],
#     k_rrf: int = 60,
#     top_k: int = 6,
# ):
#     """
#     ì—¬ëŸ¬ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸(dense/sparse)ë¥¼ RRFë¡œ ê²°í•©.
#     - meta['rrf_score'] : ì›ì‹œ RRF ëˆ„ì ê°’
#     - meta['rrf_sim']   : 0~1 ì •ê·œí™” ìœ ì‚¬ë„
#     - ë°˜í™˜ distance     : 1 - rrf_sim (ì‘ì„ìˆ˜ë¡ ê°€ê¹ë‹¤)
#     """
#     pools: Dict[str, Dict[str, Any]] = {}
#     for rank, (doc, _) in enumerate(dense, start=1):
#         key = (doc.metadata or {}).get("chunk_hash") or f"d:{rank}:{hash(doc.page_content)%10_000_000}"
#         pools.setdefault(key, {"doc": doc, "rrf": 0.0})
#         pools[key]["rrf"] += 1.0 / (k_rrf + rank)
#     for rank, (doc, _) in enumerate(sparse, start=1):
#         key = (doc.metadata or {}).get("chunk_hash") or f"s:{rank}:{hash(doc.page_content)%10_000_000}"
#         pools.setdefault(key, {"doc": doc, "rrf": 0.0})
#         pools[key]["rrf"] += 1.0 / (k_rrf + rank)

#     fused = sorted(pools.values(), key=lambda x: x["rrf"], reverse=True)[:top_k]
#     max_rrf = max((f["rrf"] for f in fused), default=1.0)

#     out: List[Tuple[Document, float]] = []
#     for f in fused:
#         doc = f["doc"]
#         rrf = f["rrf"]
#         rrf_sim = rrf / max_rrf if max_rrf > 0 else 0.0
#         dist = 1.0 - rrf_sim
#         meta = dict(doc.metadata or {})
#         meta["rrf_score"] = rrf
#         meta["rrf_sim"] = rrf_sim
#         doc.metadata = meta
#         out.append((doc, dist))
#     return out

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # ì„ê³„ê°’/ì¿¼í„°/ë¦¬ë­ì»¤
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def _parse_kind_quota(s: str) -> Dict[str, int]:
#     out: Dict[str, int] = {}
#     for tok in s.split(","):
#         if ":" in tok:
#             k, v = tok.split(":", 1)
#             try:
#                 out[k.strip()] = int(v)
#             except:
#                 pass
#     return out

# _KIND_QUOTA = _parse_kind_quota(KIND_QUOTA)
# _cross_encoder = None

# def _get_cross_encoder():
#     global _cross_encoder
#     if _cross_encoder is None and CrossEncoder and ENABLE_RERANK:
#         try:
#             _cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL)
#             print(f"âœ… Cross-encoder ë¡œë“œ: {CROSS_ENCODER_MODEL}")
#         except Exception as e:
#             print(f"âš ï¸ Cross-encoder ë¡œë“œ ì‹¤íŒ¨: {e}")
#     return _cross_encoder

# def _adaptive_threshold(sims: List[float], base: float) -> float:
#     if not sims:
#         return base
#     xs = sorted(sims)
#     q30 = xs[int(len(xs)*0.30)]
#     thr = max(base, min(q30, 0.35))  # cap 0.35
#     return thr

# def _apply_threshold_and_quota(pairs: List[Tuple[Document, float]], top_k: int) -> List[Tuple[Document, float]]:
#     """
#     - rrf_sim(ê°€ëŠ¥ ì‹œ) ê¸°ë°˜ ì„ê³„ê°’ ì»·, ì—†ìœ¼ë©´ distanceâ†’_stable_similarity ë°±ì—….
#     - ë¶„í¬ ê¸°ë°˜ ì ì‘í˜• ì„ê³„ê°’ ì‚¬ìš©(í•˜í•œ: SIM_THRESHOLD).
#     - source/kind ì¿¼í„° ì ìš©.
#     - ì»· ì‚¬ìœ  ì¹´ìš´íŠ¸ ë¡œê¹….
#     """
#     scored_meta = []
#     for doc, dist in pairs:
#         meta = doc.metadata or {}
#         sim = meta.get("rrf_sim", _stable_similarity(dist))
#         scored_meta.append((doc, dist, sim))

#     sims = [s for _, _, s in scored_meta]
#     thr = _adaptive_threshold(sims, SIM_THRESHOLD)

#     scored = []
#     dropped_reason_counts = {"threshold": 0, "doc_quota": 0, "kind_quota": 0}

#     for doc, dist, sim in scored_meta:
#         if sim >= thr:
#             scored.append((doc, dist, sim))
#         else:
#             dropped_reason_counts["threshold"] += 1

#     by_src: Dict[str, int] = {}
#     by_kind: Dict[str, int] = {}
#     kept: List[Tuple[Document, float]] = []

#     for doc, dist, sim in sorted(scored, key=lambda x: x[2], reverse=True):
#         meta = doc.metadata or {}
#         src = meta.get("source", "unknown")
#         kind = meta.get("content_kind") or meta.get("content_type") or "text"

#         if by_src.get(src, 0) >= DOC_QUOTA:
#             dropped_reason_counts["doc_quota"] += 1
#             continue
#         if _KIND_QUOTA.get(kind, 10**9) <= by_kind.get(kind, 0):
#             dropped_reason_counts["kind_quota"] += 1
#             continue

#         kept.append((doc, dist))
#         by_src[src] = by_src.get(src, 0) + 1
#         by_kind[kind] = by_kind.get(kind, 0) + 1
#         if len(kept) >= top_k:
#             break

#     print(f"ì„ê³„/ì¿¼í„° ì ìš© ê²°ê³¼: thr={thr:.3f}, kept={len(kept)}, dropped={dropped_reason_counts}")
#     return kept

# def _clip(txt: str, max_chars: int) -> str:
#     return txt if len(txt) <= max_chars else txt[:max_chars]

# def _rerank(query: str, pairs: List[Tuple[Document, float]], top_k: int) -> List[Tuple[Document, float]]:
#     # MultiBERTëŠ” ë‹¤êµ­ì–´ ì§€ì› â†’ í•œêµ­ì–´ í¬í•¨ ì§ˆì˜ë„ CE ì ìš©
#     ce = _get_cross_encoder()
#     if not ce or not pairs:
#         return pairs[:top_k]
#     try:
#         texts = [(query, _clip(d.page_content, CE_MAX_CHARS)) for d, _ in pairs]
#         # sentence-transformers CrossEncoderëŠ” batch_size ì¸ìë¥¼ ì§€ì›
#         scores = ce.predict(texts, batch_size=CE_BATCH_SIZE)
#         ranked = sorted(zip(pairs, scores), key=lambda x: x[1], reverse=True)[:top_k]
#         return [p for (p, _) in ranked]
#     except Exception as e:
#         print(f"âš ï¸ Cross-encoder ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
#         return pairs[:top_k]

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # ì„œë¹„ìŠ¤
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# class RetrieveService:
#     def __init__(self):
#         self.vectorstore = _vectorstore
#         self.sparse = _SparseSearcher(SPARSE_INDEX_PATH, COLLECTION_NAME)
#         self.llm = _llm
#         self.prompt = _prompt
#         self.parser = _parser

#     def retrieve_documents(self, query: str, top_k: int = 6) -> List[Tuple[str, dict]]:
#         try:
#             # ì¿¼ë¦¬ í™•ì¥
#             q_exp = expand_query(self.vectorstore, query)
#             print(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰(í•˜ì´ë¸Œë¦¬ë“œ RRF): '{q_exp}'")

#             # dense: LLM multi-query ì¬ì‘ì„±(ë³´ìˆ˜í™”) â†’ ì†Œê·œëª¨ RRF ìœµí•©
#             dense: List[Tuple[Document, float]] = _multi_query_dense(
#                 self.vectorstore, q_exp, self.llm, top_each=max(3, top_k // 2)
#             )

#             # sparse
#             sparse: List[Tuple[Document, float]] = self.sparse.search(q_exp, k=max(12, top_k * 2))

#             # 1) RRF ê²°í•© (í›„ë³´êµ° ì¶©ë¶„ í™•ë³´)
#             fused = _rrf_fusion(dense, sparse, k_rrf=60, top_k=max(top_k * 3, 20))

#             # 1-1) í˜ì´ì§€/ì¶œì²˜ ì¤‘ë³µ ì œê±°(ë³´ìˆ˜ì )
#             fused = _dedup_by_source_page(fused)

#             # 2) ì„ê³„ + ì¿¼í„°(ì ì‘í˜• ì»·)
#             fused = _apply_threshold_and_quota(fused, top_k=max(top_k * 2, 20))

#             # 3) CE ë¦¬ë­í¬ â†’ ìµœì¢… top_k
#             fused = _rerank(query, fused, top_k=top_k)

#             if not fused:
#                 print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
#                 return []

#             contexts: List[Tuple[str, dict]] = []
#             print(f"âœ… ìµœì¢… ìƒìœ„ {len(fused)}ê°œ ê²°ê³¼")
#             for i, (doc, distance) in enumerate(fused, start=1):
#                 meta = dict(doc.metadata or {})
#                 sim = meta.get("rrf_sim", _stable_similarity(distance))
#                 meta["similarity_score"] = sim
#                 preview = (doc.page_content[:100] + "...") if len(doc.page_content) > 100 else doc.page_content
#                 print(
#                     f"  [Rank {i}] sim={sim:.4f} src={meta.get('source')} "
#                     f"sheet={meta.get('sheet')} page={meta.get('page')} "
#                     f"chunk={meta.get('chunk_idx')} kind={meta.get('content_kind') or meta.get('content_type')}"
#                 )
#                 print(f"          ë‚´ìš©: {preview}")
#                 contexts.append((doc.page_content, meta))
#             return contexts
#         except Exception as e:
#             print(f"âŒ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#             return []

#     def generate_answer(self, query: str, contexts: List[Tuple[str, dict]], model: str | None = None) -> str:
#         if not contexts:
#             return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
#         try:
#             def _rank_key(item):
#                 _, meta = item
#                 kind = (meta.get("content_kind") or meta.get("content_type") or "text")
#                 pri = 2
#                 if kind in ("image_vlm",): pri = 0
#                 if kind in ("image_ocr", "text"): pri = 3
#                 return (pri, meta.get("similarity_score", 0.0))

#             contexts = sorted(contexts, key=_rank_key, reverse=True)

#             model_label = model or getattr(self.llm, "model", getattr(self.llm, "model_name", "unknown"))
#             print(f"âš™ï¸ ë‹µë³€ ìƒì„± ì¤‘... (ì»¨í…ìŠ¤íŠ¸ {len(contexts)}ê°œ, ëª¨ë¸: {model_label})")
#             context_text = _truncate_context_blocks(contexts, max_chars=MAX_CONTEXT_CHARS)
#             # ë¸”ë¡ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°(3íšŒì§¸ë¶€í„° ì œê±°)
#             context_text = _dedup_sentences_blockwise(context_text)

#             used_llm = self.llm if model is None else ChatOpenAI(model=model, temperature=0)
#             chain = self.prompt | used_llm | self.parser
#             answer: str = chain.invoke({"question": query, "context": context_text})
#             print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
#             return answer
#         except Exception as e:
#             print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#             return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

#     def query_rag(self, query: str, top_k: int = 6, model: str | None = None, show_sources: bool = True) -> str:
#         print(f"\nğŸ” ì§ˆì˜: {query}")
#         contexts = self.retrieve_documents(query, top_k)
#         if not contexts:
#             return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ë¶€ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#         answer = self.generate_answer(query, contexts, model)
#         if show_sources:
#             sources = []
#             for _, meta in contexts:
#                 src = meta.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
#                 sheet = meta.get('sheet'); page = meta.get('page')
#                 chunk = meta.get('chunk_idx', 'N/A')
#                 kind = meta.get('content_kind') or meta.get('content_type')
#                 tag = f"- {src}"
#                 if sheet: tag += f" / ì‹œíŠ¸ {sheet}"
#                 if page: tag += f" / í˜ì´ì§€ {page}"
#                 tag += f" (ì²­í¬ {chunk}, ìœ í˜• {kind})"
#                 if tag not in sources:
#                     sources.append(tag)
#             return f"{answer}\n\nğŸ“‹ ì°¸ê³ í•œ ë¬¸ì„œ:\n" + "\n".join(sources)
#         return answer

#     def interactive_mode(self):
#         print("\nğŸ¯ ëŒ€í™”í˜• RAG ê²€ìƒ‰ ì‹œì‘! (ì¢…ë£Œ: ë¹ˆ ì¤„)")
#         print("-" * 60)
#         while True:
#             try:
#                 q = input("\n> ").strip()
#                 if not q:
#                     print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
#                     break
#                 print("\n=== ë‹µë³€ ===")
#                 print(self.query_rag(q))
#             except KeyboardInterrupt:
#                 print("\n\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
#                 break
#             except Exception as e:
#                 print(f"âŒ ì˜¤ë¥˜: {e}")

# _retrieve_service = RetrieveService()

# @tool
# def rag_search_tool(query: str) -> str:
#     """ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ê²€ìƒ‰/ë‹µë³€ (PDF/DOCX/XLSX ë“±)."""
#     print(f"\nğŸ“š RAG ë„êµ¬ ì‹¤í–‰: '{query}'")
#     return _retrieve_service.query_rag(query, top_k=6)

# rag_tools = [rag_search_tool]

# def retrieve_documents(query: str, top_k: int = 6) -> List[Tuple[str, dict]]:
#     return RetrieveService().retrieve_documents(query, top_k)

# def generate_answer(query: str, contexts: List[Tuple[str, dict]], model: str | None = None) -> str:
#     return RetrieveService().generate_answer(query, contexts, model)

# def query_rag(query: str, top_k: int = 6, model: str | None = None) -> str:
#     return RetrieveService().query_rag(query, top_k, model)

# def _healthcheck_vectorstore() -> bool:
#     try:
#         _ = _vectorstore.similarity_search("__healthcheck__", k=1)
#         return True
#     except Exception as e:
#         print(f"âŒ Chroma í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨: {e}")
#         return False

# def main():
#     print("=" * 80)
#     print("ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± ì„œë¹„ìŠ¤ (í•˜ì´ë¸Œë¦¬ë“œ RRF)")
#     print("=" * 80)
#     print(f"ğŸ“ CHROMA_PATH: {CHROMA_PATH}")
#     print(f"ğŸ—„ï¸ COLLECTION_NAME: {COLLECTION_NAME}")
#     print(f"ğŸ”¤ EMBED_MODEL: {EMBED_MODEL}")
#     print(f"ğŸ§  CHAT_MODEL: {CHAT_MODEL}")
#     print(f"ğŸ§» MAX_CONTEXT_CHARS: {MAX_CONTEXT_CHARS}")
#     print(f"ğŸ“¦ SPARSE_INDEX_PATH: {SPARSE_INDEX_PATH}")
#     print(f"âš™ï¸ SIM_THRESHOLD={SIM_THRESHOLD}, ENABLE_RERANK={ENABLE_RERANK}, KIND_QUOTA='{KIND_QUOTA}', "
#           f"N_QUERY_REWRITES={N_QUERY_REWRITES}, MAX_PER_PAGE={MAX_PER_PAGE}, "
#           f"CE_MAX_CHARS={CE_MAX_CHARS}, CE_BATCH_SIZE={CE_BATCH_SIZE}")

#     if not _healthcheck_vectorstore():
#         print("ë¨¼ì € ingest íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì ì¬í•˜ì„¸ìš”.")
#         return
#     RetrieveService().interactive_mode()

# if __name__ == "__main__":
#     import sys
#     if len(sys.argv) > 1 and sys.argv[1] == "--test":
#         print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ê°„ë‹¨ ì§ˆì˜ 3ê°œ ì‹¤í–‰")
#         for q in ["ê¸°ë¡ë¬¼ ê´€ë¦¬", "ê´€ë¦¬ê¸°ì¤€í‘œê°€ ë­ì•¼?", "ì•¼ê°„ ë° íœ´ì¼ê·¼ë¡œ ê´€ë ¨ ê·œì • ì•Œë ¤ì¤˜"]:
#             print("\nQ:", q)
#             print(query_rag(q))
#             print("-" * 60)
#     else:
#         main()
