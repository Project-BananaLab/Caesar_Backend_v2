# app/rag/internal_data_rag/internal_ingest.py
# -*- coding: utf-8 -*-
# ë¬¸ì„œ ì„ë² ë”© ë° ChromaDB ì €ì¥ ì„œë¹„ìŠ¤

import os
import sys
import zipfile
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any

import pdfplumber
import docx
import openpyxl
from dotenv import load_dotenv

import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()

# Chroma/Collection
CHROMA_PATH = os.getenv("CHROMA_PATH", "./chroma_data")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "inside_data")

# ì²­í‚¹ íŒŒë¼ë¯¸í„° (í•„ìš”ì‹œ .envë¡œ ì¡°ì ˆ)
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "1000"))       # ì²­í¬ í¬ê¸°
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "150"))  # ì˜¤ë²„ë©

# ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ ì œí•œ ì„¤ì •
XLSX_MAX_ROWS_PER_SHEET = int(os.getenv("XLSX_MAX_ROWS_PER_SHEET", "10000"))
XLSX_MAX_COLS_PER_SHEET = int(os.getenv("XLSX_MAX_COLS_PER_SHEET", "512"))
XLSX_SKIP_HIDDEN_SHEETS = os.getenv("XLSX_SKIP_HIDDEN_SHEETS", "true").lower() == "true"

# ì„ë² ë”© API ìš”ì²­ ë°°ì¹˜ ì œí•œ
EMBED_MAX_TOKENS_PER_REQUEST = int(os.getenv("EMBED_MAX_TOKENS_PER_REQUEST", "280000"))
EMBED_MAX_ITEMS_PER_REQUEST = int(os.getenv("EMBED_MAX_ITEMS_PER_REQUEST", "256"))

# tiktokenì€ ì„ íƒì 
try:
    import tiktoken
    _TIKTOKEN_ENC = tiktoken.get_encoding("cl100k_base")
except Exception:
    _TIKTOKEN_ENC = None

# OpenAI
client = OpenAI()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸: ì‹¤ì œ Office Open XML í¬ë§· ìŠ¤ë‹ˆí•‘(.docx/.xlsx êµ¬ë¶„)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _detect_office_kind(path: Path) -> Optional[str]:
    """
    ZIP ê¸°ë°˜ Office ë¬¸ì„œì˜ ì‹¤ì œ ì¢…ë¥˜ë¥¼ ì¶”ì •:
      - 'docx'  : word/document.xml ì¡´ì¬
      - 'xlsx'  : xl/workbook.xml ì¡´ì¬
      - None    : ZIP ì•„ë‹˜ ë˜ëŠ” Office OpenXML ì•„ë‹˜
    """
    try:
        if not zipfile.is_zipfile(path):
            return None
        with zipfile.ZipFile(path) as z:
            names = set(z.namelist())
        if any(n.startswith("word/") for n in names):
            return "docx"
        if any(n.startswith("xl/") for n in names):
            return "xlsx"
        return None
    except Exception:
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„ë² ë”© ë°°ì¹˜ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _estimate_tokens(text: str) -> int:
    """ì„ë² ë”© í† í° ëŒ€ëµì¹˜. tiktoken ìˆìœ¼ë©´ ì •í™•, ì—†ìœ¼ë©´ ë¬¸ììˆ˜/4 ê·¼ì‚¬."""
    if _TIKTOKEN_ENC is not None:
        try:
            return len(_TIKTOKEN_ENC.encode(text))
        except Exception:
            pass
    return max(1, len(text) // 4)


def embed_texts_batched(texts: List[str]) -> List[List[float]]:
    """í† í°/ì•„ì´í…œ ì˜ˆì‚°ì„ ì§€ì¼œê°€ë©° ì—¬ëŸ¬ ë²ˆìœ¼ë¡œ ë‚˜ëˆ  ì„ë² ë”©."""
    if not texts:
        return []

    batches: List[List[str]] = []
    current: List[str] = []
    current_tokens = 0

    for t in texts:
        tk = _estimate_tokens(t)

        # ë‹¨ì¼ ì²­í¬ê°€ ì˜ˆì‚°ì„ ë„˜ë”ë¼ë„(ê±°ì˜ ì—†ì§€ë§Œ) ë‹¨ë… ë°°ì¹˜ë¡œ ë³´ëƒ„
        if tk > EMBED_MAX_TOKENS_PER_REQUEST:
            if current:
                batches.append(current)
                current, current_tokens = [], 0
            batches.append([t])
            continue

        if current and (
            current_tokens + tk > EMBED_MAX_TOKENS_PER_REQUEST
            or len(current) >= EMBED_MAX_ITEMS_PER_REQUEST
        ):
            batches.append(current)
            current, current_tokens = [], 0

        current.append(t)
        current_tokens += tk

    if current:
        batches.append(current)

    all_embeddings: List[List[float]] = []
    for i, batch in enumerate(batches, 1):
        print(f"  ğŸ” ì„ë² ë”© ë°°ì¹˜ {i}/{len(batches)} (items={len(batch)}) ìš”ì²­ ì¤‘...")
        resp = client.embeddings.create(
            model="text-embedding-3-small",
            input=batch
        )
        all_embeddings.extend([d.embedding for d in resp.data])

    return all_embeddings


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class IngestService:
    """ë¬¸ì„œ ì„ë² ë”© ë° ChromaDB ì €ì¥ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", " ", ""]
        )
        # ì§€ì›ë˜ëŠ” íŒŒì¼ í™•ì¥ì
        self.supported_extensions = {".pdf", ".docx", ".xlsx", ".csv", ".txt"}

    # ========================= íŒŒì¼ íŒŒì‹± =========================
    def read_pdf(self, path: Path) -> str:  # PDF íŒŒì¼ íŒŒì‹±
        texts = []
        try:
            with pdfplumber.open(str(path)) as pdf:
                for page in pdf.pages:
                    t = page.extract_text() or ""
                    if t.strip():
                        texts.append(t)
        except Exception as e:
            raise ValueError(f"PDF ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        return "\n\n".join(texts)

    def read_docx(self, path: Path) -> str:  # DOCX íŒŒì¼ íŒŒì‹±
        try:
            d = docx.Document(str(path))
        except Exception as e:
            raise ValueError(f"DOCX ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        acc: List[str] = []
        acc.extend([p.text for p in d.paragraphs if p.text and p.text.strip()])
        # í…Œì´ë¸” ì¶”ì¶œ(ê°„ë‹¨)
        for table in d.tables:
            for row in table.rows:
                cells = [c.text.strip() for c in row.cells]
                if any(cells):
                    acc.append(" | ".join(cells))
        return "\n".join(acc)

    def read_xlsx(self, path: Path) -> str:  # XLSX íŒŒì¼ íŒŒì‹± (í­ì£¼ ë°©ì§€ íŠ¸ë¦¬ë°/ìº¡ ì ìš©)
        wb = None
        try:
            wb = openpyxl.load_workbook(str(path), data_only=True, read_only=True)
            
            if not wb.worksheets:
                raise ValueError("ì—‘ì…€ì— ì›Œí¬ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

            acc: List[str] = []
            for ws in wb.worksheets:
                # ìˆ¨ê¹€ ì‹œíŠ¸ ìŠ¤í‚µ ì˜µì…˜
                try:
                    if XLSX_SKIP_HIDDEN_SHEETS and getattr(ws, "sheet_state", "visible") != "visible":
                        continue
                except Exception:
                    pass

                acc.append(f"\n### [Sheet] {ws.title}")
                rows = 0

                # ì—´ ìƒí•œ ìº¡ì„ openpyxl ë ˆë²¨ì—ì„œ ë°”ë¡œ ì ìš©
                iter_kwargs = {"values_only": True}
                if XLSX_MAX_COLS_PER_SHEET and XLSX_MAX_COLS_PER_SHEET > 0:
                    iter_kwargs["max_col"] = XLSX_MAX_COLS_PER_SHEET

                for row in ws.iter_rows(**iter_kwargs):
                    if rows >= XLSX_MAX_ROWS_PER_SHEET:
                        acc.append(f"...(truncated at {XLSX_MAX_ROWS_PER_SHEET} rows)")
                        break

                    # í–‰ ìš°ì¸¡ì˜ ë¹ˆ ì—´ íŠ¸ë¦¬ë°: ì‹¤ì œ ê°’ì´ ìˆëŠ” ë§ˆì§€ë§‰ ì—´ê¹Œì§€ë§Œ ì‚¬ìš©
                    last = -1
                    # (ì—´ ìº¡ì´ ì ìš©ëœ ë²”ìœ„ ë‚´ì—ì„œë§Œ ê²€ì‚¬)
                    for i, v in enumerate(row):
                        sv = (str(v).strip() if v is not None else "")
                        if sv != "":
                            last = i

                    if last < 0:
                        continue  # ì™„ì „ ë¹ˆ í–‰ì€ ìŠ¤í‚µ

                    # ìµœì¢… ì‚¬ìš©í•  ì—´ í­ ê²°ì •
                    width = last + 1
                    if XLSX_MAX_COLS_PER_SHEET and XLSX_MAX_COLS_PER_SHEET > 0:
                        width = min(width, XLSX_MAX_COLS_PER_SHEET)

                    # ìµœì¢… ë¬¸ìì—´ êµ¬ì„±
                    row_vals = []
                    for v in row[:width]:
                        row_vals.append("" if v is None else str(v).strip())

                    acc.append(" | ".join(row_vals))
                    rows += 1

            return "\n".join(acc)
            
        except Exception as e:
            # ì•”í˜¸í™”/ì†ìƒ/ë¹„ì •ìƒ êµ¬ì¡° ë“± ëª…í™•í•œ ë©”ì‹œì§€ ì „ë‹¬
            raise ValueError(f"ì—‘ì…€ ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        finally:
            # Excel ì›Œí¬ë¶ ëª…ì‹œì ìœ¼ë¡œ ë‹«ê¸° (ì„ì‹œ íŒŒì¼ ì •ë¦¬ ë¬¸ì œ í•´ê²°)
            if wb is not None:
                try:
                    wb.close()
                except Exception:
                    pass

    def read_csv(self, path: Path) -> str:  # CSV íŒŒì¼ íŒŒì‹±
        """CSV íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (í…Œì´ë¸” í˜•íƒœ ìœ ì§€)"""
        import csv
        try:
            acc: List[str] = []
            with open(path, 'r', encoding='utf-8-sig', newline='') as f:
                # CSV ë°©ì–¸ ìë™ ê°ì§€ ì‹œë„
                try:
                    sample = f.read(2048)
                    f.seek(0)
                    dialect = csv.Sniffer().sniff(sample)
                    reader = csv.reader(f, dialect)
                except Exception:
                    # ê°ì§€ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
                    f.seek(0)
                    reader = csv.reader(f)
                
                for row_num, row in enumerate(reader):
                    if row_num > 10000:  # í–‰ ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ ë³´í˜¸)
                        acc.append("...(truncated at 10000 rows)")
                        break
                    
                    # ë¹ˆ í–‰ ìŠ¤í‚µ
                    if not any(cell.strip() for cell in row):
                        continue
                    
                    # í…Œì´ë¸” í˜•íƒœë¡œ íŒŒì´í”„ êµ¬ë¶„ì ì‚¬ìš©
                    acc.append(" | ".join(str(cell).strip() for cell in row))
                    
            return "\n".join(acc)
            
        except UnicodeDecodeError:
            # UTF-8 ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
            try:
                with open(path, 'r', encoding='cp949', newline='') as f:
                    reader = csv.reader(f)
                    acc = []
                    for row_num, row in enumerate(reader):
                        if row_num > 10000:
                            acc.append("...(truncated at 10000 rows)")
                            break
                        if not any(cell.strip() for cell in row):
                            continue
                        acc.append(" | ".join(str(cell).strip() for cell in row))
                    return "\n".join(acc)
            except Exception as e:
                raise ValueError(f"CSV ì¸ì½”ë”© ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        except Exception as e:
            raise ValueError(f"CSV ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")

    def read_txt(self, path: Path) -> str:  # ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ íŒŒì‹±
        """ì¼ë°˜ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì§€ì›)"""
        encodings = ['utf-8-sig', 'utf-8', 'cp949', 'euc-kr', 'latin1']
        
        for encoding in encodings:
            try:
                with open(path, 'r', encoding=encoding) as f:
                    content = f.read()
                    if content.strip():  # ë¹ˆ íŒŒì¼ì´ ì•„ë‹ˆë©´ ì„±ê³µ
                        return content
            except (UnicodeDecodeError, UnicodeError):
                continue
            except Exception as e:
                raise ValueError(f"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        
        # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨
        raise ValueError(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì¸ì½”ë”©ì„ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

    def load_text(self, file_path: str, verbose: bool = True) -> str:
        """í™•ì¥ì + ì‹¤ì œ í¬ë§· ìŠ¤ë‹ˆí•‘ìœ¼ë¡œ ì ì ˆí•œ íŒŒì„œ ì„ íƒ"""
        p = Path(file_path)
        ext = p.suffix.lower()

        if verbose:
            print(f"  ğŸ“„ íŒŒì¼ íŒŒì‹± ì¤‘: {p.name} ({ext})")

        actual = _detect_office_kind(p)  # ì‹¤ì œ í¬ë§· ìŠ¤ë‹ˆí•‘(ZIP ê¸°ë°˜ Office ë¬¸ì„œì˜ ì‹¤ì œ ì¢…ë¥˜ë¥¼ ì¶”ì •)

        try:
            if ext == ".pdf":   # PDF íŒŒì¼ íŒŒì‹±
                return self.read_pdf(p)

            if ext == ".docx" or (actual == "docx" and ext != ".xlsx"):  # DOCX íŒŒì¼ íŒŒì‹±
                if verbose and ext != ".docx" and actual == "docx":
                    print(" âš ï¸ í™•ì¥ìì™€ ë‹¤ë¥¸ ì‹¤ì œ í¬ë§·(docx) ê°ì§€ â†’ docx íŒŒì„œ ì‚¬ìš©")
                return self.read_docx(p)

            if ext == ".xlsx" or (actual == "xlsx" and ext != ".docx"):  # XLSX íŒŒì¼ íŒŒì‹±
                if verbose and ext != ".xlsx" and actual == "xlsx":
                    print("  âš ï¸ í™•ì¥ìì™€ ë‹¤ë¥¸ ì‹¤ì œ í¬ë§·(xlsx) ê°ì§€ â†’ xlsx íŒŒì„œ ì‚¬ìš©")
                return self.read_xlsx(p)

            if ext == ".csv":  # CSV íŒŒì¼ íŒŒì‹±
                return self.read_csv(p)

            if ext == ".txt":  # í…ìŠ¤íŠ¸ íŒŒì¼ íŒŒì‹±
                return self.read_txt(p)

            # ë§ˆì§€ë§‰ ë³´ë£¨: ì‹¤ì œ í¬ë§· ê¸°ì¤€ ì‹œë„
            if actual == "docx":
                if verbose:
                    print("  âš ï¸ í™•ì¥ì ë¯¸ì§€ì›/ë¶ˆëª…ì´ë‚˜ ì‹¤ì œ í¬ë§·(docx) ê°ì§€ â†’ docx íŒŒì„œ ì‚¬ìš©")
                return self.read_docx(p)
            if actual == "xlsx":
                if verbose:
                    print("  âš ï¸ í™•ì¥ì ë¯¸ì§€ì›/ë¶ˆëª…ì´ë‚˜ ì‹¤ì œ í¬ë§·(xlsx) ê°ì§€ â†’ xlsx íŒŒì„œ ì‚¬ìš©")
                return self.read_xlsx(p)

            if verbose:
                print(f"  âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {ext} (ì‹¤ì œ í¬ë§· ë¯¸í™•ì¸)")
            return ""

        except Exception as e:
            if verbose:
                print(f"  âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({p.name}): {e}")
            return ""

    # ========================= Chroma í—¬í¼ =========================
    def get_chroma_collection(self, collection_name: Optional[str] = None):
        """
        (ìˆ˜ì •) íšŒì‚¬ ì½”ë“œë³„ë¡œ ì»¬ë ‰ì…˜ì„ ë¶„ë¦¬í•˜ê¸° ìœ„í•´ collection_name ì£¼ì… í—ˆìš©.
        - collection_name ì´ Noneì´ë©´ ê¸°ì¡´ í™˜ê²½ë³€ìˆ˜ COLLECTION_NAME ì‚¬ìš©.
        """
        name = collection_name or COLLECTION_NAME
        try:
            # ChromaDB ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
            Path(CHROMA_PATH).mkdir(parents=True, exist_ok=True)
            chroma = chromadb.PersistentClient(
                path=CHROMA_PATH,
                settings=Settings(
                    anonymized_telemetry=False,
                    is_persistent=True,
                ),
            )
            return chroma.get_or_create_collection(name=name)
        except Exception as e:
            print(f"ChromaDB ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
            print("ìƒˆë¡œìš´ ChromaDB ì¸ìŠ¤í„´ìŠ¤ë¡œ ì¬ì‹œë„ ì¤‘...")
            chroma = chromadb.Client()
            return chroma.get_or_create_collection(name=name)

    # ========================= (ì‹ ê·œ) ì™¸ë¶€ ë©”íƒ€ ë³‘í•© + ì»¬ë ‰ì…˜ ì§€ì • =========================
    def ingest_single_file_with_metadata(
        self,
        file_path: str,
        *,
        collection_name: str,
        extra_meta: Dict[str, Any],
        show_preview: bool = True
    ) -> Tuple[int, bool]:
        """
        (ì‹ ê·œ) íŒŒì¼ í•˜ë‚˜ë¥¼ ì¸ë±ì‹±í•˜ë©´ì„œ, ê° ì²­í¬ì˜ ë©”íƒ€ë°ì´í„°ì— extra_meta ë¥¼ ë³‘í•©í•˜ì—¬ ì €ì¥.
        - collection_name : íšŒì‚¬ ì½”ë“œ(ì˜ˆ: 'CAESAR2024') â†’ íšŒì‚¬ë³„ ì»¬ë ‰ì…˜ ë¶„ë¦¬
        - extra_meta      : {'doc_id': int, 'company_id': int, 'user_id': Optional[int], 'is_private': bool}
        - return          : (chunks_count, success_flag)
        """
        print(f"ğŸ“‚ ì…ë ¥ íŒŒì¼: {file_path} (collection={collection_name})")
        try:
            # 1) íŒŒì¼ ë¡œë“œ ë° ê²€ì¦
            raw_text = self.load_text(file_path, verbose=False)
            if not raw_text.strip():
                print(f"âŒ ë¹ˆ íŒŒì¼ì´ê±°ë‚˜ ì½ê¸° ì‹¤íŒ¨: {file_path}")
                return 0, False

            print(f"âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ, ì „ì²´ ê¸¸ì´: {len(raw_text):,} chars")

            # 2) í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self.text_splitter.split_text(raw_text)

            # ê° ì²­í¬ì˜ í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶œë ¥(ì˜µì…˜)
            if show_preview:
                for i, c in enumerate(chunks[:3]):
                    print(f"  [Chunk {i}] {len(c):,} chars / preview: {c[:100]}...")

            print(f"ğŸª“ ì²­í‚¹ ì™„ë£Œ â†’ ì´ {len(chunks)} chunks")
            if not chunks:
                print("âŒ ì²­í‚¹ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                return 0, False

            # 3) ì„ë² ë”© ìƒì„±
            print("âš™ï¸ ì„ë² ë”© ìƒì„± ì¤‘...")
            embeddings = embed_texts_batched(chunks)
            if not embeddings:
                print("âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨(ë¹ˆ ì…ë ¥).")
                return 0, False
            print(f"âœ… ì„ë² ë”© ì™„ë£Œ â†’ shape: {len(embeddings)} x {len(embeddings[0])}")

            # 4) íšŒì‚¬ ì½”ë“œ ì»¬ë ‰ì…˜ìœ¼ë¡œ ì €ì¥
            collection = self.get_chroma_collection(collection_name)

            # ğŸ“ ê¸°ì¡´ ë™ì¼ ë¬¸ì„œ ì²­í¬ ì‚­ì œ(doc_id ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€)
            file_name = Path(file_path).name
            try:
                # doc_idê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë¬¸ì„œì˜ ì²­í¬ë§Œ ì‚­ì œ, ì—†ìœ¼ë©´ íŒŒì¼ëª…ìœ¼ë¡œ ì‚­ì œ
                if extra_meta and "doc_id" in extra_meta:
                    existing = collection.get(where={"doc_id": extra_meta["doc_id"]})
                else:
                    existing = collection.get(where={"source": file_name})
                
                if existing and existing.get("ids"):
                    collection.delete(ids=existing["ids"])
                    print(f"ğŸ—‘ ê¸°ì¡´ {len(existing['ids'])} ì²­í¬ ì‚­ì œ")
            except Exception as e:
                print(f"âš ï¸ ê¸°ì¡´ ì²­í¬ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}")
                pass

            # ìƒˆ ë°ì´í„° ì¶”ê°€
            base_id = Path(file_path).stem
            ids = [f"{base_id}-{i}" for i in range(len(chunks))]

            # ê¸°ì¡´ ë©”íƒ€ ìœ ì§€ + extra_meta ë³‘í•©
            metadatas = []
            for i in range(len(chunks)):
                m = {
                    "source": file_name,       # ê¸°ì¡´ ë©”íƒ€
                    "chunk_idx": i,            # ê¸°ì¡´ ë©”íƒ€
                }
                if isinstance(extra_meta, dict):
                    m.update(extra_meta)       # â† ë³‘í•©: doc_id/company_id/user_id/is_private
                metadatas.append(m)

            collection.add(
                ids=ids,
                metadatas=metadatas,
                embeddings=embeddings,
                documents=chunks,
            )

            print(f"ğŸ‰ ì™„ë£Œ! {len(chunks)} chunks â†’ Chroma collection '{collection_name}' ì €ì¥")
            return len(chunks), True

        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return 0, False

    # ê¸°ì¡´ ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ë©”ì„œë“œëŠ” ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ingest_single_file_with_metadata ì‚¬ìš©)
    # ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬ ê¸°ëŠ¥ì€ ê´€ë¦¬ì ì—…ë¡œë“œì—ì„œ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°

    # ë¶ˆí•„ìš”í•œ í¸ì˜ í•¨ìˆ˜ ì œê±° - IngestService í´ë˜ìŠ¤ë¥¼ ì§ì ‘ ì‚¬ìš©

# ========================= CLI =========================
def main():
    print("=" * 80)
    print("ğŸ“š ë¬¸ì„œ ì„ë² ë”© ì„œë¹„ìŠ¤")
    print("=" * 80)

    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•:")
        print("  í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ì²˜ë¦¬: python ingest_service.py <íŒŒì¼ê²½ë¡œ>")
        print("\nğŸ“ ì‹¤ì œ ê´€ë¦¬ì ì—…ë¡œë“œëŠ” /api/admin/files/upload ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        print("\nì˜ˆì‹œ:")
        print("  python ingest_service.py ./storage/data/document.pdf")
        sys.exit(1)

    path = sys.argv[1]

    try:
        path_obj = Path(path)

        if path_obj.is_file():  # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
            print("ğŸ“„ ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ")
            # ê°œë³„ íŒŒì¼ í…ŒìŠ¤íŠ¸ìš© - ì‹¤ì œ ê´€ë¦¬ì ì—…ë¡œë“œëŠ” file_ingest_service.py ì‚¬ìš©
            svc = IngestService()
            success = svc.ingest_single_file_with_metadata(
                str(path_obj),
                collection_name=COLLECTION_NAME,
                extra_meta={},
                show_preview=True
            )
            sys.exit(0 if success[1] else 1)

        else:
            print(f"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {path}")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
