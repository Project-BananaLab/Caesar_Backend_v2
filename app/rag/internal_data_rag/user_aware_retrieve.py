# app/rag/internal_data_rag/user_aware_retrieve.py
# -*- coding: utf-8 -*-
"""
ì‚¬ìš©ìžë³„ ê¶Œí•œì„ ê³ ë ¤í•œ RAG ë¬¸ì„œ ê²€ìƒ‰ ì„œë¹„ìŠ¤
- íšŒì‚¬ ê³µê°œ ë¬¸ì„œ: ê°™ì€ íšŒì‚¬ ì§ì› ëª¨ë‘ ì ‘ê·¼ ê°€ëŠ¥
- ê°œì¸ ë¬¸ì„œ: ë³¸ì¸ë§Œ ì ‘ê·¼ ê°€ëŠ¥
- ë‹¤ë¥¸ ì§ì›ì˜ ê°œì¸ ë¬¸ì„œ: ì ‘ê·¼ ë¶ˆê°€

- TF-IDF í¬ì†Œ ê²€ìƒ‰ + ê¶Œí•œ ì‚¬í›„ í•„í„° + RRF ê²°í•©
- í¬ì†Œ ì¸ë±ìŠ¤ê°€ ë¡œì»¬ì— ì—†ìœ¼ë©´ S3ì—ì„œ ìžë™ ë‹¤ìš´ë¡œë“œ í›„ ìºì‹œ
- ì§ˆì˜ í™•ìž¥(ë™ì˜ì–´/í”Œëž«í¼ ì •ê·œí™” + PRF) â†’ dense/sparseì— ë™ì¼ ì ìš©

ì¶”ê°€:
- ìœ ì‚¬ë„ ìž„ê³„ê°’ ì»· + ë¬¸ì„œ/ìœ í˜• ì¿¼í„°ë§
- Cross-encoder Rerank
- LLM Multi-Query
- ì¶œì²˜/íŽ˜ì´ì§€ dedup + ë¬¸ìž¥ ì¤‘ë³µ ì œê±°
"""

import os
import re
import json
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
from sqlalchemy.orm import Session
from dotenv import load_dotenv

# í¬ì†Œ ê²€ìƒ‰(ìžˆìœ¼ë©´ ì‚¬ìš©)
try:
    from sklearn.metrics.pairwise import cosine_similarity
    import joblib
except Exception:
    cosine_similarity = None
    joblib = None

# Cross-encoder (ì„ íƒ)
try:
    from sentence_transformers import CrossEncoder
except Exception:
    CrossEncoder = None

load_dotenv()

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import tool
from langchain_core.documents import Document

from app.utils.db import get_db
from app.features.login.employee_google import crud as employee_crud
from app.features.admin.services.file_ingest_service import _get_company_code
from app.rag.internal_data_rag.internal_retrieve import (
    _stable_similarity,
    _truncate_context_blocks,
    MAX_CONTEXT_CHARS,
    CHROMA_PATH,
    EMBED_MODEL,
    CHAT_MODEL,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½/ì˜µì…˜ (internal_retrieve.pyì™€ ë™ì¼í•˜ê²Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SPARSE_INDEX_PATH = os.getenv("SPARSE_INDEX_PATH", os.path.join(CHROMA_PATH, "sparse"))

# ìž„ê³„/ì¿¼í„°/ë¦¬ëž­í¬ (internal_retrieve.pyì™€ ë™ì¼)
SIM_THRESHOLD = float(os.getenv("SIM_THRESHOLD", "0.25"))
DOC_QUOTA = int(os.getenv("DOC_QUOTA", "15"))
KIND_QUOTA = os.getenv("KIND_QUOTA", "text:12,image_ocr:8,image_vlm:4")
ENABLE_RERANK = os.getenv("ENABLE_RERANK", "true").lower() == "true"
CROSS_ENCODER_MODEL = os.getenv("CROSS_ENCODER_MODEL", "cross-encoder/ms-marco-MultiBERT-L-12")
# CE ìž…ë ¥ ìµœì í™”
CE_MAX_CHARS = int(os.getenv("CE_MAX_CHARS", "1800"))
CE_BATCH_SIZE = int(os.getenv("CE_BATCH_SIZE", "16"))

# LLM Multi-Query (ë³´ìˆ˜í™”)
ENABLE_LLM_MULTIQUERY = os.getenv("ENABLE_LLM_MULTIQUERY", "true").lower() == "true"
N_QUERY_REWRITES = int(os.getenv("N_QUERY_REWRITES", "1"))  # ê¸°ë³¸ 1ë¡œ ì™„í™”
SHORT_QUERY_CHAR = int(os.getenv("SHORT_QUERY_CHAR", "40"))

# ë””ë“‘ íŒŒë¼ë¯¸í„°
MAX_PER_PAGE = int(os.getenv("MAX_PER_PAGE", "2"))  # íŽ˜ì´ì§€ë‹¹ ìµœëŒ€ ìœ ì§€ ê°œìˆ˜

# S3(ì˜µì…˜) â€” í¬ì†Œ ì¸ë±ìŠ¤ ë™ê¸°í™” (ì´ë¦„ í†µì¼)
S3_BUCKET = os.getenv("S3_BUCKET")
S3_REGION = os.getenv("S3_REGION")
AWS_S3_PREFIX = os.getenv("AWS_S3_PREFIX", "rag/sparse")

# ì§ˆì˜ ë™ì˜ì–´ ì‚¬ì „(ì™¸ë¶€ íŒŒì¼ ê²½ë¡œ - ì„ íƒ)
QUERY_SYNONYM_PATH = os.getenv("QUERY_SYNONYM_PATH")  # e.g. ./config/synonyms.json

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# NEW: ì§ˆì˜ í™•ìž¥ ìœ í‹¸(ë™ì˜ì–´/ì •ê·œí™” + PRF)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_synonym_dict() -> dict:
    try:
        if QUERY_SYNONYM_PATH and Path(QUERY_SYNONYM_PATH).exists():
            with open(QUERY_SYNONYM_PATH, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return {
        "ìœ íŠœë¸Œ": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ê°œì¸ë°©ì†¡", "ì˜ìƒ ì—…ë¡œë“œ"],
        "ë¸Œì´ë¡œê·¸": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ê°œì¸ë°©ì†¡", "ì˜ìƒ ì œìž‘", "ë™ì˜ìƒ ì—…ë¡œë“œ"],
        "í‹±í†¡": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ì†Œì…œë¯¸ë””ì–´"],
        "ì¸ìŠ¤íƒ€": ["ì†Œì…œë¯¸ë””ì–´", "SNS", "ì½˜í…ì¸  ì—…ë¡œë“œ"],
        "ë¼ì´ë¸Œ": ["ì‹¤ì‹œê°„ ë°©ì†¡", "ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë°", "ì¸í„°ë„· ê°œì¸ë°©ì†¡"],
        "ìˆ˜ìµ": ["ê´‘ê³ ìˆ˜ìµ", "ìˆ˜ìµ ì°½ì¶œ", "í˜‘ì°¬", "ê°„ì ‘ê´‘ê³ ", "PPL"],
        "í˜‘ì°¬": ["ê°„ì ‘ê´‘ê³ ", "PPL", "ëŒ€ê°€ì„± ì œê³µ"],
        "ê²¸ì—…": ["ê²¸ì—…í—ˆê°€", "ê²¸ì—…ì˜ í—ˆê°€", "ë¶€ì—…"],
        "ë³´ì•ˆ": ["ì§ë¬´ìƒ ë¹„ë°€", "ì˜ì—…ë¹„ë°€", "ì •ë³´ë³´ì•ˆ"],
        "ëª…ì˜ˆí›¼ì†": ["ëª…ì˜ˆ ì¹¨í•´", "ê¶Œë¦¬ ì¹¨í•´"],
        # ì¸ì‚¬/ê²½ë ¥ ê´€ë ¨ ë™ì˜ì–´ ì¶”ê°€
        "ì´ë ¥ì‚¬": ["ì´ë ¥ì„œ", "ê²½ë ¥", "ê²½í—˜", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„", "ê°œì¸ì •ë³´"],
        "ì´ë ¥ì„œ": ["ì´ë ¥ì‚¬", "ê²½ë ¥ì„œ", "ê²½ë ¥", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„", "ê°œì¸ì •ë³´"],
        "ê²½ë ¥": ["ì´ë ¥ì„œ", "ì´ë ¥ì‚¬", "ê²½í—˜", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„"],
        "ê²½í—˜": ["ì´ë ¥ì„œ", "ì´ë ¥ì‚¬", "ê²½ë ¥", "ì»¤ë¦¬ì–´"],
    }

_SYNONYM_DICT = _load_synonym_dict()
_NORMALIZE_MAP = {
    "youtube": "ìœ íŠœë¸Œ",
    "yt": "ìœ íŠœë¸Œ",
    "vlog": "ë¸Œì´ë¡œê·¸",
    "shorts": "ì‡¼ì¸ ",
    "tiktok": "í‹±í†¡",
    "instagram": "ì¸ìŠ¤íƒ€",
}

def _normalize_terms(q: str) -> str:
    low = q.lower()
    for a, b in _NORMALIZE_MAP.items():
        low = re.sub(rf"\b{re.escape(a)}\b", b, low)
    return low

_STOP = set(["ë°","ë˜ëŠ”","ê·¸ë¦¬ê³ ","ê´€ë ¨","ê´€ë ¨ëœ","ì—¬ë¶€","ê²ƒ","ìˆ˜","ë“±","í•´ë‹¹","í•˜ëŠ”","ê²½ìš°","ëŒ€í•œ","ìœ¼ë¡œ","ì—ì„œ","í•˜ë‹¤"])
def _extract_keywords(text: str, top_k: int = 6) -> list:
    toks = re.split(r"[^ê°€-íž£A-Za-z0-9]+", text)
    cnt: Dict[str, int] = {}
    for t in toks:
        if len(t) < 2 or t in _STOP:
            continue
        cnt[t] = cnt.get(t, 0) + 1
    return [w for w,_ in sorted(cnt.items(), key=lambda x: x[1], reverse=True)[:top_k]]

def _expand_query_base(q: str) -> str:
    qn = _normalize_terms(q)
    aug: List[str] = []
    for k, vs in _SYNONYM_DICT.items():
        if k in qn:
            aug.extend(vs)
    if any(x in qn for x in ["ìœ íŠœë¸Œ","ë¸Œì´ë¡œê·¸","í‹±í†¡","ì¸ìŠ¤íƒ€","ë¼ì´ë¸Œ","ì‡¼ì¸ "]):
        aug.extend(["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ê²¸ì—…í—ˆê°€", "ì œ82ì¡°", "ì¸í„°ë„· ê°œì¸ë°©ì†¡ í™œë™ ì œí•œ"])
    aug = list(dict.fromkeys([v for v in aug if v not in q]))  # ì¤‘ë³µ ì œê±°
    return q if not aug else f"{q} " + " ".join(aug)

def _augment_with_prf(vectorstore, q: str, k_init: int = 4, kw_top: int = 6) -> str:
    try:
        hits = vectorstore.similarity_search_with_score(q, k=k_init)
        if not hits:
            return q
        blob = "\n".join([d.page_content for d,_ in hits])
        kws = _extract_keywords(blob, top_k=kw_top)
        kws = [w for w in kws if w not in q]
        return f"{q} " + " ".join(kws) if kws else q
    except Exception:
        return q

def expand_query(vectorstore, query: str) -> str:
    q1 = _expand_query_base(query)
    q2 = _augment_with_prf(vectorstore, q1)
    if q2 != query:
        print(f"ðŸ§© ì¿¼ë¦¬ í™•ìž¥: '{query}' â†’ '{q2}'")
    return q2

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ðŸ”Ž ì •í™• ë¬¸ìžì—´ ë¶€ìŠ¤íŒ… (ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _boost_exact_match(query: str, pairs: List[Tuple[Document, float]], factor: float = 0.85) -> List[Tuple[Document, float]]:
    """
    ì¿¼ë¦¬(ì •í™• ë¬¸ìžì—´)ê°€ ì²­í¬ ë³¸ë¬¸ì— í¬í•¨ë˜ë©´ ê±°ë¦¬ë¥¼ factor ë°°ë¡œ ì¤„ì—¬(=ìœ ì‚¬ë„â†‘) ìƒìœ„ë¡œ ëŒì–´ì˜¬ë¦¼.
    - ëŒ€ì†Œë¬¸ìž ë¬´ì‹œ ë§¤ì¹­ë„ í¬í•¨
    """
    q = (query or "").strip()
    if not q or not pairs:
        return pairs
    q_low = q.lower()
    boosted: List[Tuple[Document, float]] = []
    for doc, dist in pairs:
        txt = doc.page_content or ""
        if (q in txt) or (q_low in txt.lower()):
            dist *= factor
        boosted.append((doc, dist))
    return boosted

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# S3 ë‹¤ìš´ë¡œë“œ ìœ í‹¸ â€” í¬ì†Œ ì¸ë±ìŠ¤ ë¯¸ì¡´ìž¬ ì‹œ ì›ê²©ì—ì„œ í™•ë³´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _s3_configured() -> bool:
    return all(os.getenv(k) for k in ["S3_BUCKET", "S3_REGION", "AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"])

def _download_sparse_from_s3(local_docs_fp: str, local_tfidf_fp: str, collection_name: str):
    if not _s3_configured():
        return False
    try:
        import boto3
        s3 = boto3.client(
            "s3",
            region_name=os.getenv("S3_REGION"),
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
        )
        prefix = os.getenv("AWS_S3_PREFIX", "rag/sparse")
        key_docs = f"{prefix}/{collection_name}_docs.joblib"
        key_tfidf = f"{prefix}/{collection_name}_tfidf.joblib"
        bucket = os.getenv("S3_BUCKET")
        s3.download_file(bucket, key_docs, local_docs_fp)
        s3.download_file(bucket, key_tfidf, local_tfidf_fp)
        print(f"â˜ï¸  S3ì—ì„œ í¬ì†Œ ì¸ë±ìŠ¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ â†’ {local_docs_fp}, {local_tfidf_fp}")
        return True
    except Exception as e:
        print(f"âš ï¸ S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨(í¬ì†Œ ê²€ìƒ‰ ë¹„í™œì„±): {e}")
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¬ì†Œ ê²€ìƒ‰ê¸°(TF-IDF)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class _SparseSearcher:
    def __init__(self, base_dir: str, collection_name: str):
        os.makedirs(base_dir, exist_ok=True)
        self.collection_name = collection_name
        self.data_fp = os.path.join(base_dir, f"{collection_name}_docs.joblib")
        self.index_fp = os.path.join(base_dir, f"{collection_name}_tfidf.joblib")
        self.enabled = joblib is not None and cosine_similarity is not None
        self.loaded = False
        self.ids = []; self.docs = []; self.metas = []
        self.vectorizer = None; self.mat = None

    def _ensure_local(self):
        if os.path.exists(self.data_fp) and os.path.exists(self.index_fp):
            return
        _download_sparse_from_s3(self.data_fp, self.index_fp, self.collection_name)

    def _lazy_load(self):
        if self.loaded or not self.enabled:
            return
        self._ensure_local()
        if not (os.path.exists(self.data_fp) and os.path.exists(self.index_fp)):
            self.enabled = False
            print("â„¹ï¸ TF-IDF ë¯¸í™œì„±í™”(ë¡œì»¬ .joblib ì—†ìŒ) â†’ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©.")
            return
        self.ids, self.docs, self.metas = joblib.load(self.data_fp)
        self.vectorizer, self.mat = joblib.load(self.index_fp)
        self.loaded = True
        print(f"âœ… TF-IDF í™œì„±í™”: {self.collection_name} (docs={len(self.ids)})")

    def search(self, query: str, k: int = 20) -> List[Tuple[Document, float]]:
        if not self.enabled:
            return []
        self._lazy_load()
        if not self.loaded:
            return []
        qv = self.vectorizer.transform([query])
        sims = cosine_similarity(qv, self.mat).ravel()
        idxs = sims.argsort()[::-1][:k]
        out = []
        for i in idxs:
            out.append((Document(page_content=self.docs[i], metadata=self.metas[i]), float(1 - sims[i])))
        return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RRF ê²°í•© + ë„ìš°ë¯¸ (internal_retrieve.pyì™€ ë™ì¼)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _rrf_fusion(dense: List[Tuple[Document, float]], sparse: List[Tuple[Document, float]], k_rrf: int = 60, top_k: int = 6):
    """
    ì—¬ëŸ¬ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸(dense/sparse)ë¥¼ RRFë¡œ ê²°í•©.
    - meta['rrf_score'] : ì›ì‹œ RRF ëˆ„ì ê°’
    - meta['rrf_sim']   : 0~1 ì •ê·œí™” ìœ ì‚¬ë„
    - ë°˜í™˜ distance     : 1 - rrf_sim (ìž‘ì„ìˆ˜ë¡ ê°€ê¹ë‹¤)
    """
    pools: Dict[str, Dict[str, Any]] = {}
    for rank, (doc, _) in enumerate(dense, start=1):
        key = (doc.metadata or {}).get("chunk_hash") or f"d:{rank}:{hash(doc.page_content)%10_000_000}"
        pools.setdefault(key, {"doc": doc, "rrf": 0.0})
        pools[key]["rrf"] += 1.0 / (k_rrf + rank)
    for rank, (doc, _) in enumerate(sparse, start=1):
        key = (doc.metadata or {}).get("chunk_hash") or f"s:{rank}:{hash(doc.page_content)%10_000_000}"
        pools.setdefault(key, {"doc": doc, "rrf": 0.0})
        pools[key]["rrf"] += 1.0 / (k_rrf + rank)

    fused = sorted(pools.values(), key=lambda x: x["rrf"], reverse=True)[:top_k]
    max_rrf = max((f["rrf"] for f in fused), default=1.0)

    out: List[Tuple[Document, float]] = []
    for f in fused:
        doc = f["doc"]
        rrf = f["rrf"]
        rrf_sim = rrf / max_rrf if max_rrf > 0 else 0.0
        dist = 1.0 - rrf_sim
        meta = dict(doc.metadata or {})
        meta["rrf_score"] = rrf
        meta["rrf_sim"] = rrf_sim
        doc.metadata = meta
        out.append((doc, dist))
    return out

def _parse_kind_quota(s: str) -> Dict[str, int]:
    out: Dict[str, int] = {}
    for tok in s.split(","):
        if ":" in tok:
            k, v = tok.split(":", 1)
            try:
                out[k.strip()] = int(v)
            except:
                pass
    return out

_KIND_QUOTA = _parse_kind_quota(KIND_QUOTA)
_cross_encoder = None
def _get_cross_encoder():
    global _cross_encoder
    if _cross_encoder is None and CrossEncoder and ENABLE_RERANK:
        try:
            _cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL)
            print(f"âœ… Cross-encoder ë¡œë“œ: {CROSS_ENCODER_MODEL}")
        except Exception as e:
            print(f"âš ï¸ Cross-encoder ë¡œë“œ ì‹¤íŒ¨: {e}")
    return _cross_encoder

def _adaptive_threshold(sims: List[float], base: float) -> float:
    if not sims:
        return base
    xs = sorted(sims)
    q30 = xs[int(len(xs)*0.30)]
    thr = max(base, min(q30, 0.35))  # cap 0.35
    return thr

def _apply_threshold_and_quota(pairs: List[Tuple[Document, float]], top_k: int) -> List[Tuple[Document, float]]:
    """
    - rrf_sim(ê°€ëŠ¥ ì‹œ) ê¸°ë°˜ ìž„ê³„ê°’ ì»·, ì—†ìœ¼ë©´ distanceâ†’_stable_similarity ë°±ì—….
    - ë¶„í¬ ê¸°ë°˜ ì ì‘í˜• ìž„ê³„ê°’ ì‚¬ìš©(í•˜í•œ: SIM_THRESHOLD).
    - source/kind ì¿¼í„° ì ìš©.
    - ì»· ì‚¬ìœ  ì¹´ìš´íŠ¸ ë¡œê¹….
    """
    scored_meta = []
    for doc, dist in pairs:
        meta = doc.metadata or {}
        sim = meta.get("rrf_sim", _stable_similarity(dist))
        scored_meta.append((doc, dist, sim))

    sims = [s for _, _, s in scored_meta]
    thr = _adaptive_threshold(sims, SIM_THRESHOLD)

    scored = []
    dropped_reason_counts = {"threshold": 0, "doc_quota": 0, "kind_quota": 0}

    for doc, dist, sim in scored_meta:
        if sim >= thr:
            scored.append((doc, dist, sim))
        else:
            dropped_reason_counts["threshold"] += 1

    by_src: Dict[str, int] = {}
    by_kind: Dict[str, int] = {}
    kept: List[Tuple[Document, float]] = []

    for doc, dist, sim in sorted(scored, key=lambda x: x[2], reverse=True):
        meta = doc.metadata or {}
        src = meta.get("source", "unknown")
        kind = meta.get("content_kind") or meta.get("content_type") or "text"

        if by_src.get(src, 0) >= DOC_QUOTA:
            dropped_reason_counts["doc_quota"] += 1
            continue
        if _KIND_QUOTA.get(kind, 10**9) <= by_kind.get(kind, 0):
            dropped_reason_counts["kind_quota"] += 1
            continue

        kept.append((doc, dist))
        by_src[src] = by_src.get(src, 0) + 1
        by_kind[kind] = by_kind.get(kind, 0) + 1
        if len(kept) >= top_k:
            break

    print(f"ìž„ê³„/ì¿¼í„° ì ìš© ê²°ê³¼: thr={thr:.3f}, kept={len(kept)}, dropped={dropped_reason_counts}")
    return kept

def _dedup_by_source_page(pairs: List[Tuple[Document, float]]) -> List[Tuple[Document, float]]:
    """
    (source, page, sheet) ê¸°ì¤€ ë³´ìˆ˜ì  ë””ë“‘.
    - pageê°€ Noneì´ë©´ ë””ë“‘í•˜ì§€ ì•ŠìŒ(ë©”íƒ€ ë¶€ì‹¤ ë³´í˜¸)
    - íŽ˜ì´ì§€ë‹¹ ìµœëŒ€ MAX_PER_PAGEê°œê¹Œì§€ í—ˆìš©
    """
    counts: Dict[Tuple[Any, Any, Any], int] = {}
    out = []
    for doc, dist in pairs:
        m = doc.metadata or {}
        src = m.get("source")
        page = m.get("page")
        sheet = m.get("sheet")
        if page is None:
            out.append((doc, dist))
            continue
        key = (src, page, sheet)
        if counts.get(key, 0) >= MAX_PER_PAGE:
            continue
        counts[key] = counts.get(key, 0) + 1
        out.append((doc, dist))
    return out

# ë¸”ë¡ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°(ë™ì¼ ë¼ì¸ 3íšŒì§¸ë¶€í„° ì œê±°)
def _dedup_sentences_blockwise(text: str) -> str:
    blocks = [b for b in re.split(r"\n\s*---\s*\n", text) if b.strip()]
    new_blocks = []
    for b in blocks:
        lines = [ln for ln in b.splitlines()]
        seen: Dict[str, int] = {}
        acc = []
        for ln in lines:
            key = ln.strip().lower()
            c = seen.get(key, 0) + 1
            seen[key] = c
            if c > 2:
                continue
            acc.append(ln)
        new_blocks.append("\n".join(acc))
    return "\n\n---\n\n".join(new_blocks)

# LLM Multi-Query ìž¬ìž‘ì„± (ë³´ìˆ˜í™”)
def _should_multiquery(q: str) -> bool:
    return ENABLE_LLM_MULTIQUERY and len(q) <= SHORT_QUERY_CHAR

def _llm_rewrites(llm: ChatOpenAI, query: str, n: int) -> List[str]:
    if not _should_multiquery(query) or n <= 0:
        return []
    try:
        prompt = (
            "ë‹¤ìŒ ì§ˆë¬¸ì„ ë¬¸ì„œ ê²€ìƒ‰ì— ê°•ê±´í•œ í˜•íƒœë¡œ {n}ê°œ ìž¬ìž‘ì„±í•˜ì„¸ìš”.\n"
            "- í•µì‹¬ í‚¤ì›Œë“œëŠ” ìœ ì§€í•˜ë˜ í‘œí˜„ì„ ë‹¤ì–‘í™”(ë™ì˜ì–´/ì „ë¬¸ìš©ì–´)\n"
            "- í•œêµ­ì–´ ê³µì‹ ë¬¸ì„œ ìš©ì–´ ì„ í˜¸\n"
            "- í•œ ì¤„ì— í•˜ë‚˜ì”© ì¶œë ¥"
        ).format(n=n)
        txt = llm.invoke(prompt + "\n\nì§ˆë¬¸: " + query).content
        rewrites = [line.strip("-â€¢ ").strip() for line in txt.splitlines() if line.strip()]
        uniq, seen = [], set()
        for r in rewrites:
            k = r.lower()
            if k not in seen:
                seen.add(k); uniq.append(r)
        return uniq[:n]
    except Exception:
        return []

def _multi_query_dense(vectorstore, base_q: str, llm: ChatOpenAI, top_each: int) -> List[Tuple[Document, float]]:
    qs = [base_q]
    qs.extend(_llm_rewrites(llm, base_q, N_QUERY_REWRITES))
    qs = list(dict.fromkeys([q for q in qs if q]))
    pool: List[Tuple[Document, float]] = []
    for i, q in enumerate(qs):
        results = vectorstore.similarity_search_with_score(q, k=top_each)
        weight = 2.0 if i == 0 else 1.0  # ì›ë³¸ ì¿¼ë¦¬ ê°€ì¤‘
        pool.extend([(doc, dist / weight) for doc, dist in results])
    return _rrf_fusion(pool, [], k_rrf=60, top_k=min(6, len(qs) * top_each))  # ì†Œê·œëª¨ RRF

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê¶Œí•œ ì„œë¹„ìŠ¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class UserAwareRAGService:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(model=EMBED_MODEL)
        self.llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)
        self.parser = StrOutputParser()
        self.prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ë‹¹ì‹ ì€ ì‚¬ë‚´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•ížˆ ë‹µí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. "
                    "ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹µë³€í•˜ê³ , "
                    "ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥´ëŠ” ë‚´ìš©ì€ 'ëª¨ë¥¸ë‹¤'ê³  ëª…í™•ížˆ ë§í•˜ì„¸ìš”. "
                    "ê°€ëŠ¥í•œ í•œ ì¶œì²˜ ë¬¸ì„œëª…ê³¼ í•¨ê»˜ ë‹µë³€í•˜ì„¸ìš”.",
                ),
                ("user", "ì§ˆë¬¸: {question}\n\nì°¸ê³  ì»¨í…ìŠ¤íŠ¸(ì—¬ëŸ¬ ì²­í¬):\n{context}"),
            ]
        )

    def get_user_context(self, user_id: str) -> Optional[dict]:
        try:
            db = next(get_db())
            employee = employee_crud.get_employee_by_google_id(db, google_user_id=user_id)
            if not employee:
                print(f"âŒ ì‚¬ìš©ìžë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {user_id}")
                return None
            company_code = _get_company_code(db, employee.company_id)
            return {"employee_id": employee.id, "company_id": employee.company_id, "company_code": company_code}
        except Exception as e:
            print(f"âŒ ì‚¬ìš©ìž ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
        finally:
            db.close()

    def _allow_meta(self, meta: dict, company_id: int, employee_id: int) -> bool:
        if not isinstance(meta, dict):
            return False
        if meta.get("company_id") != company_id:
            return False
        if meta.get("is_private") is True:
            return meta.get("user_id") == employee_id
        return True

    def retrieve_documents_with_permission(
        self, query: str, user_id: str, top_k: int = 6
    ) -> List[Tuple[str, dict]]:
        user_context = self.get_user_context(user_id)
        if not user_context:
            print(f"âŒ ì‚¬ìš©ìž ì •ë³´ ì—†ìŒ - ê²€ìƒ‰ ì¤‘ë‹¨: {user_id}")
            return []

        company_code = user_context["company_code"]
        company_id = user_context["company_id"]
        employee_id = user_context["employee_id"]

        try:
            # íšŒì‚¬ë³„ ë²¡í„°ìŠ¤í† ì–´
            import chromadb
            client = chromadb.CloudClient(
                tenant=os.getenv("CHROMA_TENANT"),
                database=os.getenv("CHROMA_DATABASE"),
                api_key=os.getenv("CHROMA_API_KEY"),
            )
            vectorstore = Chroma(
                client=client,
                collection_name=company_code,
                embedding_function=self.embeddings,
            )

            # ì¿¼ë¦¬ í™•ìž¥
            q_exp = expand_query(vectorstore, query)
            print(f"ðŸ” ê¶Œí•œë³„ ë¬¸ì„œ ê²€ìƒ‰(í•˜ì´ë¸Œë¦¬ë“œ RRF): '{q_exp}' (íšŒì‚¬: {company_code}, ì‚¬ìš©ìž: {employee_id})")

            # ê¶Œí•œë³„ í•„í„°
            company_filter = {"$and": [{"company_id": {"$eq": company_id}}, {"is_private": {"$eq": False}}]}
            personal_filter = {"$and": [{"company_id": {"$eq": company_id}}, {"is_private": {"$eq": True}}, {"user_id": {"$eq": employee_id}}]}

            # dense: LLM multi-query ìž¬ìž‘ì„±(ë³´ìˆ˜í™”) â†’ ì†Œê·œëª¨ RRF ìœµí•©
            dense_public_pool = []
            dense_personal_pool = []
            
            # Multi-query ìƒì„±
            queries = [q_exp]
            queries.extend(_llm_rewrites(self.llm, q_exp, N_QUERY_REWRITES))
            queries = list(dict.fromkeys([q for q in queries if q]))
            
            for i, q in enumerate(queries):
                weight = 2.0 if i == 0 else 1.0  # ì›ë³¸ ì¿¼ë¦¬ ê°€ì¤‘
                # íšŒì‚¬ ê³µê°œ ë¬¸ì„œ ê²€ìƒ‰
                public_results = vectorstore.similarity_search_with_score(q, k=max(3, top_k//2), filter=company_filter)
                dense_public_pool.extend([(doc, dist / weight) for doc, dist in public_results])
                # ê°œì¸ ë¬¸ì„œ ê²€ìƒ‰
                personal_results = vectorstore.similarity_search_with_score(q, k=max(3, top_k//2), filter=personal_filter)
                dense_personal_pool.extend([(doc, dist / weight) for doc, dist in personal_results])
            
            dense_pool = dense_public_pool + dense_personal_pool

            # sparse: í¬ì†Œ ê²€ìƒ‰(TF-IDF) â€“ íšŒì‚¬ë³„ ì¸ë±ìŠ¤ í›„ ê¶Œí•œ ì‚¬í›„ í•„í„°
            sparse_searcher = _SparseSearcher(SPARSE_INDEX_PATH, company_code)
            sparse_all = sparse_searcher.search(q_exp, k=max(12, top_k * 2))
            sparse = [(d, dist) for (d, dist) in sparse_all if self._allow_meta(d.metadata or {}, company_id, employee_id)]

            # 1) RRF ê²°í•© (í›„ë³´êµ° ì¶©ë¶„ í™•ë³´)
            fused = _rrf_fusion(dense_pool, sparse, k_rrf=60, top_k=max(top_k * 3, 20))

            # 1-1) íŽ˜ì´ì§€/ì¶œì²˜ ì¤‘ë³µ ì œê±°(ë³´ìˆ˜ì )
            fused = _dedup_by_source_page(fused)

            # 2) ìž„ê³„ + ì¿¼í„°(ì ì‘í˜• ì»·)
            fused = _apply_threshold_and_quota(fused, top_k=max(top_k * 2, 20))

            # 2-1) ðŸ”Ž ì •í™• ë¬¸ìžì—´ ë¶€ìŠ¤íŒ…
            fused = _boost_exact_match(query, fused)

            # 3) CE ë¦¬ëž­í¬ â†’ ìµœì¢… top_k
            fused = self._rerank(query, fused, top_k)

            if not fused:
                print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []

            contexts: List[Tuple[str, dict]] = []
            print(f"âœ… ìµœì¢… ìƒìœ„ {len(fused)}ê°œ ê²°ê³¼")
            for i, (doc, distance) in enumerate(fused, start=1):
                meta = dict(doc.metadata or {})
                sim = meta.get("rrf_sim", _stable_similarity(distance))
                meta["similarity_score"] = sim
                is_private = meta.get("is_private", False)
                doc_type = "ê°œì¸ ë¬¸ì„œ" if is_private else "íšŒì‚¬ ë¬¸ì„œ"
                preview = (doc.page_content[:100] + "...") if len(doc.page_content) > 100 else doc.page_content
                print(
                    f"  [Rank {i}] sim={sim:.4f}, {doc_type}, src={meta.get('source')} "
                    f"sheet={meta.get('sheet')} page={meta.get('page')} "
                    f"chunk={meta.get('chunk_idx')} kind={meta.get('content_kind') or meta.get('content_type')}"
                )
                print(f"          ë‚´ìš©: {preview}")
                contexts.append((doc.page_content, meta))

            return contexts

        except Exception as e:
            print(f"âŒ ê¶Œí•œë³„ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def _clip(self, txt: str, max_chars: int) -> str:
        return txt if len(txt) <= max_chars else txt[:max_chars]

    def _rerank(self, query: str, pairs: List[Tuple[Document, float]], top_k: int) -> List[Tuple[Document, float]]:
        # MultiBERTëŠ” ë‹¤êµ­ì–´ ì§€ì› â†’ í•œêµ­ì–´ í¬í•¨ ì§ˆì˜ë„ CE ì ìš©
        ce = _get_cross_encoder()
        if not ce or not pairs:
            return pairs[:top_k]
        try:
            texts = [(query, self._clip(d.page_content, CE_MAX_CHARS)) for d, _ in pairs]
            # sentence-transformers CrossEncoderëŠ” batch_size ì¸ìžë¥¼ ì§€ì›
            scores = ce.predict(texts, batch_size=CE_BATCH_SIZE)
            ranked = sorted(zip(pairs, scores), key=lambda x: x[1], reverse=True)[:top_k]
            return [p for (p, _) in ranked]
        except Exception as e:
            print(f"âš ï¸ Cross-encoder ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return pairs[:top_k]

    def generate_answer(self, query: str, contexts: List[Tuple[str, dict]], model: str = None) -> str:
        if not contexts:
            return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
        try:
            def _rank_key(item):
                _, meta = item
                kind = (meta.get("content_kind") or meta.get("content_type") or "text")
                pri = 2
                if kind in ("image_vlm",): pri = 0
                if kind in ("image_ocr", "text"): pri = 3
                return (pri, meta.get("similarity_score", 0.0))

            contexts = sorted(contexts, key=_rank_key, reverse=True)
            model_label = model or getattr(self.llm, "model", getattr(self.llm, "model_name", "unknown"))
            print(f"âš™ï¸ ë‹µë³€ ìƒì„± ì¤‘... (ì»¨í…ìŠ¤íŠ¸ {len(contexts)}ê°œ, ëª¨ë¸: {model_label})")
            context_text = _truncate_context_blocks(contexts, max_chars=MAX_CONTEXT_CHARS)
            # ë¸”ë¡ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°(3íšŒì§¸ë¶€í„° ì œê±°)
            context_text = _dedup_sentences_blockwise(context_text)
            used_llm = self.llm if model is None else ChatOpenAI(model=model, temperature=0)
            chain = self.prompt | used_llm | self.parser
            answer = chain.invoke({"question": query, "context": context_text})
            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            return answer
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def query_rag_with_permission(self, query: str, user_id: str, top_k: int = 6, model: str = None, show_sources: bool = True) -> str:
        print(f"\nðŸ” ê¶Œí•œë³„ RAG ì§ˆì˜: {query} (ì‚¬ìš©ìž: {user_id})")
        contexts = self.retrieve_documents_with_permission(query, user_id, top_k)
        if not contexts:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì ‘ê·¼ ê°€ëŠ¥í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        answer = self.generate_answer(query, contexts, model)
        if show_sources:
            sources = []
            for _, meta in contexts:
                is_private = meta.get("is_private", False)
                doc_type = "ê°œì¸ ë¬¸ì„œ" if is_private else "íšŒì‚¬ ë¬¸ì„œ"
                src = f"- {meta.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')} ({doc_type}, ì²­í¬ {meta.get('chunk_idx', 'N/A')})"
                if src not in sources:
                    sources.append(src)
            return f"{answer}\n\nðŸ“‹ ì°¸ê³ í•œ ë¬¸ì„œ:\n" + "\n".join(sources)
        return answer

# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_user_aware_rag_service = UserAwareRAGService()

# ========================= LangChain ë„êµ¬ ì •ì˜ =========================
@tool
def user_aware_rag_search(query: str, user_id: str) -> str:
    """ì‚¬ìš©ìžë³„ ê¶Œí•œì„ ê³ ë ¤í•œ RAG ê²€ìƒ‰/ë‹µë³€ (íšŒì‚¬ ê³µê°œ + ë³¸ì¸ ê°œì¸ ë¬¸ì„œ)."""
    try:
        return _user_aware_rag_service.query_rag_with_permission(
            query=query, user_id=user_id, top_k=6, show_sources=True
        )
    except Exception as e:
        print(f"âŒ ì‚¬ìš©ìžë³„ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# ========================= ì‚¬ìš©ìžë³„ ë„êµ¬ íŒ©í† ë¦¬ =========================
def create_user_aware_rag_tools(user_id: str) -> list:
    """íŠ¹ì • ì‚¬ìš©ìžë¥¼ ìœ„í•œ RAG ë„êµ¬ ìƒì„± (user_id ë°”ì¸ë”©)."""
    print(f"ðŸ”§ ì‚¬ìš©ìžë³„ RAG ë„êµ¬ ìƒì„± ì¤‘: {user_id}")

    @tool
    def internal_rag_search(query: str) -> str:
        """ì—…ë¡œë“œëœ íšŒì‚¬/ê°œì¸ ë¬¸ì„œì—ì„œ ê²€ìƒ‰/ë‹µë³€(í•˜ì´ë¸Œë¦¬ë“œ)."""
        print(f"ðŸ” internal_rag_search í˜¸ì¶œë¨: query='{query}', user_id='{user_id}'")
        try:
            result = _user_aware_rag_service.query_rag_with_permission(
                query=query, user_id=user_id, top_k=6, show_sources=True
            )
            return result
        except Exception as e:
            print(f"âŒ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return f"ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    print(f"âœ… RAG ë„êµ¬ ìƒì„± ì™„ë£Œ: {user_id}")
    return [internal_rag_search]

# ê¸°ì¡´ ë²”ìš© ë„êµ¬
user_aware_rag_tools = [user_aware_rag_search]


# # app/rag/internal_data_rag/user_aware_retrieve.py
# # -*- coding: utf-8 -*-
# """
# ì‚¬ìš©ìžë³„ ê¶Œí•œì„ ê³ ë ¤í•œ RAG ë¬¸ì„œ ê²€ìƒ‰ ì„œë¹„ìŠ¤
# - íšŒì‚¬ ê³µê°œ ë¬¸ì„œ: ê°™ì€ íšŒì‚¬ ì§ì› ëª¨ë‘ ì ‘ê·¼ ê°€ëŠ¥
# - ê°œì¸ ë¬¸ì„œ: ë³¸ì¸ë§Œ ì ‘ê·¼ ê°€ëŠ¥
# - ë‹¤ë¥¸ ì§ì›ì˜ ê°œì¸ ë¬¸ì„œ: ì ‘ê·¼ ë¶ˆê°€

# - TF-IDF í¬ì†Œ ê²€ìƒ‰ + ê¶Œí•œ ì‚¬í›„ í•„í„° + RRF ê²°í•©
# - í¬ì†Œ ì¸ë±ìŠ¤ê°€ ë¡œì»¬ì— ì—†ìœ¼ë©´ S3ì—ì„œ ìžë™ ë‹¤ìš´ë¡œë“œ í›„ ìºì‹œ
# - ì§ˆì˜ í™•ìž¥(ë™ì˜ì–´/í”Œëž«í¼ ì •ê·œí™” + PRF) â†’ dense/sparseì— ë™ì¼ ì ìš©

# ì¶”ê°€:
# - ìœ ì‚¬ë„ ìž„ê³„ê°’ ì»· + ë¬¸ì„œ/ìœ í˜• ì¿¼í„°ë§
# - Cross-encoder Rerank
# - LLM Multi-Query
# - ì¶œì²˜/íŽ˜ì´ì§€ dedup + ë¬¸ìž¥ ì¤‘ë³µ ì œê±°
# """

# import os
# import re
# import json
# from pathlib import Path
# from typing import List, Tuple, Optional, Dict, Any
# from sqlalchemy.orm import Session
# from dotenv import load_dotenv

# # í¬ì†Œ ê²€ìƒ‰(ìžˆìœ¼ë©´ ì‚¬ìš©)
# try:
#     from sklearn.metrics.pairwise import cosine_similarity
#     import joblib
# except Exception:
#     cosine_similarity = None
#     joblib = None

# # Cross-encoder (ì„ íƒ)
# try:
#     from sentence_transformers import CrossEncoder
# except Exception:
#     CrossEncoder = None

# load_dotenv()

# from langchain_openai import ChatOpenAI, OpenAIEmbeddings
# from langchain_chroma import Chroma
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from langchain_core.tools import tool
# from langchain_core.documents import Document

# from app.utils.db import get_db
# from app.features.login.employee_google import crud as employee_crud
# from app.features.admin.services.file_ingest_service import _get_company_code
# from app.rag.internal_data_rag.internal_retrieve import (
#     _stable_similarity,
#     _truncate_context_blocks,
#     MAX_CONTEXT_CHARS,
#     CHROMA_PATH,
#     EMBED_MODEL,
#     CHAT_MODEL,
# )

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # í™˜ê²½/ì˜µì…˜ (internal_retrieve.pyì™€ ë™ì¼í•˜ê²Œ)
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SPARSE_INDEX_PATH = os.getenv("SPARSE_INDEX_PATH", os.path.join(CHROMA_PATH, "sparse"))

# # ìž„ê³„/ì¿¼í„°/ë¦¬ëž­í¬ (internal_retrieve.pyì™€ ë™ì¼)
# SIM_THRESHOLD = float(os.getenv("SIM_THRESHOLD", "0.25"))
# DOC_QUOTA = int(os.getenv("DOC_QUOTA", "15"))
# KIND_QUOTA = os.getenv("KIND_QUOTA", "text:12,image_ocr:8,image_vlm:4")
# ENABLE_RERANK = os.getenv("ENABLE_RERANK", "true").lower() == "true"
# CROSS_ENCODER_MODEL = os.getenv("CROSS_ENCODER_MODEL", "cross-encoder/ms-marco-MultiBERT-L-12")
# # CE ìž…ë ¥ ìµœì í™”
# CE_MAX_CHARS = int(os.getenv("CE_MAX_CHARS", "1800"))
# CE_BATCH_SIZE = int(os.getenv("CE_BATCH_SIZE", "16"))

# # LLM Multi-Query (ë³´ìˆ˜í™”)
# ENABLE_LLM_MULTIQUERY = os.getenv("ENABLE_LLM_MULTIQUERY", "true").lower() == "true"
# N_QUERY_REWRITES = int(os.getenv("N_QUERY_REWRITES", "1"))  # ê¸°ë³¸ 1ë¡œ ì™„í™”
# SHORT_QUERY_CHAR = int(os.getenv("SHORT_QUERY_CHAR", "40"))

# # ë””ë“‘ íŒŒë¼ë¯¸í„°
# MAX_PER_PAGE = int(os.getenv("MAX_PER_PAGE", "2"))  # íŽ˜ì´ì§€ë‹¹ ìµœëŒ€ ìœ ì§€ ê°œìˆ˜

# # S3(ì˜µì…˜) â€” í¬ì†Œ ì¸ë±ìŠ¤ ë™ê¸°í™” (ì´ë¦„ í†µì¼)
# S3_BUCKET = os.getenv("S3_BUCKET")
# S3_REGION = os.getenv("S3_REGION")
# AWS_S3_PREFIX = os.getenv("AWS_S3_PREFIX", "rag/sparse")

# # ì§ˆì˜ ë™ì˜ì–´ ì‚¬ì „(ì™¸ë¶€ íŒŒì¼ ê²½ë¡œ - ì„ íƒ)
# QUERY_SYNONYM_PATH = os.getenv("QUERY_SYNONYM_PATH")  # e.g. ./config/synonyms.json

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # NEW: ì§ˆì˜ í™•ìž¥ ìœ í‹¸(ë™ì˜ì–´/ì •ê·œí™” + PRF)
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def _load_synonym_dict() -> dict:
#     try:
#         if QUERY_SYNONYM_PATH and Path(QUERY_SYNONYM_PATH).exists():
#             with open(QUERY_SYNONYM_PATH, "r", encoding="utf-8") as f:
#                 return json.load(f)
#     except Exception:
#         pass
#     return {
#         "ìœ íŠœë¸Œ": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ê°œì¸ë°©ì†¡", "ì˜ìƒ ì—…ë¡œë“œ"],
#         "ë¸Œì´ë¡œê·¸": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ê°œì¸ë°©ì†¡", "ì˜ìƒ ì œìž‘", "ë™ì˜ìƒ ì—…ë¡œë“œ"],
#         "í‹±í†¡": ["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ì†Œì…œë¯¸ë””ì–´"],
#         "ì¸ìŠ¤íƒ€": ["ì†Œì…œë¯¸ë””ì–´", "SNS", "ì½˜í…ì¸  ì—…ë¡œë“œ"],
#         "ë¼ì´ë¸Œ": ["ì‹¤ì‹œê°„ ë°©ì†¡", "ë¼ì´ë¸Œ ìŠ¤íŠ¸ë¦¬ë°", "ì¸í„°ë„· ê°œì¸ë°©ì†¡"],
#         "ìˆ˜ìµ": ["ê´‘ê³ ìˆ˜ìµ", "ìˆ˜ìµ ì°½ì¶œ", "í˜‘ì°¬", "ê°„ì ‘ê´‘ê³ ", "PPL"],
#         "í˜‘ì°¬": ["ê°„ì ‘ê´‘ê³ ", "PPL", "ëŒ€ê°€ì„± ì œê³µ"],
#         "ê²¸ì—…": ["ê²¸ì—…í—ˆê°€", "ê²¸ì—…ì˜ í—ˆê°€", "ë¶€ì—…"],
#         "ë³´ì•ˆ": ["ì§ë¬´ìƒ ë¹„ë°€", "ì˜ì—…ë¹„ë°€", "ì •ë³´ë³´ì•ˆ"],
#         "ëª…ì˜ˆí›¼ì†": ["ëª…ì˜ˆ ì¹¨í•´", "ê¶Œë¦¬ ì¹¨í•´"],
#         # ì¸ì‚¬/ê²½ë ¥ ê´€ë ¨ ë™ì˜ì–´ ì¶”ê°€
#         "ì´ë ¥ì‚¬": ["ì´ë ¥ì„œ", "ê²½ë ¥", "ê²½í—˜", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„", "ê°œì¸ì •ë³´"],
#         "ì´ë ¥ì„œ": ["ì´ë ¥ì‚¬", "ê²½ë ¥ì„œ", "ê²½ë ¥", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„", "ê°œì¸ì •ë³´"],
#         "ê²½ë ¥": ["ì´ë ¥ì„œ", "ì´ë ¥ì‚¬", "ê²½í—˜", "ì»¤ë¦¬ì–´", "í”„ë¡œí•„"],
#         "ê²½í—˜": ["ì´ë ¥ì„œ", "ì´ë ¥ì‚¬", "ê²½ë ¥", "ì»¤ë¦¬ì–´"],
#     }

# _SYNONYM_DICT = _load_synonym_dict()
# _NORMALIZE_MAP = {
#     "youtube": "ìœ íŠœë¸Œ",
#     "yt": "ìœ íŠœë¸Œ",
#     "vlog": "ë¸Œì´ë¡œê·¸",
#     "shorts": "ì‡¼ì¸ ",
#     "tiktok": "í‹±í†¡",
#     "instagram": "ì¸ìŠ¤íƒ€",
# }

# def _normalize_terms(q: str) -> str:
#     low = q.lower()
#     for a, b in _NORMALIZE_MAP.items():
#         low = re.sub(rf"\b{re.escape(a)}\b", b, low)
#     return low

# _STOP = set(["ë°","ë˜ëŠ”","ê·¸ë¦¬ê³ ","ê´€ë ¨","ê´€ë ¨ëœ","ì—¬ë¶€","ê²ƒ","ìˆ˜","ë“±","í•´ë‹¹","í•˜ëŠ”","ê²½ìš°","ëŒ€í•œ","ìœ¼ë¡œ","ì—ì„œ","í•˜ë‹¤"])
# def _extract_keywords(text: str, top_k: int = 6) -> list:
#     toks = re.split(r"[^ê°€-íž£A-Za-z0-9]+", text)
#     cnt: Dict[str, int] = {}
#     for t in toks:
#         if len(t) < 2 or t in _STOP:
#             continue
#         cnt[t] = cnt.get(t, 0) + 1
#     return [w for w,_ in sorted(cnt.items(), key=lambda x: x[1], reverse=True)[:top_k]]

# def _expand_query_base(q: str) -> str:
#     qn = _normalize_terms(q)
#     aug: List[str] = []
#     for k, vs in _SYNONYM_DICT.items():
#         if k in qn:
#             aug.extend(vs)
#     if any(x in qn for x in ["ìœ íŠœë¸Œ","ë¸Œì´ë¡œê·¸","í‹±í†¡","ì¸ìŠ¤íƒ€","ë¼ì´ë¸Œ","ì‡¼ì¸ "]):
#         aug.extend(["ì¸í„°ë„· ê°œì¸ë°©ì†¡", "ë™ì˜ìƒ ê³µìœ  ì„œë¹„ìŠ¤", "ê²¸ì—…í—ˆê°€", "ì œ82ì¡°", "ì¸í„°ë„· ê°œì¸ë°©ì†¡ í™œë™ ì œí•œ"])
#     aug = list(dict.fromkeys([v for v in aug if v not in q]))  # ì¤‘ë³µ ì œê±°
#     return q if not aug else f"{q} " + " ".join(aug)

# def _augment_with_prf(vectorstore, q: str, k_init: int = 4, kw_top: int = 6) -> str:
#     try:
#         hits = vectorstore.similarity_search_with_score(q, k=k_init)
#         if not hits:
#             return q
#         blob = "\n".join([d.page_content for d,_ in hits])
#         kws = _extract_keywords(blob, top_k=kw_top)
#         kws = [w for w in kws if w not in q]
#         return f"{q} " + " ".join(kws) if kws else q
#     except Exception:
#         return q

# def expand_query(vectorstore, query: str) -> str:
#     q1 = _expand_query_base(query)
#     q2 = _augment_with_prf(vectorstore, q1)
#     if q2 != query:
#         print(f"ðŸ§© ì¿¼ë¦¬ í™•ìž¥: '{query}' â†’ '{q2}'")
#     return q2

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # S3 ë‹¤ìš´ë¡œë“œ ìœ í‹¸ â€” í¬ì†Œ ì¸ë±ìŠ¤ ë¯¸ì¡´ìž¬ ì‹œ ì›ê²©ì—ì„œ í™•ë³´
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def _s3_configured() -> bool:
#     return all(os.getenv(k) for k in ["S3_BUCKET", "S3_REGION", "AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"])

# def _download_sparse_from_s3(local_docs_fp: str, local_tfidf_fp: str, collection_name: str):
#     if not _s3_configured():
#         return False
#     try:
#         import boto3
#         s3 = boto3.client(
#             "s3",
#             region_name=os.getenv("S3_REGION"),
#             aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
#             aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
#         )
#         prefix = os.getenv("AWS_S3_PREFIX", "rag/sparse")
#         key_docs = f"{prefix}/{collection_name}_docs.joblib"
#         key_tfidf = f"{prefix}/{collection_name}_tfidf.joblib"
#         bucket = os.getenv("S3_BUCKET")
#         s3.download_file(bucket, key_docs, local_docs_fp)
#         s3.download_file(bucket, key_tfidf, local_tfidf_fp)
#         print(f"â˜ï¸  S3ì—ì„œ í¬ì†Œ ì¸ë±ìŠ¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ â†’ {local_docs_fp}, {local_tfidf_fp}")
#         return True
#     except Exception as e:
#         print(f"âš ï¸ S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨(í¬ì†Œ ê²€ìƒ‰ ë¹„í™œì„±): {e}")
#         return False

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # í¬ì†Œ ê²€ìƒ‰ê¸°(TF-IDF)
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# class _SparseSearcher:
#     def __init__(self, base_dir: str, collection_name: str):
#         os.makedirs(base_dir, exist_ok=True)
#         self.collection_name = collection_name
#         self.data_fp = os.path.join(base_dir, f"{collection_name}_docs.joblib")
#         self.index_fp = os.path.join(base_dir, f"{collection_name}_tfidf.joblib")
#         self.enabled = joblib is not None and cosine_similarity is not None
#         self.loaded = False
#         self.ids = []; self.docs = []; self.metas = []
#         self.vectorizer = None; self.mat = None

#     def _ensure_local(self):
#         if os.path.exists(self.data_fp) and os.path.exists(self.index_fp):
#             return
#         _download_sparse_from_s3(self.data_fp, self.index_fp, self.collection_name)

#     def _lazy_load(self):
#         if self.loaded or not self.enabled:
#             return
#         self._ensure_local()
#         if not (os.path.exists(self.data_fp) and os.path.exists(self.index_fp)):
#             self.enabled = False
#             print("â„¹ï¸ TF-IDF ë¯¸í™œì„±í™”(ë¡œì»¬ .joblib ì—†ìŒ) â†’ ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš©.")
#             return
#         self.ids, self.docs, self.metas = joblib.load(self.data_fp)
#         self.vectorizer, self.mat = joblib.load(self.index_fp)
#         self.loaded = True
#         print(f"âœ… TF-IDF í™œì„±í™”: {self.collection_name} (docs={len(self.ids)})")

#     def search(self, query: str, k: int = 20) -> List[Tuple[Document, float]]:
#         if not self.enabled:
#             return []
#         self._lazy_load()
#         if not self.loaded:
#             return []
#         qv = self.vectorizer.transform([query])
#         sims = cosine_similarity(qv, self.mat).ravel()
#         idxs = sims.argsort()[::-1][:k]
#         out = []
#         for i in idxs:
#             out.append((Document(page_content=self.docs[i], metadata=self.metas[i]), float(1 - sims[i])))
#         return out

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # RRF ê²°í•© + ë„ìš°ë¯¸ (internal_retrieve.pyì™€ ë™ì¼)
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def _rrf_fusion(dense: List[Tuple[Document, float]], sparse: List[Tuple[Document, float]], k_rrf: int = 60, top_k: int = 6):
#     """
#     ì—¬ëŸ¬ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸(dense/sparse)ë¥¼ RRFë¡œ ê²°í•©.
#     - meta['rrf_score'] : ì›ì‹œ RRF ëˆ„ì ê°’
#     - meta['rrf_sim']   : 0~1 ì •ê·œí™” ìœ ì‚¬ë„
#     - ë°˜í™˜ distance     : 1 - rrf_sim (ìž‘ì„ìˆ˜ë¡ ê°€ê¹ë‹¤)
#     """
#     pools: Dict[str, Dict[str, Any]] = {}
#     for rank, (doc, _) in enumerate(dense, start=1):
#         key = (doc.metadata or {}).get("chunk_hash") or f"d:{rank}:{hash(doc.page_content)%10_000_000}"
#         pools.setdefault(key, {"doc": doc, "rrf": 0.0})
#         pools[key]["rrf"] += 1.0 / (k_rrf + rank)
#     for rank, (doc, _) in enumerate(sparse, start=1):
#         key = (doc.metadata or {}).get("chunk_hash") or f"s:{rank}:{hash(doc.page_content)%10_000_000}"
#         pools.setdefault(key, {"doc": doc, "rrf": 0.0})
#         pools[key]["rrf"] += 1.0 / (k_rrf + rank)

#     fused = sorted(pools.values(), key=lambda x: x["rrf"], reverse=True)[:top_k]
#     max_rrf = max((f["rrf"] for f in fused), default=1.0)

#     out: List[Tuple[Document, float]] = []
#     for f in fused:
#         doc = f["doc"]
#         rrf = f["rrf"]
#         rrf_sim = rrf / max_rrf if max_rrf > 0 else 0.0
#         dist = 1.0 - rrf_sim
#         meta = dict(doc.metadata or {})
#         meta["rrf_score"] = rrf
#         meta["rrf_sim"] = rrf_sim
#         doc.metadata = meta
#         out.append((doc, dist))
#     return out

# def _parse_kind_quota(s: str) -> Dict[str, int]:
#     out: Dict[str, int] = {}
#     for tok in s.split(","):
#         if ":" in tok:
#             k, v = tok.split(":", 1)
#             try:
#                 out[k.strip()] = int(v)
#             except:
#                 pass
#     return out

# _KIND_QUOTA = _parse_kind_quota(KIND_QUOTA)
# _cross_encoder = None
# def _get_cross_encoder():
#     global _cross_encoder
#     if _cross_encoder is None and CrossEncoder and ENABLE_RERANK:
#         try:
#             _cross_encoder = CrossEncoder(CROSS_ENCODER_MODEL)
#             print(f"âœ… Cross-encoder ë¡œë“œ: {CROSS_ENCODER_MODEL}")
#         except Exception as e:
#             print(f"âš ï¸ Cross-encoder ë¡œë“œ ì‹¤íŒ¨: {e}")
#     return _cross_encoder

# def _adaptive_threshold(sims: List[float], base: float) -> float:
#     if not sims:
#         return base
#     xs = sorted(sims)
#     q30 = xs[int(len(xs)*0.30)]
#     thr = max(base, min(q30, 0.35))  # cap 0.35
#     return thr

# def _apply_threshold_and_quota(pairs: List[Tuple[Document, float]], top_k: int) -> List[Tuple[Document, float]]:
#     """
#     - rrf_sim(ê°€ëŠ¥ ì‹œ) ê¸°ë°˜ ìž„ê³„ê°’ ì»·, ì—†ìœ¼ë©´ distanceâ†’_stable_similarity ë°±ì—….
#     - ë¶„í¬ ê¸°ë°˜ ì ì‘í˜• ìž„ê³„ê°’ ì‚¬ìš©(í•˜í•œ: SIM_THRESHOLD).
#     - source/kind ì¿¼í„° ì ìš©.
#     - ì»· ì‚¬ìœ  ì¹´ìš´íŠ¸ ë¡œê¹….
#     """
#     scored_meta = []
#     for doc, dist in pairs:
#         meta = doc.metadata or {}
#         sim = meta.get("rrf_sim", _stable_similarity(dist))
#         scored_meta.append((doc, dist, sim))

#     sims = [s for _, _, s in scored_meta]
#     thr = _adaptive_threshold(sims, SIM_THRESHOLD)

#     scored = []
#     dropped_reason_counts = {"threshold": 0, "doc_quota": 0, "kind_quota": 0}

#     for doc, dist, sim in scored_meta:
#         if sim >= thr:
#             scored.append((doc, dist, sim))
#         else:
#             dropped_reason_counts["threshold"] += 1

#     by_src: Dict[str, int] = {}
#     by_kind: Dict[str, int] = {}
#     kept: List[Tuple[Document, float]] = []

#     for doc, dist, sim in sorted(scored, key=lambda x: x[2], reverse=True):
#         meta = doc.metadata or {}
#         src = meta.get("source", "unknown")
#         kind = meta.get("content_kind") or meta.get("content_type") or "text"

#         if by_src.get(src, 0) >= DOC_QUOTA:
#             dropped_reason_counts["doc_quota"] += 1
#             continue
#         if _KIND_QUOTA.get(kind, 10**9) <= by_kind.get(kind, 0):
#             dropped_reason_counts["kind_quota"] += 1
#             continue

#         kept.append((doc, dist))
#         by_src[src] = by_src.get(src, 0) + 1
#         by_kind[kind] = by_kind.get(kind, 0) + 1
#         if len(kept) >= top_k:
#             break

#     print(f"ìž„ê³„/ì¿¼í„° ì ìš© ê²°ê³¼: thr={thr:.3f}, kept={len(kept)}, dropped={dropped_reason_counts}")
#     return kept

# def _dedup_by_source_page(pairs: List[Tuple[Document, float]]) -> List[Tuple[Document, float]]:
#     """
#     (source, page, sheet) ê¸°ì¤€ ë³´ìˆ˜ì  ë””ë“‘.
#     - pageê°€ Noneì´ë©´ ë””ë“‘í•˜ì§€ ì•ŠìŒ(ë©”íƒ€ ë¶€ì‹¤ ë³´í˜¸)
#     - íŽ˜ì´ì§€ë‹¹ ìµœëŒ€ MAX_PER_PAGEê°œê¹Œì§€ í—ˆìš©
#     """
#     counts: Dict[Tuple[Any, Any, Any], int] = {}
#     out = []
#     for doc, dist in pairs:
#         m = doc.metadata or {}
#         src = m.get("source")
#         page = m.get("page")
#         sheet = m.get("sheet")
#         if page is None:
#             out.append((doc, dist))
#             continue
#         key = (src, page, sheet)
#         if counts.get(key, 0) >= MAX_PER_PAGE:
#             continue
#         counts[key] = counts.get(key, 0) + 1
#         out.append((doc, dist))
#     return out

# # ë¸”ë¡ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°(ë™ì¼ ë¼ì¸ 3íšŒì§¸ë¶€í„° ì œê±°)
# def _dedup_sentences_blockwise(text: str) -> str:
#     blocks = [b for b in re.split(r"\n\s*---\s*\n", text) if b.strip()]
#     new_blocks = []
#     for b in blocks:
#         lines = [ln for ln in b.splitlines()]
#         seen: Dict[str, int] = {}
#         acc = []
#         for ln in lines:
#             key = ln.strip().lower()
#             c = seen.get(key, 0) + 1
#             seen[key] = c
#             if c > 2:
#                 continue
#             acc.append(ln)
#         new_blocks.append("\n".join(acc))
#     return "\n\n---\n\n".join(new_blocks)

# # LLM Multi-Query ìž¬ìž‘ì„± (ë³´ìˆ˜í™”)
# def _should_multiquery(q: str) -> bool:
#     return ENABLE_LLM_MULTIQUERY and len(q) <= SHORT_QUERY_CHAR

# def _llm_rewrites(llm: ChatOpenAI, query: str, n: int) -> List[str]:
#     if not _should_multiquery(query) or n <= 0:
#         return []
#     try:
#         prompt = (
#             "ë‹¤ìŒ ì§ˆë¬¸ì„ ë¬¸ì„œ ê²€ìƒ‰ì— ê°•ê±´í•œ í˜•íƒœë¡œ {n}ê°œ ìž¬ìž‘ì„±í•˜ì„¸ìš”.\n"
#             "- í•µì‹¬ í‚¤ì›Œë“œëŠ” ìœ ì§€í•˜ë˜ í‘œí˜„ì„ ë‹¤ì–‘í™”(ë™ì˜ì–´/ì „ë¬¸ìš©ì–´)\n"
#             "- í•œêµ­ì–´ ê³µì‹ ë¬¸ì„œ ìš©ì–´ ì„ í˜¸\n"
#             "- í•œ ì¤„ì— í•˜ë‚˜ì”© ì¶œë ¥"
#         ).format(n=n)
#         txt = llm.invoke(prompt + "\n\nì§ˆë¬¸: " + query).content
#         rewrites = [line.strip("-â€¢ ").strip() for line in txt.splitlines() if line.strip()]
#         uniq, seen = [], set()
#         for r in rewrites:
#             k = r.lower()
#             if k not in seen:
#                 seen.add(k); uniq.append(r)
#         return uniq[:n]
#     except Exception:
#         return []

# def _multi_query_dense(vectorstore, base_q: str, llm: ChatOpenAI, top_each: int) -> List[Tuple[Document, float]]:
#     qs = [base_q]
#     qs.extend(_llm_rewrites(llm, base_q, N_QUERY_REWRITES))
#     qs = list(dict.fromkeys([q for q in qs if q]))
#     pool: List[Tuple[Document, float]] = []
#     for i, q in enumerate(qs):
#         results = vectorstore.similarity_search_with_score(q, k=top_each)
#         weight = 2.0 if i == 0 else 1.0  # ì›ë³¸ ì¿¼ë¦¬ ê°€ì¤‘
#         pool.extend([(doc, dist / weight) for doc, dist in results])
#     return _rrf_fusion(pool, [], k_rrf=60, top_k=min(6, len(qs) * top_each))  # ì†Œê·œëª¨ RRF

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # ê¶Œí•œ ì„œë¹„ìŠ¤
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# class UserAwareRAGService:
#     def __init__(self):
#         self.embeddings = OpenAIEmbeddings(model=EMBED_MODEL)
#         self.llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)
#         self.parser = StrOutputParser()
#         self.prompt = ChatPromptTemplate.from_messages(
#             [
#                 (
#                     "system",
#                     "ë‹¹ì‹ ì€ ì‚¬ë‚´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•ížˆ ë‹µí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. "
#                     "ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹µë³€í•˜ê³ , "
#                     "ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥´ëŠ” ë‚´ìš©ì€ 'ëª¨ë¥¸ë‹¤'ê³  ëª…í™•ížˆ ë§í•˜ì„¸ìš”. "
#                     "ê°€ëŠ¥í•œ í•œ ì¶œì²˜ ë¬¸ì„œëª…ê³¼ í•¨ê»˜ ë‹µë³€í•˜ì„¸ìš”.",
#                 ),
#                 ("user", "ì§ˆë¬¸: {question}\n\nì°¸ê³  ì»¨í…ìŠ¤íŠ¸(ì—¬ëŸ¬ ì²­í¬):\n{context}"),
#             ]
#         )

#     def get_user_context(self, user_id: str) -> Optional[dict]:
#         try:
#             db = next(get_db())
#             employee = employee_crud.get_employee_by_google_id(db, google_user_id=user_id)
#             if not employee:
#                 print(f"âŒ ì‚¬ìš©ìžë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {user_id}")
#                 return None
#             company_code = _get_company_code(db, employee.company_id)
#             return {"employee_id": employee.id, "company_id": employee.company_id, "company_code": company_code}
#         except Exception as e:
#             print(f"âŒ ì‚¬ìš©ìž ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
#             return None
#         finally:
#             db.close()

#     def _allow_meta(self, meta: dict, company_id: int, employee_id: int) -> bool:
#         if not isinstance(meta, dict):
#             return False
#         if meta.get("company_id") != company_id:
#             return False
#         if meta.get("is_private") is True:
#             return meta.get("user_id") == employee_id
#         return True

#     def retrieve_documents_with_permission(
#         self, query: str, user_id: str, top_k: int = 6
#     ) -> List[Tuple[str, dict]]:
#         user_context = self.get_user_context(user_id)
#         if not user_context:
#             print(f"âŒ ì‚¬ìš©ìž ì •ë³´ ì—†ìŒ - ê²€ìƒ‰ ì¤‘ë‹¨: {user_id}")
#             return []

#         company_code = user_context["company_code"]
#         company_id = user_context["company_id"]
#         employee_id = user_context["employee_id"]

#         try:
#             # íšŒì‚¬ë³„ ë²¡í„°ìŠ¤í† ì–´
#             import chromadb
#             client = chromadb.CloudClient(
#                 tenant=os.getenv("CHROMA_TENANT"),
#                 database=os.getenv("CHROMA_DATABASE"),
#                 api_key=os.getenv("CHROMA_API_KEY"),
#             )
#             vectorstore = Chroma(
#                 client=client,
#                 collection_name=company_code,
#                 embedding_function=self.embeddings,
#             )

#             # ì¿¼ë¦¬ í™•ìž¥
#             q_exp = expand_query(vectorstore, query)
#             print(f"ðŸ” ê¶Œí•œë³„ ë¬¸ì„œ ê²€ìƒ‰(í•˜ì´ë¸Œë¦¬ë“œ RRF): '{q_exp}' (íšŒì‚¬: {company_code}, ì‚¬ìš©ìž: {employee_id})")

#             # ê¶Œí•œë³„ í•„í„°
#             company_filter = {"$and": [{"company_id": {"$eq": company_id}}, {"is_private": {"$eq": False}}]}
#             personal_filter = {"$and": [{"company_id": {"$eq": company_id}}, {"is_private": {"$eq": True}}, {"user_id": {"$eq": employee_id}}]}

#             # dense: LLM multi-query ìž¬ìž‘ì„±(ë³´ìˆ˜í™”) â†’ ì†Œê·œëª¨ RRF ìœµí•©
#             # ê¶Œí•œë³„ í•„í„°ë¡œ ê²€ìƒ‰
#             dense_public_pool = []
#             dense_personal_pool = []
            
#             # Multi-query ìƒì„±
#             queries = [q_exp]
#             queries.extend(_llm_rewrites(self.llm, q_exp, N_QUERY_REWRITES))
#             queries = list(dict.fromkeys([q for q in queries if q]))
            
#             for i, q in enumerate(queries):
#                 weight = 2.0 if i == 0 else 1.0  # ì›ë³¸ ì¿¼ë¦¬ ê°€ì¤‘
#                 # íšŒì‚¬ ê³µê°œ ë¬¸ì„œ ê²€ìƒ‰
#                 public_results = vectorstore.similarity_search_with_score(q, k=max(3, top_k//2), filter=company_filter)
#                 dense_public_pool.extend([(doc, dist / weight) for doc, dist in public_results])
#                 # ê°œì¸ ë¬¸ì„œ ê²€ìƒ‰
#                 personal_results = vectorstore.similarity_search_with_score(q, k=max(3, top_k//2), filter=personal_filter)
#                 dense_personal_pool.extend([(doc, dist / weight) for doc, dist in personal_results])
            
#             dense_pool = dense_public_pool + dense_personal_pool

#             # sparse: í¬ì†Œ ê²€ìƒ‰(TF-IDF) â€“ íšŒì‚¬ë³„ ì¸ë±ìŠ¤ í›„ ê¶Œí•œ ì‚¬í›„ í•„í„°
#             sparse_searcher = _SparseSearcher(SPARSE_INDEX_PATH, company_code)
#             sparse_all = sparse_searcher.search(q_exp, k=max(12, top_k * 2))
#             sparse = [(d, dist) for (d, dist) in sparse_all if self._allow_meta(d.metadata or {}, company_id, employee_id)]

#             # 1) RRF ê²°í•© (í›„ë³´êµ° ì¶©ë¶„ í™•ë³´)
#             fused = _rrf_fusion(dense_pool, sparse, k_rrf=60, top_k=max(top_k * 3, 20))

#             # 1-1) íŽ˜ì´ì§€/ì¶œì²˜ ì¤‘ë³µ ì œê±°(ë³´ìˆ˜ì )
#             fused = _dedup_by_source_page(fused)

#             # 2) ìž„ê³„ + ì¿¼í„°(ì ì‘í˜• ì»·)
#             fused = _apply_threshold_and_quota(fused, top_k=max(top_k * 2, 20))

#             # 3) CE ë¦¬ëž­í¬ â†’ ìµœì¢… top_k
#             fused = self._rerank(query, fused, top_k)

#             if not fused:
#                 print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
#                 return []

#             contexts: List[Tuple[str, dict]] = []
#             print(f"âœ… ìµœì¢… ìƒìœ„ {len(fused)}ê°œ ê²°ê³¼")
#             for i, (doc, distance) in enumerate(fused, start=1):
#                 meta = dict(doc.metadata or {})
#                 sim = meta.get("rrf_sim", _stable_similarity(distance))
#                 meta["similarity_score"] = sim
#                 is_private = meta.get("is_private", False)
#                 doc_type = "ê°œì¸ ë¬¸ì„œ" if is_private else "íšŒì‚¬ ë¬¸ì„œ"
#                 preview = (doc.page_content[:100] + "...") if len(doc.page_content) > 100 else doc.page_content
#                 print(
#                     f"  [Rank {i}] sim={sim:.4f}, {doc_type}, src={meta.get('source')} "
#                     f"sheet={meta.get('sheet')} page={meta.get('page')} "
#                     f"chunk={meta.get('chunk_idx')} kind={meta.get('content_kind') or meta.get('content_type')}"
#                 )
#                 print(f"          ë‚´ìš©: {preview}")
#                 contexts.append((doc.page_content, meta))

#             return contexts

#         except Exception as e:
#             print(f"âŒ ê¶Œí•œë³„ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#             return []

#     def _clip(self, txt: str, max_chars: int) -> str:
#         return txt if len(txt) <= max_chars else txt[:max_chars]

#     def _rerank(self, query: str, pairs: List[Tuple[Document, float]], top_k: int) -> List[Tuple[Document, float]]:
#         # MultiBERTëŠ” ë‹¤êµ­ì–´ ì§€ì› â†’ í•œêµ­ì–´ í¬í•¨ ì§ˆì˜ë„ CE ì ìš©
#         ce = _get_cross_encoder()
#         if not ce or not pairs:
#             return pairs[:top_k]
#         try:
#             texts = [(query, self._clip(d.page_content, CE_MAX_CHARS)) for d, _ in pairs]
#             # sentence-transformers CrossEncoderëŠ” batch_size ì¸ìžë¥¼ ì§€ì›
#             scores = ce.predict(texts, batch_size=CE_BATCH_SIZE)
#             ranked = sorted(zip(pairs, scores), key=lambda x: x[1], reverse=True)[:top_k]
#             return [p for (p, _) in ranked]
#         except Exception as e:
#             print(f"âš ï¸ Cross-encoder ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
#             return pairs[:top_k]

#     def generate_answer(self, query: str, contexts: List[Tuple[str, dict]], model: str = None) -> str:
#         if not contexts:
#             return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
#         try:
#             def _rank_key(item):
#                 _, meta = item
#                 kind = (meta.get("content_kind") or meta.get("content_type") or "text")
#                 pri = 2
#                 if kind in ("image_vlm",): pri = 0
#                 if kind in ("image_ocr", "text"): pri = 3
#                 return (pri, meta.get("similarity_score", 0.0))

#             contexts = sorted(contexts, key=_rank_key, reverse=True)
#             model_label = model or getattr(self.llm, "model", getattr(self.llm, "model_name", "unknown"))
#             print(f"âš™ï¸ ë‹µë³€ ìƒì„± ì¤‘... (ì»¨í…ìŠ¤íŠ¸ {len(contexts)}ê°œ, ëª¨ë¸: {model_label})")
#             context_text = _truncate_context_blocks(contexts, max_chars=MAX_CONTEXT_CHARS)
#             # ë¸”ë¡ ë‹¨ìœ„ ì¤‘ë³µ ì œê±°(3íšŒì§¸ë¶€í„° ì œê±°)
#             context_text = _dedup_sentences_blockwise(context_text)
#             used_llm = self.llm if model is None else ChatOpenAI(model=model, temperature=0)
#             chain = self.prompt | used_llm | self.parser
#             answer = chain.invoke({"question": query, "context": context_text})
#             print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
#             return answer
#         except Exception as e:
#             print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#             return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

#     def query_rag_with_permission(self, query: str, user_id: str, top_k: int = 6, model: str = None, show_sources: bool = True) -> str:
#         print(f"\nðŸ” ê¶Œí•œë³„ RAG ì§ˆì˜: {query} (ì‚¬ìš©ìž: {user_id})")
#         contexts = self.retrieve_documents_with_permission(query, user_id, top_k)
#         if not contexts:
#             return "ì£„ì†¡í•©ë‹ˆë‹¤. ì ‘ê·¼ ê°€ëŠ¥í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#         answer = self.generate_answer(query, contexts, model)
#         if show_sources:
#             sources = []
#             for _, meta in contexts:
#                 is_private = meta.get("is_private", False)
#                 doc_type = "ê°œì¸ ë¬¸ì„œ" if is_private else "íšŒì‚¬ ë¬¸ì„œ"
#                 src = f"- {meta.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')} ({doc_type}, ì²­í¬ {meta.get('chunk_idx', 'N/A')})"
#                 if src not in sources:
#                     sources.append(src)
#             return f"{answer}\n\nðŸ“‹ ì°¸ê³ í•œ ë¬¸ì„œ:\n" + "\n".join(sources)
#         return answer

# # ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
# _user_aware_rag_service = UserAwareRAGService()

# # ========================= LangChain ë„êµ¬ ì •ì˜ =========================
# @tool
# def user_aware_rag_search(query: str, user_id: str) -> str:
#     """ì‚¬ìš©ìžë³„ ê¶Œí•œì„ ê³ ë ¤í•œ RAG ê²€ìƒ‰/ë‹µë³€ (íšŒì‚¬ ê³µê°œ + ë³¸ì¸ ê°œì¸ ë¬¸ì„œ)."""
#     try:
#         return _user_aware_rag_service.query_rag_with_permission(
#             query=query, user_id=user_id, top_k=6, show_sources=True
#         )
#     except Exception as e:
#         print(f"âŒ ì‚¬ìš©ìžë³„ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
#         return f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# # ========================= ì‚¬ìš©ìžë³„ ë„êµ¬ íŒ©í† ë¦¬ =========================
# def create_user_aware_rag_tools(user_id: str) -> list:
#     """íŠ¹ì • ì‚¬ìš©ìžë¥¼ ìœ„í•œ RAG ë„êµ¬ ìƒì„± (user_id ë°”ì¸ë”©)."""
#     print(f"ðŸ”§ ì‚¬ìš©ìžë³„ RAG ë„êµ¬ ìƒì„± ì¤‘: {user_id}")

#     @tool
#     def internal_rag_search(query: str) -> str:
#         """ì—…ë¡œë“œëœ íšŒì‚¬/ê°œì¸ ë¬¸ì„œì—ì„œ ê²€ìƒ‰/ë‹µë³€(í•˜ì´ë¸Œë¦¬ë“œ)."""
#         print(f"ðŸ” internal_rag_search í˜¸ì¶œë¨: query='{query}', user_id='{user_id}'")
#         try:
#             result = _user_aware_rag_service.query_rag_with_permission(
#                 query=query, user_id=user_id, top_k=6, show_sources=True
#             )
#             return result
#         except Exception as e:
#             print(f"âŒ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
#             return f"ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

#     print(f"âœ… RAG ë„êµ¬ ìƒì„± ì™„ë£Œ: {user_id}")
#     return [internal_rag_search]

# # ê¸°ì¡´ ë²”ìš© ë„êµ¬
# user_aware_rag_tools = [user_aware_rag_search]
