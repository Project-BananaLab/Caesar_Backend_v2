# app/rag/internal_data_rag/user_aware_retrieve.py
# -*- coding: utf-8 -*-
"""
ì‚¬ìš©ìë³„ ê¶Œí•œì„ ê³ ë ¤í•œ RAG ë¬¸ì„œ ê²€ìƒ‰ ì„œë¹„ìŠ¤
- íšŒì‚¬ ê³µê°œ ë¬¸ì„œ: ê°™ì€ íšŒì‚¬ ì§ì› ëª¨ë‘ ì ‘ê·¼ ê°€ëŠ¥
- ê°œì¸ ë¬¸ì„œ: ë³¸ì¸ë§Œ ì ‘ê·¼ ê°€ëŠ¥
- ë‹¤ë¥¸ ì§ì›ì˜ ê°œì¸ ë¬¸ì„œ: ì ‘ê·¼ ë¶ˆê°€
"""

import os
from typing import List, Tuple, Optional
from sqlalchemy.orm import Session
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import tool
from langchain_core.documents import Document

from app.utils.db import get_db
from app.features.login.employee_google import crud as employee_crud
from app.features.admin.services.file_ingest_service import _get_company_code
from app.rag.internal_data_rag.internal_retrieve import (
    _stable_similarity,
    _truncate_context_blocks,
    MAX_CONTEXT_CHARS,
    CHROMA_PATH,
    EMBED_MODEL,
    CHAT_MODEL,
)


class UserAwareRAGService:
    """ì‚¬ìš©ìë³„ ê¶Œí•œì„ ê³ ë ¤í•œ RAG ê²€ìƒ‰ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.embeddings = OpenAIEmbeddings(model=EMBED_MODEL)
        self.llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)
        self.parser = StrOutputParser()
        self.prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "ë‹¹ì‹ ì€ ì‚¬ë‚´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•íˆ ë‹µí•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
                    "ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë‹µë³€í•˜ê³ , "
                    "ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥´ëŠ” ë‚´ìš©ì€ 'ëª¨ë¥¸ë‹¤'ê³  ëª…í™•íˆ ë§í•˜ì„¸ìš”. "
                    "ê°€ëŠ¥í•œ í•œ ì¶œì²˜ ë¬¸ì„œëª…ê³¼ í•¨ê»˜ ë‹µë³€í•˜ì„¸ìš”.",
                ),
                (
                    "user",
                    "ì§ˆë¬¸: {question}\n\n" "ì°¸ê³  ì»¨í…ìŠ¤íŠ¸(ì—¬ëŸ¬ ì²­í¬):\n{context}",
                ),
            ]
        )

    def get_user_context(self, user_id: str) -> Optional[dict]:
        """
        user_id(google_user_id)ë¡œë¶€í„° ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ
        Returns: {"employee_id": int, "company_id": int, "company_code": str} or None
        """
        try:
            db = next(get_db())
            employee = employee_crud.get_employee_by_google_id(
                db, google_user_id=user_id
            )
            if not employee:
                print(f"âŒ ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {user_id}")
                return None

            company_code = _get_company_code(db, employee.company_id)
            return {
                "employee_id": employee.id,
                "company_id": employee.company_id,
                "company_code": company_code,
            }
        except Exception as e:
            print(f"âŒ ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
        finally:
            db.close()

    def retrieve_documents_with_permission(
        self, query: str, user_id: str, top_k: int = 5
    ) -> List[Tuple[str, dict]]:
        """
        ì‚¬ìš©ìë³„ ê¶Œí•œì„ ê³ ë ¤í•œ ë¬¸ì„œ ê²€ìƒ‰
        - íšŒì‚¬ ê³µê°œ ë¬¸ì„œ(is_private=False): ê°™ì€ íšŒì‚¬ ì§ì› ëª¨ë‘ ì ‘ê·¼ ê°€ëŠ¥
        - ê°œì¸ ë¬¸ì„œ(is_private=True): ë³¸ì¸(user_id)ë§Œ ì ‘ê·¼ ê°€ëŠ¥
        """
        user_context = self.get_user_context(user_id)
        if not user_context:
            print(f"âŒ ì‚¬ìš©ì ì •ë³´ ì—†ìŒ - ê²€ìƒ‰ ì¤‘ë‹¨: {user_id}")
            return []

        company_code = user_context["company_code"]
        employee_id = user_context["employee_id"]

        try:
            print(
                f"ğŸ” ê¶Œí•œë³„ ë¬¸ì„œ ê²€ìƒ‰: '{query}' (íšŒì‚¬: {company_code}, ì‚¬ìš©ì: {employee_id})"
            )

            # íšŒì‚¬ë³„ ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰ (Chroma Cloud ì‚¬ìš©)
            import chromadb

            # Chroma Cloud í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            client = chromadb.CloudClient(
                tenant=os.getenv("CHROMA_TENANT"),
                database=os.getenv("CHROMA_DATABASE"),
                api_key=os.getenv("CHROMA_API_KEY"),
            )

            vectorstore = Chroma(
                client=client,
                collection_name=company_code,
                embedding_function=self.embeddings,
            )

            # ë©”íƒ€ë°ì´í„° í•„í„°ë¡œ ê¶Œí•œ ìˆëŠ” ë¬¸ì„œë§Œ ê²€ìƒ‰
            # Chroma í•„í„° ë¬¸ë²•: {"$and": [ì¡°ê±´1, ì¡°ê±´2]} ë˜ëŠ” {"$or": [ì¡°ê±´1, ì¡°ê±´2]}

            # 1. íšŒì‚¬ ê³µê°œ ë¬¸ì„œ ê²€ìƒ‰ (company_id ì¼ì¹˜ AND is_private=False)
            company_filter = {
                "$and": [
                    {"company_id": {"$eq": user_context["company_id"]}},
                    {"is_private": {"$eq": False}},
                ]
            }

            # 2. ê°œì¸ ë¬¸ì„œ ê²€ìƒ‰ (company_id ì¼ì¹˜ AND is_private=True AND user_id ì¼ì¹˜)
            personal_filter = {
                "$and": [
                    {"company_id": {"$eq": user_context["company_id"]}},
                    {"is_private": {"$eq": True}},
                    {"user_id": {"$eq": employee_id}},
                ]
            }

            print(f"ğŸ” íšŒì‚¬ ê³µê°œ ë¬¸ì„œ ê²€ìƒ‰ í•„í„°: {company_filter}")
            print(f"ğŸ” ê°œì¸ ë¬¸ì„œ ê²€ìƒ‰ í•„í„°: {personal_filter}")

            # íšŒì‚¬ ê³µê°œ ë¬¸ì„œ ê²€ìƒ‰
            company_results = vectorstore.similarity_search_with_score(
                query, k=top_k, filter=company_filter
            )
            print(f"ğŸ“Š íšŒì‚¬ ê³µê°œ ë¬¸ì„œ: {len(company_results)}ê°œ ë°œê²¬")

            # ê°œì¸ ë¬¸ì„œ ê²€ìƒ‰
            personal_results = vectorstore.similarity_search_with_score(
                query, k=top_k, filter=personal_filter
            )
            print(f"ğŸ“Š ê°œì¸ ë¬¸ì„œ: {len(personal_results)}ê°œ ë°œê²¬")

            # ë‘ ê²°ê³¼ í•©ì¹˜ê³  ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            all_results = company_results + personal_results
            all_results.sort(key=lambda x: x[1])  # distance ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬

            # top_k ê°œìˆ˜ë§Œí¼ë§Œ ì„ íƒ
            results = all_results[:top_k]

            if not results:
                print("âŒ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return []

            # ë©”íƒ€ë°ì´í„° í•„í„°ë¡œ ì´ë¯¸ ê¶Œí•œ ê²€ì¦ëœ ê²°ê³¼ ì²˜ë¦¬
            contexts = []
            print(f"âœ… ê¶Œí•œ í•„í„°ë§ëœ {len(results)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

            for i, (doc, distance) in enumerate(results, start=1):
                similarity = _stable_similarity(distance)
                meta = dict(doc.metadata or {})
                meta["similarity_score"] = similarity

                is_private = meta.get("is_private", False)
                doc_type = "ê°œì¸ ë¬¸ì„œ" if is_private else "íšŒì‚¬ ë¬¸ì„œ"
                preview = (
                    (doc.page_content[:80] + "...")
                    if len(doc.page_content) > 80
                    else doc.page_content
                )

                print(
                    f"  [Rank {i}] ìœ ì‚¬ë„={similarity:.4f}, {doc_type}, source={meta.get('source')}"
                )
                print(f"          ë‚´ìš©: {preview}")

                contexts.append((doc.page_content, meta))

            return contexts

        except Exception as e:
            print(f"âŒ ê¶Œí•œë³„ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return []

    def generate_answer(
        self, query: str, contexts: List[Tuple[str, dict]], model: str = None
    ) -> str:
        """í”„ë¡¬í”„íŠ¸ì— ì»¨í…ìŠ¤íŠ¸ë¥¼ ì£¼ì…í•´ LLM í˜¸ì¶œ"""
        if not contexts:
            return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."

        try:
            model_label = model or CHAT_MODEL
            print(
                f"âš™ï¸ ë‹µë³€ ìƒì„± ì¤‘... ({len(contexts)}ê°œ ì»¨í…ìŠ¤íŠ¸, ëª¨ë¸: {model_label})"
            )

            # ì»¨í…ìŠ¤íŠ¸ íŠ¸ë ì¼€ì´ì…˜ (ìœ ì‚¬ë„ ìˆœ)
            context_text = _truncate_context_blocks(
                contexts, max_chars=MAX_CONTEXT_CHARS
            )

            # LCEL ì²´ì¸ ì‹¤í–‰
            used_llm = (
                self.llm if model is None else ChatOpenAI(model=model, temperature=0)
            )
            chain = self.prompt | used_llm | self.parser
            answer = chain.invoke({"question": query, "context": context_text})

            print("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            return answer
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def query_rag_with_permission(
        self,
        query: str,
        user_id: str,
        top_k: int = 4,
        model: str = None,
        show_sources: bool = True,
    ) -> str:
        """ì‚¬ìš©ìë³„ ê¶Œí•œì„ ê³ ë ¤í•œ RAG ì§ˆì˜ ì²˜ë¦¬"""
        print(f"\nğŸ” ê¶Œí•œë³„ RAG ì§ˆì˜: {query} (ì‚¬ìš©ì: {user_id})")

        contexts = self.retrieve_documents_with_permission(query, user_id, top_k)
        if not contexts:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì ‘ê·¼ ê°€ëŠ¥í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        answer = self.generate_answer(query, contexts, model)

        if show_sources:
            sources = []
            for _, meta in contexts:
                is_private = meta.get("is_private", False)
                doc_type = "ê°œì¸ ë¬¸ì„œ" if is_private else "íšŒì‚¬ ë¬¸ì„œ"
                src = f"- {meta.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')} ({doc_type}, ì²­í¬ {meta.get('chunk_idx', 'N/A')})"
                if src not in sources:
                    sources.append(src)
            return f"{answer}\n\nğŸ“‹ ì°¸ê³ í•œ ë¬¸ì„œ:\n" + "\n".join(sources)

        return answer


# ì „ì—­ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
_user_aware_rag_service = UserAwareRAGService()


# ========================= LangChain ë„êµ¬ ì •ì˜ =========================


@tool
def user_aware_rag_search(query: str, user_id: str) -> str:
    """
    ì‚¬ìš©ìë³„ ê¶Œí•œì„ ê³ ë ¤í•œ ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬
    - íšŒì‚¬ ê³µê°œ ë¬¸ì„œ: ê°™ì€ íšŒì‚¬ ì§ì› ëª¨ë‘ ì ‘ê·¼ ê°€ëŠ¥
    - ê°œì¸ ë¬¸ì„œ: ë³¸ì¸ë§Œ ì ‘ê·¼ ê°€ëŠ¥

    Args:
        query (str): ê²€ìƒ‰í•  ì§ˆë¬¸
        user_id (str): ì‚¬ìš©ì ID (Google User ID)

    Returns:
        str: ê²€ìƒ‰ ê²°ê³¼ ë° ë‹µë³€
    """
    try:
        return _user_aware_rag_service.query_rag_with_permission(
            query=query, user_id=user_id, top_k=4, show_sources=True
        )
    except Exception as e:
        print(f"âŒ ì‚¬ìš©ìë³„ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return f"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"


# ========================= ì‚¬ìš©ìë³„ ë„êµ¬ íŒ©í† ë¦¬ =========================


def create_user_aware_rag_tools(user_id: str) -> list:
    """
    íŠ¹ì • ì‚¬ìš©ìë¥¼ ìœ„í•œ RAG ë„êµ¬ ìƒì„± (user_id ë°”ì¸ë”©)

    Args:
        user_id (str): ì‚¬ìš©ì ID (Google User ID)

    Returns:
        list: ì‚¬ìš©ìë³„ë¡œ ë°”ì¸ë”©ëœ RAG ë„êµ¬ ëª©ë¡
    """
    print(f"ğŸ”§ ì‚¬ìš©ìë³„ RAG ë„êµ¬ ìƒì„± ì¤‘: {user_id}")

    @tool
    def internal_rag_search(query: str) -> str:
        """

      


        Search and answer questions from uploaded files (PDF, DOCX, XLSX, etc.).
        
        ğŸ¯ This tool ONLY searches content from uploaded files:
        - PDF documents (regulations, manuals, reports, etc.)
        - Word documents (policies, guidelines, contracts, etc.)  
        - Excel files (data, forms, status reports, etc.)
        - Other uploaded document files
        
        Search scope:
        - Company public documents: Company policies, manuals, announcements, forms, etc.
        - Personal uploaded documents: Files personally uploaded by the user
        
        Use when:
        - File-based questions like "production status writing methods", "employee regulations", "manuals"
        - Specific content or procedure inquiries from uploaded documents
        - Questions about forms, templates, formats
        
        Args:
            query (str): Question or keyword to search in uploaded documents
            

        Returns:
            str: Answer based on uploaded documents
        """
        print(f"ğŸ” internal_rag_search í˜¸ì¶œë¨: query='{query}', user_id='{user_id}'")
        try:
            result = _user_aware_rag_service.query_rag_with_permission(
                query=query,
                user_id=user_id,  # íŒ©í† ë¦¬ì—ì„œ ì „ë‹¬ë°›ì€ user_id ë°”ì¸ë”©
                top_k=4,
                show_sources=True,
            )
            return result
        except Exception as e:
            print(f"âŒ RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return f"ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    print(f"âœ… RAG ë„êµ¬ ìƒì„± ì™„ë£Œ: {user_id}")
    return [internal_rag_search]


# ê¸°ì¡´ ë²”ìš© ë„êµ¬ (ë§¤ê°œë³€ìˆ˜ 2ê°œ - ì§ì ‘ í˜¸ì¶œìš©)
user_aware_rag_tools = [user_aware_rag_search]
